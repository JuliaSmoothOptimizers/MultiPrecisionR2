<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>FPMPNLPModel · MultiPrecisionR2.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="/FPMPNLPModel/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="MultiPrecisionR2.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultiPrecisionR2.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../reference/">Reference</a></li><li><a class="tocitem" href="../MPCounters/">MPCounters</a></li><li class="is-active"><a class="tocitem" href>FPMPNLPModel</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Fields"><span>Fields</span></a></li><li><a class="tocitem" href="#γfunc"><span><code>γfunc</code></span></a></li><li class="toplevel"><a class="tocitem" href="#Constructors"><span>Constructors</span></a></li><li><a class="tocitem" href="#Keyword-arguments"><span>Keyword arguments</span></a></li><li><a class="tocitem" href="#Checks-upon-instantiation"><span>Checks upon instantiation</span></a></li><li class="toplevel"><a class="tocitem" href="#Evaluation-Errors"><span>Evaluation Errors</span></a></li><li><a class="tocitem" href="#Taking-Norm-Computation-Error-Into-Account"><span>Taking Norm Computation Error Into Account</span></a></li><li><a class="tocitem" href="#Relative-Error-Evaluation"><span>Relative Error Evaluation</span></a></li><li><a class="tocitem" href="#Interval-Error-Evaluation"><span>Interval Error Evaluation</span></a></li><li><a class="tocitem" href="#HPFormat"><span>HPFormat</span></a></li><li class="toplevel"><a class="tocitem" href="#Interface"><span>Interface</span></a></li><li class="toplevel"><a class="tocitem" href="#Examples"><span>Examples</span></a></li><li><a class="tocitem" href="#Interval-Evaluations"><span>Interval Evaluations</span></a></li><li><a class="tocitem" href="#Relative-error"><span>Relative error</span></a></li></ul></li><li><a class="tocitem" href="../MultiPrecisionR2/">MultiPrecisionR2</a></li><li><a class="tocitem" href="../tutorial_FPMPNLPModel/">FPMPNLPModel Tutorial</a></li><li><a class="tocitem" href="../tutorial_MPR2_basic_use/">MPR2 Tutorial: Basic Use </a></li><li><a class="tocitem" href="../tutorial_MPR2_advanced_use/">MPR2 Tutorial: Advanced Use </a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>FPMPNLPModel</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>FPMPNLPModel</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/MultiPrecisionR2" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="FPMPNLPModel:-Multi-Precision-Models"><a class="docs-heading-anchor" href="#FPMPNLPModel:-Multi-Precision-Models">FPMPNLPModel: Multi-Precision Models</a><a id="FPMPNLPModel:-Multi-Precision-Models-1"></a><a class="docs-heading-anchor-permalink" href="#FPMPNLPModel:-Multi-Precision-Models" title="Permalink"></a></h1><p><code>FPMPNLPModel</code> (Floating Point Multi Precision Non Linear Model) is a subtype of <code>AbstractNLPModel</code> and implements the <code>NLPModel API</code> defined in <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl">NLPModels.jl</a> to deal with multiple floating point formats and handle evaluation errors.</p><h1 id="Fields"><a class="docs-heading-anchor" href="#Fields">Fields</a><a id="Fields-1"></a><a class="docs-heading-anchor-permalink" href="#Fields" title="Permalink"></a></h1><p><code>FPMPNLPModel</code> structure contains a <code>NLPModel</code> field and additional field related to multiple precision. The types are:</p><ul><li><code>D &lt; AbstractFloat</code></li><li><code>S &lt; AbstractVector</code></li><li><code>H &lt; AbstractFloat</code></li><li><code>B &lt; Tuple}</code></li></ul><table><tr><th style="text-align: right">Field</th><th style="text-align: right">Type</th><th style="text-align: right">Notes</th></tr><tr><td style="text-align: right"><code>Model</code></td><td style="text-align: right"><code>AbstractNLPModel{D, S}</code></td><td style="text-align: right">Base model</td></tr><tr><td style="text-align: right"><code>meta</code></td><td style="text-align: right"><code>NLPModelMeta</code></td><td style="text-align: right">meta data</td></tr><tr><td style="text-align: right"><code>counters</code></td><td style="text-align: right"><code>MPCounters</code></td><td style="text-align: right">multi-precision counters</td></tr><tr><td style="text-align: right"><code>FPList</code></td><td style="text-align: right"><code>Vector{DataType}</code></td><td style="text-align: right">List Floating Point formats used</td></tr><tr><td style="text-align: right"><code>EpsList</code></td><td style="text-align: right"><code>Vector{H}</code></td><td style="text-align: right">List of epsilon machine corresponding to <code>FPList</code></td></tr><tr><td style="text-align: right"><code>UList</code></td><td style="text-align: right"><code>Vector{H}</code></td><td style="text-align: right">List of unit roundoff corresponding to <code>FPList</code></td></tr><tr><td style="text-align: right"><code>OFList</code></td><td style="text-align: right"><code>Vector{H}</code></td><td style="text-align: right">List of largest representable numbers corresponding to <code>FPList</code></td></tr><tr><td style="text-align: right"><code>γfunc</code></td><td style="text-align: right"></td><td style="text-align: right">dot product error model function callback.</td></tr><tr><td style="text-align: right"><code>ωfRelErr</code></td><td style="text-align: right"><code>Vector{H}</code></td><td style="text-align: right">List of relative error factor for objective function evaluation corresponding to FP formats in <code>FPList</code></td></tr><tr><td style="text-align: right"><code>ωgRelErr</code></td><td style="text-align: right"><code>Vector{H}</code></td><td style="text-align: right">List of relative error factor for gradient evaluation corresponding to FP formats in <code>FPList</code></td></tr><tr><td style="text-align: right"><code>ObjEvalMode</code></td><td style="text-align: right"><code>Int</code></td><td style="text-align: right">objective function error evaluation mode</td></tr><tr><td style="text-align: right"><code>GradEvalMode</code></td><td style="text-align: right"><code>Int</code></td><td style="text-align: right">gradient error evaluation mode</td></tr><tr><td style="text-align: right"><code>X</code></td><td style="text-align: right"><code>B</code></td><td style="text-align: right">Container used for interval evaluation, memory pre-allocation</td></tr><tr><td style="text-align: right"><code>G</code></td><td style="text-align: right"><code>B</code></td><td style="text-align: right">Container used for interval evaluation, memory pre-allocation</td></tr></table><h2 id="γfunc"><a class="docs-heading-anchor" href="#γfunc"><code>γfunc</code></a><a id="γfunc-1"></a><a class="docs-heading-anchor-permalink" href="#γfunc" title="Permalink"></a></h2><p><code>γfunc</code> callback function provides the error on dot product: <span>$|x.y - fl(x.y)|\leq |x|.|y|\gamma func(n,u)$</span> with <span>$n$</span> the dimension of <span>$x$</span> and <span>$y$</span> vector and <span>$u$</span> the unit-roundoff of the FP format used to perform the dot product operation.  The expected template is: <code>γfunc(n::Int,u::AbstractFloat)</code> with <code>n</code> the dimension of the problem and <code>u</code> the unit roundoff of the considered FPFormat.  By default, <code>γfunc(n,u) = n*u</code> is used. This function is used to take finite-precision norm computation error into account, to further guarantee gradient error bounds (details below).</p><h1 id="Constructors"><a class="docs-heading-anchor" href="#Constructors">Constructors</a><a id="Constructors-1"></a><a class="docs-heading-anchor-permalink" href="#Constructors" title="Permalink"></a></h1><ol><li><p><code>FPMPNLPModel(Model::AbstractNLPModel{D,S},FPList::Vector{K}; kwargs...) where {D,S,K&lt;:DataType}</code></p></li><li><p><code>FPMPNLPModel(f,x0, FPList::Vector{DataType}; kwargs...)</code></p></li></ol><p>Build a <code>ADNLPModel</code> from <code>f</code> and <code>x0</code> and call constructor 1.</p><h2 id="Keyword-arguments"><a class="docs-heading-anchor" href="#Keyword-arguments">Keyword arguments</a><a id="Keyword-arguments-1"></a><a class="docs-heading-anchor-permalink" href="#Keyword-arguments" title="Permalink"></a></h2><ul><li><code>HPFormat=Float64</code> : high precision format (must be at least as accurate as <code>FPList[end]</code>), corrensponds to <code>H</code> parameter after instantiation</li><li><code>γfunc=nothing</code> : use default callback if not provided (see Fields section above)</li><li><code>ωfRelErr=HPFormat.(sqrt.(eps.(FPList)))</code>: use relative error model by default for objective evaluation</li><li><code>ωgRelErr=HPFormat.(sqrt.(eps.(FPList)))</code>: use relative error model by default for gradient evaluation</li><li><code>obj_int_eval = false</code> : if true, use interval arithmetic for objective value and error evaluation</li><li><code>grad_int_eval = false</code> : if true, use interval arithmetic for gradient value and error evaluation</li></ul><h2 id="Checks-upon-instantiation"><a class="docs-heading-anchor" href="#Checks-upon-instantiation">Checks upon instantiation</a><a id="Checks-upon-instantiation-1"></a><a class="docs-heading-anchor-permalink" href="#Checks-upon-instantiation" title="Permalink"></a></h2><p>Some checks are performed upon instanctiation. These checks include:</p><ul><li>Length consistency of vector fields:  <code>FPList</code>, <code>EpsList</code>, <code>UList</code>;</li><li><code>HPFormat</code> is at least as accurate as the highest precision floating point format in <code>FPList</code>. Ideally HPFormat is more accurate to ensure the numerical stability;</li><li>Interval evaluations: it might happen that interval evaluation of objective function and/or gradient is type-unstable or returns an error. The constructor returns an error in this case. This type of error is most likely due to <a href="https://github.com/JuliaIntervals/IntervalArithmetic.jl/blob/master/README.md">IntervalArithmetic.jl</a> package;</li><li><code>FPList</code> is ordered by increasing floating point format accuracy.</li></ul><p>This checks can return <code>@warn</code> or <code>error</code>.</p><h1 id="Evaluation-Errors"><a class="docs-heading-anchor" href="#Evaluation-Errors">Evaluation Errors</a><a id="Evaluation-Errors-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation-Errors" title="Permalink"></a></h1><p><code>FPMPNLPModel</code> provides interfaces to evaluate the objective function and gradient and evaluation errors due to finite-precision computations. The evaluation errors on the objective function and the gradient, <span>$\omega_f$</span> and <span>$\omega_g$</span>, are such that</p><ul><li>Objective function : <span>$|f(x) - fl(f(x))|\leq \omega_f$</span></li><li>Gradient : <span>$||\nabla f(x) - fl(\nabla f(x))||_2 \leq \omega_g ||fl(\nabla f(x))||_2$</span></li></ul><p>where <span>$fl()$</span> denotes the finite-precision computation with one of the FP formats in <code>FPList</code>.</p><p><code>FPNLPModel</code> enables to determine evaluation errors <span>$\omega_f$</span> and <span>$\omega_g$</span> either:</p><ul><li>based on relative error model with <code>ωfRelErr</code> and <code>ωgRelErr</code>,</li><li>in a guaranteed way based on interval evaluation with <code>IntervalArithmetic.jl</code> package.</li></ul><p>By default, the relative error model is used. Interval evaluation can be selected upon instantiation of <code>FPMPNLPModel</code> with <code>obj_int_eval</code> and <code>grad_int_eval</code> kwargs.</p><h2 id="Taking-Norm-Computation-Error-Into-Account"><a class="docs-heading-anchor" href="#Taking-Norm-Computation-Error-Into-Account">Taking Norm Computation Error Into Account</a><a id="Taking-Norm-Computation-Error-Into-Account-1"></a><a class="docs-heading-anchor-permalink" href="#Taking-Norm-Computation-Error-Into-Account" title="Permalink"></a></h2><p>For the gradient error, the 2-norm computation error due to finite-precision computations is taken into account via <code>γfunc</code>, such that <span>$\omega_g$</span> is guaranteed. The norm computation error is given by <span>$| ||x||_2 - fl(||x||_2) | \leq \beta(n+2,u) fl(||x||_2)$</span>, with <span>$n$</span> the dimension of the problem, <span>$u$</span> the unit-roundoff of the FP format used to perform the norm computation, and</p><p class="math-container">\[\beta(n,u) = \max(|\sqrt{\gamma func(n,u)-1}-1|,|\sqrt{\gamma func(n,u)+1}-1|)\]</p><p>defined from <code>γfunc</code>.</p><p>The high precision format <code>H</code> is used to compute <span>$\beta(n+2,u)$</span>.</p><h2 id="Relative-Error-Evaluation"><a class="docs-heading-anchor" href="#Relative-Error-Evaluation">Relative Error Evaluation</a><a id="Relative-Error-Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Relative-Error-Evaluation" title="Permalink"></a></h2><p>By default, evaluation errors for objective and gradient are estimated with the relative error model.</p><p>The error models are: </p><ul><li>Objective: <span>$|f(x) - fl(f(x))| \leq$</span><code>ωfRelErr[id]</code><span>$*fl(f(x))$</span> where <code>id</code> is the index of the FP format of <span>$x$</span> in <code>FPList</code>. <code>objerrmp</code> returns the value of the classic evaluation of the objective as the value of the objective, and <code>ωfRelErr[id]</code><span>$*fl(f(x))$</span> as the evaluation error <span>$\omega_f$</span>, where <code>id</code> is the index of the FP format of <span>$x$</span> in <code>FPList</code></li><li>Gradient: <span>$||\nabla f(x) - fl(\nabla f(x))||_2 \leq$</span><code>ωgRelErr[id]</code><span>$||fl(\nabla f(x))||_2$</span> where where <code>id</code> is the index of the FP format of <span>$x$</span> in <code>FPList</code>. <code>graderrmp</code> returns the value of the classic evaluation of the gradient as the value of the gradient, and <code>ωgRelErr[id]</code> as the value of the evaluation error.</li></ul><p>See keyword arguments section for <code>ωfRelErr</code> and <code>ωgRelErr</code> default values. </p><h2 id="Interval-Error-Evaluation"><a class="docs-heading-anchor" href="#Interval-Error-Evaluation">Interval Error Evaluation</a><a id="Interval-Error-Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Interval-Error-Evaluation" title="Permalink"></a></h2><p>Error of the objective (resp. gradient) evaluation can be determined with interval arithmetic. This evaluation mode can be set upon <code>FPMPNLPModel</code> instantiation with <code>obj_int_eval</code> and <code>grad_int_eval</code>.</p><ul><li><p>Objective: evaluating the objective with interval arithmetic provides the interval <span>$[\underline{f},\overline{f}]$</span> such that <span>$\underline{f}\leq fl(f) \leq \overline{f}$</span>. <code>objerrmp</code> returns the middle of the interval as the value of the objective, that is <span>$(\underline{f}+\overline{f})/2$</span>, and returns the diameter of the interval as <span>$\omega_f$</span>, that is, <span>$\omega_f = (\underline{f}-\overline{f})/2$</span></p></li><li><p>Gradient: evaluating the gradient with interval arithmetic provides the interval vector <span>$G = [\underline{g}_1,\overline{g}_1] \times ... \times [\underline{g}_n,\overline{g}_n]$</span> such that <span>$\nabla f(x) \in G$</span>. <code>graderrmp</code> returns <span>$g$</span> the middle of the interval vector as the value of the gradient. The evaluation error <span>$\nabla f(x)-g$</span> is the vector of the diameters of the element of <span>$G$</span>. The error <span>$\omega_g$</span> returned by <code>graderrmp</code> is <span>$||\nabla f(x)-g||_2/||g||_2 * (1+\beta(n+2,u))/(1-\beta(n+2,u))$</span>, the second term takes norm computation error into account.</p></li></ul><p><strong>Warning</strong></p><ul><li>Interval evaluation is slow compared with &quot;classic&quot; evaluation.</li><li>Although guaranteed, interval bounds can be quite pessimistic.</li><li>Interval evaluation might fail with rounding mode other than <code>:accurate</code> for FP formats other than <code>Float32</code> and <code>Float64</code>. When using interval evaluation, it is recommended to call </li></ul><pre><code class="language-julia hljs">using IntervalArithmetic
setrounding(Interval,:accurate)</code></pre><p>before instanciating a <code>FPMPNLPModel</code>.</p><h2 id="HPFormat"><a class="docs-heading-anchor" href="#HPFormat">HPFormat</a><a id="HPFormat-1"></a><a class="docs-heading-anchor-permalink" href="#HPFormat" title="Permalink"></a></h2><p><code>FPMPNLPModel</code> requires a high-precision FP format, given by <code>HPFormat</code> constructor&#39;s keyword argument. This format is used to compute accurately a bound on finite-precision norm evaluation error, to further guaranteed the bound <span>$\omega_g$</span> in interval evaluation context. The bound on norm error is computed via <code>γfunc</code>.</p><h1 id="Interface"><a class="docs-heading-anchor" href="#Interface">Interface</a><a id="Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Interface" title="Permalink"></a></h1><table><tr><th style="text-align: right">function</th><th style="text-align: right">desctiption</th></tr><tr><td style="text-align: right"><code>FPMPNLPModel</code></td><td style="text-align: right">constructor (see Constructor section)</td></tr><tr><td style="text-align: right"><code>get_id</code></td><td style="text-align: right">Returns index of FP format in <code>FPList</code></td></tr><tr><td style="text-align: right"><code>objerrmp</code></td><td style="text-align: right">Compute objective function value and evaluation error <span>$\omega_f$</span></td></tr><tr><td style="text-align: right"><code>graderrmp!</code></td><td style="text-align: right">Compute gradient and evaluation error <span>$\omega_g$</span> (no memory allocation for gradient)</td></tr><tr><td style="text-align: right"><code>graderrmp</code></td><td style="text-align: right">Compute gradient and evaluation error <span>$\omega_g$</span> (memory allocation for gradient)</td></tr><tr><td style="text-align: right"><code>objReachPrec</code></td><td style="text-align: right">Compute objective and evaluation error <span>$\omega_f$</span>, increases FP format precision until bound on <span>$\omega_f$</span> is reached</td></tr><tr><td style="text-align: right"><code>gradReachPrec!</code></td><td style="text-align: right">Compute gradient and evaluation error <span>$\omega_g$</span>, increases FP format precision until bound on <span>$\omega_g$</span> is reached (no mem. allocation for gradient)</td></tr><tr><td style="text-align: right"><code>gradReachPrec</code></td><td style="text-align: right">Compute gradient and evaluation error <span>$\omega_g$</span>, increases FP format precision until bound on <span>$\omega_g$</span> is reached (mem. allocation for gradient)</td></tr></table><h1 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h1><h2 id="Interval-Evaluations"><a class="docs-heading-anchor" href="#Interval-Evaluations">Interval Evaluations</a><a id="Interval-Evaluations-1"></a><a class="docs-heading-anchor-permalink" href="#Interval-Evaluations" title="Permalink"></a></h2><pre><code class="language- hljs">using MultiPrecisionR2
using IntervalArithmetic

setrounding(Interval,:accurate)
Formats = [Float16,Float32,Float64] # FP formats
f(x) = sum(x.^2) # objective function
dim = 100 # problem dimentsion
x0 = ones(100)
mpmodel = FPMPNLPModel(f,x0,Formats) # create multi-precision model, will use interval arithmetic for evaluation error.

x16 = ones(Float16,dim)
x32 = Float32.(x16)
x64 = x0

# objective evaluation 
f16, ωf16 = objerrmp(mpmodel,x16) # Float16 objective evaluation
f32, ωf32 = objerrmp(mpmodel,x32) # Float32 objective evaluation
f64, ωf64 = objerrmp(mpmodel,x64) # Float32 objective evaluation

x = (x16,x32,x64) # element of the tuple should refer to the same vector in different FP formats
bound = ωf32*1.1 # error bound reachable with Float32 precision
fx, ωfx, fid = objReachPrec(mpmodel,x,bound; π=1) # evaluate objective with increasing precision, starting with mpnlpmodel.FPFormat[π] = Float16, until evaluation error is lower than bound (satisfied with Float32)


# gradient evaluation
g16, ωg16 = graderrmp(mpmodel,x16) # Float16 gradient evaluation
g32, ωg32 = graderrmp(mpmodel,x32) # Float32 gradient evaluation
g64, ωg64 = graderrmp(mpmodel,x64) # Float32 gradient evaluation

x = (x16,x32,x64) # element of the tuple should refer to the same vector in different FP formats
bound = ωg64*1.1 # error bound reachable with Float64 precision
gx, ωgx, gid = gradReachPrec(mpmodel,x,bound; π=1) # evaluate gradient with increasing precision, starting with mpnlpmodel.FPFormat[π] = Float16, until evaluation error is lower than bound (satisfied with Float64)</code></pre><h2 id="Relative-error"><a class="docs-heading-anchor" href="#Relative-error">Relative error</a><a id="Relative-error-1"></a><a class="docs-heading-anchor-permalink" href="#Relative-error" title="Permalink"></a></h2><pre><code class="language-julia hljs">using MultiPrecisionR2

Formats = [Float16,Float32,Float64] # FP formats
f(x) = sum(x.^2) # objective function
dim = 100 # problem dimentsion
x0 = ones(100)
ω = Float64.([sqrt(eps(t)) for t in Formats]) # relative errors, have to be H format (Float64)
mpmodel = FPMPNLPModel(f,x0,Formats; ωfRelErr = ω, ωgRelErr = ω) # create multi-precision model, will use relative error model base on ωfRelErr and ωgRelErr.

x16 = ones(Float16,dim)
x32 = Float32.(x16)
x64 = x0

# objective evaluation
f16, ωf16 = objerrmp(mpmodel,x16) # Float16 objective evaluation
f32, ωf32 = objerrmp(mpmodel,x32) # Float32 objective evaluation
f64, ωf64 = objerrmp(mpmodel,x64) # Float64 objective evaluation

# gradient evaluation
g16, ωg16 = graderrmp(mpmodel,x16) # Float16 gradient evaluation
g32, ωg32 = graderrmp(mpmodel,x32) # Float32 gradient evaluation
g64, ωg64 = graderrmp(mpmodel,x64) # Float64 gradient evaluation</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0  …  2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], 1.4901161193847656e-8)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../MPCounters/">« MPCounters</a><a class="docs-footer-nextpage" href="../MultiPrecisionR2/">MultiPrecisionR2 »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 3 August 2023 17:21">Thursday 3 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
