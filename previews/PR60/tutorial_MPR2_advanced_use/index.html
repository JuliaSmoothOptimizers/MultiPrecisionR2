<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Disclaimer · MultiPrecisionR2.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="/tutorial_MPR2_advanced_use/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="MultiPrecisionR2.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultiPrecisionR2.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Disclaimer</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Disclaimer</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/MultiPrecisionR2/blob/main/docs/src/tutorial_MPR2_advanced_use.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Disclaimer"><a class="docs-heading-anchor" href="#Disclaimer">Disclaimer</a><a id="Disclaimer-1"></a><a class="docs-heading-anchor-permalink" href="#Disclaimer" title="Permalink"></a></h1><p>The reader is encouraged to read the <code>FPMPNLPModel</code> and MPR2 Basic Use tutorial before this one.</p><h1 id="Advanced-Use"><a class="docs-heading-anchor" href="#Advanced-Use">Advanced Use</a><a id="Advanced-Use-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Use" title="Permalink"></a></h1><p><code>MultiPrecisionR2.jl</code> does more than implementing MPR2 algorithm as describes in the Basic Use tutorial. <code>MPR2Precision.jl</code> enables the user to define its own strategy to select evaluation precision levels and to handle evaluation errors. This is made possible by using callback functions when calling <code>MultiPrecisionR2.solve!()</code>. The default implementation of <code>MultiPrecisionR2.solve!()</code> relies on specific implementation of these callback functions, which are included in the package. The user is free to provide its own callback functions to change the behavior of the algorithm. </p><h2 id="Diving-into-MPR2-Implementation"><a class="docs-heading-anchor" href="#Diving-into-MPR2-Implementation">Diving into MPR2 Implementation</a><a id="Diving-into-MPR2-Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Diving-into-MPR2-Implementation" title="Permalink"></a></h2><p><code>MPR2Precision.jl</code> relies on callback functions that handle the objective and gradient evaluation. These callback functions are expected to compute values for the objective and the gradient and handle the evaluation precision. In the default implementation, the conditions on the error bound of objective and gradient evaluation are dealt with in these callback functions. That is why such convergence conditions (which user might choose not to implement) does not appear in the minimal implementation description of the code below.</p><p>Note that the function <code>MPR2()</code> use to run the algorithm in the Basic Use tutorial is just an interface to call the solver with convenience. The algorithm is actually implemented in <code>solve!()</code> which takes as arguments</p><ul><li><code>m::FPMPNLPModel</code> : the multi-precision model</li><li><code>solver::MPR2Solver</code>: a structure that contains the algorithm variables (current solution, gradient, evaluation errors, ...)</li><li><code>stats::GeneriExecutionStats</code>: a structure containing algorithm status (nb. iteration, elapsed time, termination status,...), see <a href="https://github.com/JuliaSmoothOptimizers/SolverCore.jl">SolverCore.jl</a> package.</li></ul><h3 id="**Minimal-Implementation-Description**"><a class="docs-heading-anchor" href="#**Minimal-Implementation-Description**"><strong>Minimal Implementation Description</strong></a><a id="**Minimal-Implementation-Description**-1"></a><a class="docs-heading-anchor-permalink" href="#**Minimal-Implementation-Description**" title="Permalink"></a></h3><p>Here is a minimal description of the implementation of <code>solve!()</code> to understand when and why these functions are called. Please refer to the actual implementation for details. Note that the callback functions templates are not respected for the sake of explanation.</p><pre><code class="language-julia hljs">function solve!(solver::MPR2solver{T},MPnlp::FPMPNLPModel{H},stats::GenericExecutionStats;kwargs...)
  # ...
  # some initialization (parameters, containers,...)
  # ...
  compute_f_at_x!() # initialize objective value at x0
  # check for possible overflow 
  compute_g!() # initialize gradient value at x0
  # check for possible overflow
  # ...
  # some more initialization (gradient norm tolerance,...)
  # ...
  while # main loop
    # compute step as sk = -gk/σk
    # check for step overflow
    # compute candidate as c = x+s
    # check for candidate overflow
    # compute model decrease ΔT = -g^T s
    # check for model decrease under/overflow
    recompute_g!() # possibly recompute g(x) and ωg(x) if gradient error/mu indicator is too big
    if grad_prec_fail # not enough precision with max precision
      break
    end
    if grad_recomputed# gradient recomputed by recompute_g!() 
      # recompute step with new value of the gradient
      # check overflow
      # recompute candidate with new step
      # check overflow
      # recompute model decrease with new gradient and step
    end
    compute_f_at_x() # possibly recompute f(x) and ωf(x) if ωf(x) is too big
    if f_x_prec_fail# not enough precision with max precision (ωf(x) too big)
      break
    end
    compute_f_at_c!() # compute f(c) and ωf(c)
    if f_c_prec_fail # not enough precision with max precision (ωf(c) too big)
      break
    end
    # ...
    # compute ρ = (f(x) - f(c))/ΔT
    # update x and σ
    # ... 
    if ρ ≥ η₁
      compute_g!() # compute g(c) and  ωg(c)
      # check overflow
      # ...
      # update values for next iteration
      # ...
      selectPic!()
    end
    # ...
    # check for termination
    # ...
  end # while main loop
  return status()
end</code></pre><p>The callback functions, and what they are supposed to do, are </p><ul><li><code>compute_f_at_x!()</code>: <strong>Selects evaluation precision</strong> and <strong>computes objective function at the current point <span>$x_k$</span></strong> such that possible conditions on error bounds are ensured. In the main loop, although the objective at <span>$x_k$</span> has already been computed at a previous iteration (<span>$f(c_{j}) = f(x_k)$</span> with <span>$j$</span> the last successful iteration) it might be necessary to recompute it to achieve smaller evaluation error (<span>$\omega_f(x_k)$</span>) if necessary. This function is also <strong>called for initialization</strong> (before the main loop), where typically no bound on evaluation error is required.</li><li><code>compute_f_at_c!()</code>: <strong>Selects evaluation precision</strong> and <strong>computes objective function at the candidate <span>$c_k$</span></strong> such that possible conditions on error bounds are ensured.</li><li><code>compute_g!()</code>: <strong>Selects evaluation precision</strong> and <strong>computes gradient at the candidate <span>$c_k$</span></strong>. It is possible here to include condiditions on gradient bound error <span>$\omega_g(x_k)$</span>. In the default implementation, multiple sources of rounding error are taken into account in the <span>$\mu$</span> indicator that requires to compute the step, candidate and model decrease (see Rounding Errors Handling of Basic Use tutorial). That is why in the default implementation no conditions on gradient error are required in this callback function, but are implemented in <code>recompute_g!()</code>.</li><li><code>recompute_g!()</code>: <strong>Selects evaluation precision</strong> and <strong>recomputes gradient at the current point <span>$x_k$</span></strong>. This callback enables to recompute the gradient <strong>if necessary</strong> with a better precision if needed. In the default implementation, this callback implement the condition on the <span>$\mu$</span> indicator (see Rouding Error Handling section of the Basic Use tutorial) and implements strategies to achieve sufficiently low value of <span>$\mu$</span> to ensure convergence.</li><li><code>selectPic!()</code>: Select FP format for <span>$c_{k+1}$</span>, enables to lower the FP format used for evaluation at the next iteration. See <a href="#candidate-precision-selection">Candidate Precision Selection</a> section below for details.</li></ul><h3 id="**Callback-Functions:-Templates**"><a class="docs-heading-anchor" href="#**Callback-Functions:-Templates**"><strong>Callback Functions: Templates</strong></a><a id="**Callback-Functions:-Templates**-1"></a><a class="docs-heading-anchor-permalink" href="#**Callback-Functions:-Templates**" title="Permalink"></a></h3><p>The callback functions for objective and gradient evaluation share the same template.</p><ul><li><code>success::Bool = compute_f_at_c!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)</code></li><li><code>success::Bool = compute_f_at_x_default!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)</code></li><li><code>success::Bool = compute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)</code></li><li><code>success::Bool, recompute_g::Bool = recompute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)</code></li></ul><p><strong>Arguments</strong>:</p><ul><li><code>m:FPMPNLPModel{H}</code>: multi-precision model, needed to perform objective/gradient evaluation</li><li><code>solver::MPR2Solver</code> : MPR2 solver structure, contains algorithm variables</li><li><code>stats::GenericExecutionStats</code> : Algorithm stats, can be used to access elapsed time, iteration, ...</li><li><code>e::E</code>: user defined structure to store additional information if needed</li></ul><p><strong>Outputs</strong></p><ul><li><code>success::Bool</code>: <code>false</code> if the evaluation failed, typical reason is that not enough evaluation precision could be reached. If <code>success == false</code>, stops the algorithm and set <code>stats.status = :exception</code>.</li><li><code>recompute_g::Bool</code>: <code>true</code> if <code>recompute_g!()</code> has recomputed the gradient. In this case, the step, candidate and model decrease are recomputed (see <a href="#minimal-implementation-description">Minimal Implementation Description</a>)</li></ul><h3 id="**Callback-Functions:-Expected-Behaviors**"><a class="docs-heading-anchor" href="#**Callback-Functions:-Expected-Behaviors**"><strong>Callback Functions: Expected Behaviors</strong></a><a id="**Callback-Functions:-Expected-Behaviors**-1"></a><a class="docs-heading-anchor-permalink" href="#**Callback-Functions:-Expected-Behaviors**" title="Permalink"></a></h3><p><strong>Modified Variables:</strong> The callback functions are expected to update all the variables that they modify. These variables are typically the <code>solver::MPR2Solver</code> fields, and possibly extra fields in user defined <code>e</code> structure. The callback functions <code>compute_g!()</code> and <code>recompute_g!()</code> are also expected to modify <code>solver.g</code> which stores the gradient.</p><p><strong>Variable that should not be modified:</strong> The callback functions <strong>should not modify x, s, and c</strong>.</p><p>Below is a table that recaps what variables each callback can/should update. |Callback| Modified Variables| | ––– | ––––––––- | |<code>compute_f_at_x!()</code>| <code>solver.f</code>, <code>solver.ωf</code>, <code>solver.π.πf</code> |<code>compute_f_at_c!()</code>| <code>solver.f⁺</code>, <code>solver.ωf⁺</code>, <code>solver.π.πf⁺</code> |<code>compute_g!()</code>| <code>solver.g</code>, <code>solver..ωg</code>, <code>solver.π.πg</code> |<code>recompute_g()</code>| <code>solver.g</code>, <code>solver..ωg</code>, <code>solver.π.πg</code>, <code>solver.ΔT</code>, <code>solver.π.ΔT</code>, <code>solver.x_norm</code>, <code>solver.π.πnx</code>, <code>solver.s_norm</code>, <code>solver.π.πns</code>, <code>solver.ϕ</code>, <code>solver.ϕhat</code>, <code>solver.π.πc</code>, <code>solver.μ</code></p><h3 id="**Multi-Precision-Evaluation-and-Vectors-Containers**"><a class="docs-heading-anchor" href="#**Multi-Precision-Evaluation-and-Vectors-Containers**"><strong>Multi-Precision Evaluation and Vectors Containers</strong></a><a id="**Multi-Precision-Evaluation-and-Vectors-Containers**-1"></a><a class="docs-heading-anchor-permalink" href="#**Multi-Precision-Evaluation-and-Vectors-Containers**" title="Permalink"></a></h3><p>MPR2 performs objective and gradient evaluations and error bounds estimation with different FP formats. These evaluations are performed with <code>FPMPNLPModels.objerrmp()</code> and <code>FPMPNLPModels.graderrmp!()</code> functions (see <code>FPMPNLPModels</code> documentation).</p><p class="math-container">\[x_k\]</p><p>, <span>$s_k$</span>, <span>$c_k$</span>, <span>$g_k$</span> are implemented as tuple of vectors of the FP formats used to perform evaluations (<em>i.e.</em> <code>FPMPNLPModel.FPList</code>), and are stored in the <code>MPR2Solver</code> structure given as argument of the callbacks. To evaluation the objective or the gradient in a given FP format, one has to call the appropriate function with the vector of the corresponding FP format, see example below</p><p><strong>Example 1: Simple Callback</strong></p><p>Here is a simple callback function for computing the objective function that does not take evaluation error into account.</p><pre><code class="language-julia hljs">using MultiPrecisionR2

function my_compute_f_at_c!(m::FPMPNLPModel, solver::MPR2Solver)
  solver.f⁺ = obj(m, solver.c[solver.π.πc]) # FP format m.FPList[π.πc] will be used for obj evaluation, since obj() is called with the correspond FP format vector.
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">my_compute_f_at_c! (generic function with 1 method)</code></pre><p>The implementation of MPR2 automatically updates the containers for x, s, c, and g during execution, so that the user does not have to deal with that part. The function <code>umpt!()</code> (see documentation) update the containers.</p><p><strong>Warning:</strong> <code>MultiPrecisionR2.umpt!(x::Tuple, y::Vector{S})</code> updates only the vectors of <code>x</code> of FP format with precision greater or equal to the FP format of y. <code>umpt!</code> is implemented this to avoid rounding error due to casting into a lower precision format and overflow. This is closely related to the concept of &quot;forbidden evaluation&quot; detailed in the <a href="#forbidden-evaluations">next section</a>. If the precision <code>solver.π.πx</code> == 2, it means that the <code>x[i]</code>s vectors are up-to-date for i&gt;=2. </p><p><strong>Example: Container Update</strong></p><pre><code class="language-julia hljs">using MultiPrecisionR2

FP = [Float16,Float32,Float64]
xini = ones(5)
x = Tuple(fp.(xini) for fp in FP)
xnew = Float32.(zeros(5))
umpt!(x,xnew)
x # only x[2] and x[3] are updated</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(Float16[1.0, 1.0, 1.0, 1.0, 1.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0])</code></pre><p><strong>Example: Lower Precision Casting Error</strong></p><pre><code class="language-julia hljs">x64 = [70000.0,1.000000001,0.000000001,-230000.0] # Vector{Float64}
x16 = Float16.(x64) # definitely not x64</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element Vector{Float16}:
  Inf
   1.0
   0.0
 -Inf</code></pre><h3 id="**Forbidden-Evaluations**"><a class="docs-heading-anchor" href="#**Forbidden-Evaluations**"><strong>Forbidden Evaluations</strong></a><a id="**Forbidden-Evaluations**-1"></a><a class="docs-heading-anchor-permalink" href="#**Forbidden-Evaluations**" title="Permalink"></a></h3><p>A &quot;forbidden evaluation&quot; consists in evaluating the objective or the gradient with a FP format lower than the FP format of the point where it is evaluated. For example, if <code>x</code> is a <code>Vector{Float64}</code>, evaluating the objective with <code>Float16</code> implicitly casts <code>x</code> into a <code>Vector{Float16}</code> and then evaluate the objective with this <code>Float16</code> vector. The problem is that due to rounding error, the cast vector is different than the initial <code>Float64</code> vector <code>x</code>. The objective is therefore not evaluated at <code>x</code> but at a different point. This causes numerical instability in MPR2 and must be avoided.</p><p><strong>Example: Forbidden Evaluation</strong></p><pre><code class="language-julia hljs">f(x) = 1/(x-10000) # objective
x64 = 10001.0
f(x64) # returns 1 as expected
x16 = Float16(x64) # rounding error occurs: x16 != x64
f(x16) # definitely not the expected value for f(x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Inf16</code></pre><p>When implementing the callback functions, the user should make sure that the evaluation precision selected for objective or gradient evaluation (<code>solver.π.πf</code>, <code>solver.π.πg</code>) is always greater or equal than the precision of the point where the evaluation is performed (<code>solver.π.πx</code> or <code>solver.π.πc</code>).</p><h3 id="**Step-and-Candidate-Computation-Precision**"><a class="docs-heading-anchor" href="#**Step-and-Candidate-Computation-Precision**"><strong>Step and Candidate Computation Precision</strong></a><a id="**Step-and-Candidate-Computation-Precision**-1"></a><a class="docs-heading-anchor-permalink" href="#**Step-and-Candidate-Computation-Precision**" title="Permalink"></a></h3><p>MPR2 implementation checks for possible under/overflow when computing the step <code>solver.s</code> and the candidate <code>solver.c</code> and increase FP format precision if necessary to avoid that. MPR2 implementation also updates <code>solver.π.πs</code> and <code>solver.π.πc</code> if necessary. These careful step and candidate computations are implemented in the <code>MultiPrecisionR2.ComputeStep!()</code> and <code>MultiPrecisionR2.computeCandidate!()</code> (see documentation). It is important to note that, even if the gradient <code>solver.g</code> has been computed with <code>solver.π.πg</code> precision, the precision <code>solver.π.πs</code> and <code>solver.π.πc</code> can be greater than <code>solver.π.πg</code> because overflow has occurred. As a consequence, the user should not take for granted than <code>solver.π.πc</code> == <code>solver.π.πg</code> when selecting FP format for objective/gradient evaluation at <code>solver.c</code> but must rely on the candidate precision <code>solver.π.πc</code>.</p><p><strong>Example: Step Overflow</strong></p><pre><code class="language-julia hljs">g(x) = 2*x # gradient
x16 = Float16(1000)
sigma = Float16(1/2^10) # regularization parameter
g16 = g(x16)
s = g16/Float16(sigma) # overflow
s = Float32(g16)/Float32(sigma) # no overflow, s is a Float32, this is what computeStep!() does</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2.048f6</code></pre><h3 id="**Candidate-Precision-Selection**"><a class="docs-heading-anchor" href="#**Candidate-Precision-Selection**"><strong>Candidate Precision Selection</strong></a><a id="**Candidate-Precision-Selection**-1"></a><a class="docs-heading-anchor-permalink" href="#**Candidate-Precision-Selection**" title="Permalink"></a></h3><p>In light of the &quot;forbidden evaluation&quot; concept (see section <a href="#forbidden-evaluations">Forbidden Evaluations</a>), if no particular care is given, the objective/gradient evaluation precision can only increase from one iteration to another. To illustrate that consider that </p><ol><li>At iteration <span>$k$</span>: <span>$x_k$</span> is Float32, meaning that <span>$\hat{g}(x_k)$</span> is Float32 (or higher precision FP format), but let&#39;s say Float32 here. By extension, <span>$s_k = -g_k/\sigma_k$</span> , and <span>$c_k = x_k+s_k$</span> are both Float32. It means that <span>$f(x_k)$</span> and <span>$f(x_k+s_k)$</span> are computed with Float32 or higher because of forbidden evaluations.</li><li>At iteration <span>$k+1$</span>:<ul><li>If the iteration is successful: then <span>$x_{k+1} = c_k$</span> is Float32, meaning again that <span>$\hat{g}(x_{k+1})$</span> is Float32 or higher precision format, and <span>$s_{k+1}$</span> and <span>$c_{k+1}$</span> are also Float32 or higher precision format. It further means that <span>$f(x_{k+1})$</span> and <span>$f(c_{k+1})$</span> can only be computed with Float32 or higher percision format.</li><li>If the iteration is unsuccessful: then <span>$x_{k+1} = x_k$</span> is Float32 and we have the same than if the iteration is successful.</li></ul></li></ol><p>To overcome this issue, and enable to decrease the FP formats used for objective/gradient evaluation, the user has the freedom to chose at iteration <span>$k$</span> the FP format of <span>$c_{k+1}$</span>. Indeed, there is no restriction on how the candidate is computed. Considering the above example, at iteration <span>$k+1$</span>, <span>$c_{k+1}$</span> is <code>Float32</code> but we can cast it (with rounding error) into a <code>Float16</code>, without breaking the convergence. Casting <span>$c_{k+1}$</span> allows to compute <span>$f(c_{k+1})$</span> with <code>Float16</code>, and if the iteration is successful, <span>$x_{k+2} = c_{k+1}$</span> is also a <code>Float16</code>, meaning that the gradient and objective can be computed with <code>Float16</code> at <span>$x_{k+2}$</span>.</p><p>The callback function <code>selectPif!()</code>, called at the end of the main loop (see section <a href="#minimal-implementation-description">Minimal Implementation Description</a>) update <code>solver.π.πc</code> so that <code>MPnlp.FPList[π.πc]</code> will be the FP format of <code>c</code> at the next iteration. The function <code>ComputeCandidate!()</code>, at the begining of the main loop, handles the casting of the candidate into <code>MPnlp.FPList[π.πc]</code>.</p><p>The expected template for <code>selectPif!()</code> callback function is <code>function selectPic!(π::MPR2Precisions)</code>. Only <code>π.πc</code> is expected to be modify.</p><table><tr><th style="text-align: right">Callback</th><th style="text-align: right">Modified Variables</th></tr><tr><td style="text-align: right"><code>selectPic!()</code></td><td style="text-align: right"><code>solver.π.πc</code></td></tr></table><h2 id="What-MultiPrecisionR2.solve!()-Handles"><a class="docs-heading-anchor" href="#What-MultiPrecisionR2.solve!()-Handles">What <code>MultiPrecisionR2.solve!()</code> Handles</a><a id="What-MultiPrecisionR2.solve!()-Handles-1"></a><a class="docs-heading-anchor-permalink" href="#What-MultiPrecisionR2.solve!()-Handles" title="Permalink"></a></h2><ul><li>Runs the main loop until a stopping condition is reached (max iteration or <span>$\|\nabla f(x)\| \leq \epsilon$</span>).</li><li>Uses the default callback functions for whichever has not been provided by the user.</li><li>Deals with error due to norm computation to ensure <span>$\|\nabla f(x)\| \leq \epsilon$</span>.</li><li>Deals with high precision format computation to simulate &quot;exactly computed&quot; values (see Basic Use tutorial).</li><li>Update the containers <code>solver.x</code>, <code>solver.s</code>, <code>solver.c</code> and <code>solver.g</code> (see <a href="#multi-precision-evaluation-and-vector-containers">Multi-Precision Evaluation and Vector Containers</a>)</li><li>Make sure no under/overflow occurs when computing <code>solver.s</code> and <code>solver.c</code>, update the precision <code>solver.π.πs</code> and <code>solver.π.πc</code> if necessary (see <a href="#step-and-candidate-computation-precision">Step and Candidate Computation Precision</a>)</li></ul><h2 id="What-MultiPrecisionR2.solve!()-Does-not-Handle"><a class="docs-heading-anchor" href="#What-MultiPrecisionR2.solve!()-Does-not-Handle">What <code>MultiPrecisionR2.solve!()</code> Does not Handle</a><a id="What-MultiPrecisionR2.solve!()-Does-not-Handle-1"></a><a class="docs-heading-anchor-permalink" href="#What-MultiPrecisionR2.solve!()-Does-not-Handle" title="Permalink"></a></h2><ul><li><code>solve!()</code> does not check that objective/gradient evaluation are preformed with a suitable FP format in the callback functions(see sections <a href="#multi-precision-evaluation-and-vector-containers">Multi-Precision Evaluation and Vector Containers</a> and <a href="#forbidden-evaluations">Forbidden Evaluation</a>).</li><li><code>solve!()</code> does not ensure convergence if the user uses its own callback functions.</li><li><code>solve!()</code>does not update objective/gradient values and precision outside the callback functions. See <a href="#callback-functions-expected-behavior">Callback Functions: Expected Behavior</a> for proper callback implementation.</li><li><code>solve!()</code> does not handle overflow that might occur when evaluating the objective/gradient in the callback functions. It is up to the user to make sure avoid overflows (if possible). See <a href="#implementation-examples">Implementation Examples</a> for dealing with overflow properly.</li></ul><h2 id="Callback-Functions-Cheat-Sheet"><a class="docs-heading-anchor" href="#Callback-Functions-Cheat-Sheet">Callback Functions Cheat Sheet</a><a id="Callback-Functions-Cheat-Sheet-1"></a><a class="docs-heading-anchor-permalink" href="#Callback-Functions-Cheat-Sheet" title="Permalink"></a></h2><table><tr><th style="text-align: right">Callback</th><th style="text-align: right">Description</th><th style="text-align: right">Outputs</th><th style="text-align: right">Expected Modified Variables</th></tr><tr><td style="text-align: right"><code>compute_f_at_x!()</code></td><td style="text-align: right">Select obj. FP format, compute <span>$f(x_k)$</span> and <span>$ωf(x_k)$</span></td><td style="text-align: right">success::Bool : <code>false</code> if <span>$\omega_f(x_k)$</span> is too big</td><td style="text-align: right"><code>solver.f</code>, <code>solver.ωf</code>, <code>solver.π.πf</code></td></tr><tr><td style="text-align: right"><code>compute_f_at_c!()</code></td><td style="text-align: right">Select obj. FP format and compute <span>$f(c_k)$</span> and <span>$ωf(c_k)$</span></td><td style="text-align: right">success::Bool : <code>false</code> if <span>$\omega_f(c_k)$</span> is too big</td><td style="text-align: right"><code>solver.f⁺</code>, <code>solver.ωf⁺</code>, <code>solver.π.πf⁺</code></td></tr><tr><td style="text-align: right"><code>compute_g!()</code></td><td style="text-align: right">Select grad FP format and compute <span>$g(c_k)$</span> and <span>$ωg(c_k)$</span></td><td style="text-align: right">success::Bool : <code>false</code> if <span>$\omega_g(c_k)$</span> is too big</td><td style="text-align: right"><code>solver.g</code>, <code>solver.ωg</code>, <code>solver.π.πg</code></td></tr><tr><td style="text-align: right"><code>recompute_g()</code></td><td style="text-align: right">Select grad FP format and recompute <span>$g(x_k)$</span> and <span>$ωg(x_k)$</span></td><td style="text-align: right">success::Bool : <code>false</code> if <span>$\omega_g(c_k)$</span> is too big, g<em>recompute::Bool: <code>true</code> if \hat{g}(x</em>k)$ was recomputed</td><td style="text-align: right"><code>solver.g</code>, <code>solver.ωg</code>, <code>solver.π.πg</code>, <code>solver.ΔT</code>, <code>solver.π.ΔT</code>, <code>solver.x_norm</code>, <code>solver.π.πnx</code>, <code>solver.s_norm</code>, <code>solver.π.πns</code>, <code>solver.ϕ</code>, <code>solver.ϕhat</code>, <code>solver.π.πc</code>, <code>solver.μ</code></td></tr><tr><td style="text-align: right"><code>selectPic!()</code></td><td style="text-align: right">Select candidate FP format for next iteration</td><td style="text-align: right">void</td><td style="text-align: right"><code>solver.π.πc</code></td></tr></table><h2 id="Implementation-Examples"><a class="docs-heading-anchor" href="#Implementation-Examples">Implementation Examples</a><a id="Implementation-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-Examples" title="Permalink"></a></h2><h3 id="Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)"><a class="docs-heading-anchor" href="#Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)">Example 1: Precision Selection Strategy Based on Step Size (Error Free)</a><a id="Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)-1"></a><a class="docs-heading-anchor-permalink" href="#Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)" title="Permalink"></a></h3><p>This example implements a precision selection strategy for the objective and gradient based on the norm of the step size, which does not take into account evaluation errors. The strategy is to choose the FP format for evaluation such that the norm of the step is greater than the square root of the unit roundoff.</p><p>The callback functions must handle precision selection for evaluations and optionally error/warning messages if evaluation fails (typically overflow or lack of precision)</p><pre><code class="language- hljs">using MultiPrecisionR2
using LinearAlgebra
using SolverCore

function my_compute_f_at_c!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)
  πmax = length(m.FPList) # get maximal allowed precision
  eval_prec = findfirst(u -&gt; sqrt(u) &lt; solver.s_norm, m.UList) # select precision according to the criterion
  if eval_prec === nothing # not enough precsion
    @warn &quot; not enough precision for objective evaluation at c: ||s|| = $(solver.s_norm) &lt; sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))&quot;
    return false
  end
  solver.π.πf⁺ = max(eval_prec,solver.π.πc) # evaluation precision should be greater or equal to the FP format of the candidate (see forbidden evaluation)
  solver.f⁺ = obj(m,solver.c[solver.π.πf⁺]) # eval objective only. solve!() made sure c[π.πf⁺] is up-to-date (see containers section)
  while isinf(solver.f⁺) # check for overflow
    solver.π.πf⁺ += 1
    if solver.π.πf⁺ &gt; πmax
      @warn &quot; not enough precision for objective evaluation at c: overflow&quot;
      return false # objective overflow with highest precision FP format: this is a fail
    end
    solver.f⁺ = obj(m,solver.c[solver.π.πf⁺])
  end
  return true
end
  
function my_compute_f_at_x!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)
  πmax = length(m.FPList) # get maximal allowed precision
  if solver.init # initial evaluation, step = 0, choose max precision
    solver.π.πf = πmax
  else # evaluation in main loop
    eval_prec = findfirst(u -&gt; sqrt(u) &lt; solver.s_norm, m.UList) # select precision according to the criterion
    if eval_prec === nothing # not enough precsion
      @warn &quot; not enough precision for objective evaluation at x: ||s|| = $(solver.s_norm) &lt; sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))&quot;
      return false
    end
    solver.π.πf = max(eval_prec,solver.π.πx) # evaluation precision should be greater or equal to the FP format of the current solution (see forbidden evaluation)
  end
  solver.f = obj(m,solver.x[solver.π.πf]) # eval objective only. solve!() made sure x[π.πf] is up-to-date (see containers section)
  while isinf(solver.f) # check for overflow
    solver.π.πf += 1
    if solver.π.πf &gt; πmax
      @warn &quot; not enough precision for objective evaluation at x: overflow&quot;
      return false # objective overflow with highest precision FP format: this is a fail
    end
    solver.f = obj(m,solver.x[solver.π.πf])
  end
  return true
end

function my_compute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)
  πmax = length(m.FPList) # get maximal allowed precision
  if solver.init # initial evaluation, step = 0, choose max precision
    solver.π.πg = πmax
  else # evaluation in main loop
    eval_prec = findfirst(u -&gt; sqrt(u) &lt; solver.s_norm, m.UList) # select precision according to the criterion
    if eval_prec === nothing # not enough precsion
      @warn &quot; not enough precision for gradient evaluation at c: ||s|| = $(solver.s_norm) &lt; sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))&quot;
      return false
    end
    solver.π.πg = max(eval_prec,solver.π.πg) # evaluation precision should be greater or equal to the FP format of the candidate (see forbidden evaluation)
  end
  grad!(m,solver.c[solver.π.πg],solver.g[solver.π.πg]) # eval gradient only. solve!() made sure x[π.πg] is up-to-date (see containers section)
  while findfirst(elem-&gt;isinf(elem),solver.g[solver.π.πg]) !== nothing # check for overflow, gradient vector version
    solver.π.πg += 1
    if solver.π.πg &gt; πmax
      @warn &quot; not enough precision for gradient evaluation at c: overflow&quot;
      return false # objective overflow with highest precision FP format: this is a fail
    end
    grad!(m,solver.c[solver.π.πg])
  end
  return true
end

function my_recompute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)
  # simply update norm of the step, since recompute_g!() is called at the begining of the main loop after step computation
  πmax = length(m.FPList) # get maximum precision index
  solver.π.πns = solver.π.πs # select precision for step norm computation
  s_norm = norm(solver.s[solver.π.πs])
  while isinf(s_norm)|| s_norm ==0.0 # handle possible over/underflow
    solver.π.πns = solver.π.πns+1 # increase precision to avoid over/underflow
    if solver.π.πns &gt; πmax
      return false, false # overflow occurs with max precion: cannot compute s_norm with provided FP formats. Returns fail. Gradient has not been recomputed, return false.
    end
    s_norm = norm(solver.s[solver.π.πns]) # compute norm with higher precision step, solve!() made sure s[π.πns] is up-to-date
  end
  solver.s_norm = s_norm
  return false, true # gradient not recomputed, successful evaluation
end</code></pre><p>Let&#39;s try this implementation on a simple quadratic objective.</p><pre><code class="language- hljs">FP = [Float16, Float32] # selected FP formats,
f(x) = x[1]^2 + x[2]^2 # objective function
omega = [0.0,0.0] # no evaluation error
x = Float32.(1.5*ones(2)) # initial point
MPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omega, ωgRelErr = omega); # indicates the use of relative error only to avoid interval evaluation, relative errors will not be computed with above callbacks
stat = MPR2(MPmodel;
compute_f_at_x! = my_compute_f_at_x!,
compute_f_at_c! = my_compute_f_at_c!,
compute_g! = my_compute_g!,
recompute_g! = my_recompute_g!);
stat  # first-order stationary point has been found</code></pre><p>Let&#39;s now try our implementation on the Rosenbrock function.</p><pre><code class="language- hljs">FP = [Float16, Float32] # selected FP formats,
f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
omega = [0.0,0.0]
x = Float32.(1.5*ones(2)) # initial point
MPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omega, ωgRelErr = omega); # indicates the use of relative error only to avoid interval evaluation, relative errors will not be computed with above callbacks
stat = MPR2(MPmodel;
compute_f_at_x! = my_compute_f_at_x!,
compute_f_at_c! = my_compute_f_at_c!,
compute_g! = my_compute_g!,
recompute_g! = my_recompute_g!); # throw lack of precision warning</code></pre><p>The strategy implemented for precision selection does not allow to find a first-order critical point for the Rosenbrock function: the step becomes too small before MPR2 converges. Although this implementation is fast since it does not bother with evaluation errors, it is not very satisfactory since the Example in section &quot;Lack of Precision&quot; in the Basic tutorial shows that the default implementation is able to converge to a first-order critical point. This highlights that it is important to understand how rounding errors occur and affect the convergence of the algorithm (see MPR2 algorithm description in Basic Use tutorial) and that &quot;naive&quot; strategies like the one implemented above might not be satisfactory.</p><h3 id="Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision"><a class="docs-heading-anchor" href="#Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision">Example 2: Switching to Gradient Descent When Lacking Objective Precision</a><a id="Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision-1"></a><a class="docs-heading-anchor-permalink" href="#Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision" title="Permalink"></a></h3><p>It might happen that <code>solve!()</code> stops early because the objective evaluation lacks precision. Consider for example that we use consider relative evaluation error for the objective. If MPR2 converges to a point where the objective is big, the error can be big too, and if the gradient is small the convergence condition <span>$\omega f(x_k) \leq \eta_0 \Delta T_k = \|\hat{g}(x_k)\|^2/\sigma_k$</span> is likely to fail. In that case, the user might want to continue running the algorithm without caring about the objective, that is, as a simple gradient descent. <code>solve!()</code> implementation allows enough flexibility to do so. In the implementation below, the user defined structure <code>e</code> is used to indicate what &quot;mode&quot; the algorithm is running: default mode or gradient descent. The callbacks <code>compute_f_at_x!</code> sets <code>st.f = Inf</code> and <code>compute_f_at_c!</code> sets <code>st.f⁺ = 0</code> if gradient descent mode is used. This ensures that <span>$\rho_k = Inf \geq \eta_1$</span> and the step is accepted in gradient descent mode. In the implementation below, <code>compute_f_at_x!</code> and <code>compute_f_at_c!</code> selects the precision such that <span>$\omega f(x_k) \leq \eta_0 \Delta T_k$</span> in default mode. We implement <code>compute_g!</code> to set <code>σ</code> so that <code>ComputeStep!()</code> will use the learning rate <code>1/σ</code>. We use the default <code>recompute_g</code> callback.</p><pre><code class="language- hljs">using MultiPrecisionR2
using LinearAlgebra
using SolverCore

mutable struct my_struct
  gdmode::Bool
  learning_rate
end

function my_compute_f_at_c!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)
  if !e.gdmode # classic mode
    ωfBound = solver.p.η₀*solver.ΔT
    solver.π.πf⁺ = solver.π.πx # basic precision selection strategy
    solver.f⁺, solver.ωf⁺, solver.π.πf⁺ = objReachPrec(m, solver.c, ωfBound, π = solver.π.πf⁺)
    if isinf(solver.f⁺) # stop algo if objective overflow
      @warn &quot;Objective evaluation overflow at x&quot;
      return false
    end
    if solver.ωf⁺ &gt; ωfBound # evaluation error too big
      @warn &quot;Objective evaluation error at x too big to ensure convergence: switching to gradient descent&quot;
      e.gdmode = true
      solver.f⁺ = 0
      return true
    end
  else # gradient descent mode
    solver.f⁺ = 0.0
  end
  return true
end

function my_compute_f_at_x!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)
  πmax = length(m.EpsList)
  if solver.init # initial evaluation before main loop
    solver.f, solver.ωf, solver.π.πf = objReachPrec(m, solver.x, m.OFList[end], π = solver.π.πf)
  else # evaluation in the main loop
    if !e.gdmode
      ωfBound = solver.p.η₀*solver.ΔT
      if solver.ωf &gt; ωfBound # need to reevaluate the objective at x
        if solver.π.πf == πmax # already at highest precision 
          @warn &quot;Objective evaluation error at x too big to ensure convergence: switching to gradient descent&quot;
          e.gdmode = true
          solver.f = Inf
          return true
        end
        solver.π.πf += 1 # increase evaluation precision of f at x
        solver.f, solver.ωf, solver.π.πf = objReachPrec(m, solver.x, ωfBound, π = solver.π.πf)
        if isinf(solver.f) # stop algo if objective overflow
          @warn &quot;Objective evaluation overflow at x&quot;
          return false
        end
        if solver.ωf &gt; ωfBound # error evaluation too big with max precison
          @warn &quot;Objective evaluation error at x too big to ensure convergence: switching to gradient descent&quot;
          e.gdmode = true
          solver.f = Inf
          return true
        end
      end
    else # gradient descent mode
      solver.f = Inf
    end
  end
  return true
end

function my_compute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)
  solver.π.πg = solver.π.πc # default strategy, could be a callback
  solver.ωg, solver.π.πg = gradReachPrec!(m, solver.c, solver.g, m.OFList[end], π = solver.π.πg)
  if e.gdmode
    solver.σ = 1/e.learning_rate
  end
  return true
end</code></pre><p>Let us first run <code>MPR2()</code> with the default implementation and relative evaluation error.</p><pre><code class="language-julia hljs">FP = [Float16, Float32] # selected FP formats,
#f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
f(x) = x[1]^2 + x[2]^2 +0.5
omegaf = Float64.([0.01,0.005])
omegag = Float64.([0.05,0.01])
x = Float32.(1.5*ones(2)) # initial point
MPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omegaf, ωgRelErr = omegag);
stat = MPR2(MPmodel,verbose=1); # stops at iteration 3, throw lack of precision warning</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;Execution stats: unhandled exception&quot;</code></pre><p>We run <code>MPR2()</code> with the callback functions defined above and the default callbacks for <code>compute_g!()</code> and <code>recompute_g!()</code>. We use relative objective and gradient error.</p><pre><code class="language- hljs">FP = [Float16, Float32] # selected FP formats,
#f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
f(x) = x[1]^2 + x[2]^2 +0.5
omegaf = Float64.([0.01,0.005])
omegag = Float64.([0.05,0.01])
x = ones(Float32,2) # initial point
MPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omegaf, ωgRelErr = omegag);
e = my_struct(false,1e-2)
stat = MPR2(MPmodel;
e = e,
compute_f_at_x! = my_compute_f_at_x!,
compute_f_at_c! = my_compute_f_at_c!,
compute_g! = my_compute_g!); # switch to gradient descent at iteration 3, converges to first order critical point</code></pre></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 27 July 2023 20:43">Thursday 27 July 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
