<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MPR2 Tutorial: Basic Use  · MultiPrecisionR2.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="/tutorial_MPR2_basic_use/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="MultiPrecisionR2.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultiPrecisionR2.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../reference/">Reference</a></li><li><a class="tocitem" href="../MPCounters/">MPCounters</a></li><li><a class="tocitem" href="../FPMPNLPModel/">FPMPNLPModel</a></li><li><a class="tocitem" href="../MultiPrecisionR2/">MultiPrecisionR2</a></li><li><a class="tocitem" href="../tutorial_FPMPNLPModel/">FPMPNLPModel Tutorial</a></li><li class="is-active"><a class="tocitem" href>MPR2 Tutorial: Basic Use </a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Motivation"><span>Motivation</span></a></li><li class="toplevel"><a class="tocitem" href="#MPR2-Algorithm:-General-Description-and-Basic-Use"><span>MPR2 Algorithm: General Description and Basic Use</span></a></li><li><a class="tocitem" href="#**Notations**"><span><strong>Notations</strong></span></a></li><li><a class="tocitem" href="#**MPR2-Algorithm-Broad-Description**-(differs-from-package-implementation)"><span><strong>MPR2 Algorithm Broad Description</strong> (differs from package implementation)</span></a></li><li><a class="tocitem" href="#**Rounding-Errors-Handling**"><span><strong>Rounding Errors Handling</strong></span></a></li><li><a class="tocitem" href="#Conditions-on-Parameters"><span>Conditions on Parameters</span></a></li><li class="toplevel"><a class="tocitem" href="#Basic-Use"><span>Basic Use</span></a></li><li><a class="tocitem" href="#**MPR2-Solver**"><span><strong>MPR2 Solver</strong></span></a></li><li><a class="tocitem" href="#**Evaluation-Error-Modes**"><span><strong>Evaluation Error Modes</strong></span></a></li><li><a class="tocitem" href="#**High-Precision-Format**"><span><strong>High Precision Format</strong></span></a></li><li><a class="tocitem" href="#**Gamma-Function**"><span><strong>Gamma Function</strong></span></a></li><li><a class="tocitem" href="#**Lack-of-Precision**"><span><strong>Lack of Precision</strong></span></a></li></ul></li><li><a class="tocitem" href="../tutorial_MPR2_advanced_use/">MPR2 Tutorial: Advanced Use </a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>MPR2 Tutorial: Basic Use </a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>MPR2 Tutorial: Basic Use </a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/MultiPrecisionR2" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Disclaimer"><a class="docs-heading-anchor" href="#Disclaimer">Disclaimer</a><a id="Disclaimer-1"></a><a class="docs-heading-anchor-permalink" href="#Disclaimer" title="Permalink"></a></h1><p>The reader is encouraged to read the <code>FPMPNLPModel</code> tutorial before the present one. </p><h1 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h1><p>Here is the comparison between <code>R2</code> algorithm from <code>JSOSolvers.jl</code> run with <code>Float64</code> and <code>MPR2</code> from <code>MultiPrecisionR2.jl</code> run with <code>Float16</code>, <code>Float32</code> and <code>Float64</code> over a set of unconstrained problems taken from <code>OptimizationProblems.jl</code>. The time and energy savings offered by MPR2 compared with R2 is estimated with the rule of thumb:</p><p>Number of bits divided by two <span>$\implies$</span> time computation divided by two and energy consumption divided by four.</p><pre><code class="language- hljs">using MultiPrecisionR2
using NLPModels
using ADNLPModels
using OptimizationProblems
using OptimizationProblems.ADNLPProblems
using JSOSolvers
using Quadmath

FP = [Float16,Float32,Float64] # MPR2 Floating Point formats
omega = Float128.([sqrt(eps(Float16)),sqrt(eps(Float32)),0]) # MPR2 relative errors, computations assumed exact with Float64
r2_obj_eval = [0]
r2_grad_eval = [0]
mpr2_obj_eval = zeros(Float64,length(FP))
mpr2_grad_eval = zeros(Float64,length(FP))
nvar = 99 #problem dimension (if scalable)
max_iter = 1000

meta = OptimizationProblems.meta
names_pb_vars = meta[(meta.has_bounds .== false) .&amp; (meta.ncon .== 0), [:nvar, :name]] #select unconstrained problems
for pb in eachrow(names_pb_vars)
  nlp = eval(Meta.parse(&quot;ADNLPProblems.$(pb[:name])(n=$nvar,type=Val(Float64),backend = :generic)&quot;))
  mpmodel = FPMPNLPModel(nlp,FP,HPFormat = Float128, ωfRelErr=omega, ωgRelErr=omega);
  statr2 = R2(nlp,max_eval=max_iter)
  r2_obj_eval .+= nlp.counters.neval_obj
  r2_grad_eval .+= nlp.counters.neval_grad
  statmpr2 = MPR2(mpmodel,max_iter = max_iter)
  mpr2_obj_eval .+= [haskey(mpmodel.counters.neval_obj,fp) ? mpmodel.counters.neval_obj[fp] : 0 for fp in FP]
  mpr2_grad_eval .+= [haskey(mpmodel.counters.neval_grad,fp) ? mpmodel.counters.neval_grad[fp] : 0 for fp in FP]
end
mpr2_obj_time = sum(mpr2_obj_eval.*[1/4,1/2,1])
obj_time_save = mpr2_obj_time/r2_obj_eval[1]
mpr2_obj_energy = sum(mpr2_obj_eval.*[1/16,1/4,1])
obj_energy_save = mpr2_obj_energy/r2_obj_eval[1]
mpr2_grad_time = sum(mpr2_grad_eval.*[1/4,1/2,1])
grad_time_save = mpr2_grad_time/r2_grad_eval[1]
mpr2_grad_energy = sum(mpr2_grad_eval.*[1/16,1/4,1])
grad_energy_save = mpr2_grad_energy/r2_grad_eval[1]
println(&quot;Possible time saving for objective evaluation with MPR2: $(round((1-obj_time_save)*100,digits=1)) %&quot;)
println(&quot;Possible time saving for gradient evaluation with MPR2: $(round((1-grad_time_save)*100,digits=1)) %&quot;)
println(&quot;Possible energy saving for objective evaluation with MPR2: $(round((1-obj_energy_save)*100,digits=1)) %&quot;)
println(&quot;Possible energy saving for gradient evaluation with MPR2: $(round((1-grad_energy_save)*100,digits=1)) %&quot;)</code></pre><h1 id="MPR2-Algorithm:-General-Description-and-Basic-Use"><a class="docs-heading-anchor" href="#MPR2-Algorithm:-General-Description-and-Basic-Use">MPR2 Algorithm: General Description and Basic Use</a><a id="MPR2-Algorithm:-General-Description-and-Basic-Use-1"></a><a class="docs-heading-anchor-permalink" href="#MPR2-Algorithm:-General-Description-and-Basic-Use" title="Permalink"></a></h1><h2 id="**Notations**"><a class="docs-heading-anchor" href="#**Notations**"><strong>Notations</strong></a><a id="**Notations**-1"></a><a class="docs-heading-anchor-permalink" href="#**Notations**" title="Permalink"></a></h2><ul><li>FPList: List of floating point formats available</li><li><p class="math-container">\[fl\]</p>: finite-precision computation</li><li><p class="math-container">\[u\]</p>: unit round-off for a given FP format</li><li><p class="math-container">\[\delta\]</p>: rounding error induced by one FP operation (<span>$+,-,/,*$</span>), for example <span>$fl(x+y) = (x+y)(1+\delta)$</span>. Bounded as <span>$|\delta|\leq u$</span>.</li><li><p class="math-container">\[\pi\]</p>: FP format index in FPList, also called <strong>precision</strong></li><li><p class="math-container">\[f: x \rightarrow f(x)\]</p>: objective function</li><li><p class="math-container">\[\hat{f}: x,\pi_f \rightarrow fl(f(x,\pi_f))\]</p>: finite precision counterpart of <span>$f$</span> with FP format corresponding to index <span>$\pi_f$</span> in FPList  </li><li><p class="math-container">\[\nabla f: x \rightarrow \nabla f(x)\]</p>: gradient of <span>$f$</span></li><li><p class="math-container">\[\hat{g}: x, \pi_g \rightarrow fl(\nabla f(x),\pi_g)\]</p>: finite precision counterpart of <span>$\nabla f $ with FP format corresponding to index $\pi_g$</span> in FPList</li><li><p class="math-container">\[\omega_f(x)\]</p>: bound on finite precision evaluation error of <span>$f$</span>, <span>$| \hat{f}(x,\pi_f) -f(x) | \leq \omega_f(x)$</span></li><li><p class="math-container">\[\omega_g(x)\]</p>: bound on finite precision evaluation error of <span>$\nabla f$</span>, <span>$\| \hat{g}(x,\pi_g) -\nabla f(x) \| \leq \omega_g(x)\|\hat{g}(x,\pi_g)\|$</span></li></ul><h2 id="**MPR2-Algorithm-Broad-Description**-(differs-from-package-implementation)"><a class="docs-heading-anchor" href="#**MPR2-Algorithm-Broad-Description**-(differs-from-package-implementation)"><strong>MPR2 Algorithm Broad Description</strong> (differs from package implementation)</a><a id="**MPR2-Algorithm-Broad-Description**-(differs-from-package-implementation)-1"></a><a class="docs-heading-anchor-permalink" href="#**MPR2-Algorithm-Broad-Description**-(differs-from-package-implementation)" title="Permalink"></a></h2><p>MPR2 is described by the following algorithm. Note that the actual implementation in the package differs slightly. For an overview of the actual implementation, see Advanced Use tutorial.</p><p>In the algorithm, <em>compute</em> means compute with finite-precision machine computation, and <em>define</em> means &quot;compute exactly&quot;, <em>i.e.</em> with infinite precision. Defining values is necessary to ensure the theoretical convergence of MPR2. Defining a value is not possible to perform on a (finite-precision) machine, but one can use high precision FP formats (see section <a href="#high-precision-format">High Precision Format</a>)</p><p><strong>Inputs</strong>: </p><ul><li>Initial values: <span>$x_0$</span>, <span>$\sigma_0$</span></li><li>Tunable Parameters: <span>$0 &lt; \eta_0 &lt; \eta_1 &lt; \eta_2 &lt; 1$</span>, <span>$0 &lt; \gamma_1 &lt; 1 &lt; \gamma_2$</span>, <span>$\kappa_m$</span></li><li>Gradient tolerance: <span>$\epsilon$</span></li><li>List of FP formats (<em>e.g</em> [Float16, Float32, Float64])</li></ul><p><strong>Outputs</strong></p><ul><li><p class="math-container">\[x_k\]</p>such that <span>$\nabla f(x_k) \leq \epsilon \|\nabla f(x_0)\|$</span></li></ul><p><strong>Initialization</strong></p><ol><li>Compute <span>$f_0 = \hat{f}(x_0,\pi_f)$</span></li><li>Compute <span>$g_0 = \hat{g}(x_0,\pi_g)$</span></li></ol><p><strong>While</strong> <span>$\|g_k\| &gt; \dfrac{\epsilon}{1+\omega_g(x_k)}$</span> <strong>do</strong></p><ol><li>Compute <span>$s_k = -g_k/\sigma_k$</span></li><li>Compute <span>$c_k = x_k + s_k$</span></li><li>Compute <span>$\Delta T_k = g_k^Ts_k$</span> # model reduction</li><li>Define <span>$\mu_k$</span> # gradient error indicator</li></ol><ul><li><p><strong>If</strong> <span>$\mu_k \geq \kappa_m$</span></p><ol><li>Select precisions such that <span>$\mu_k \leq \kappa_m$</span>, go to 3.</li></ol></li><li><p><strong>End If</strong></p></li><li><p><strong>If</strong> <span>$\omega_f(x_k) &gt; \eta_0 \Delta T_k$</span></p><ol><li>Select <span>$\pi_f$</span> such that <span>$\omega_f(x_k) \leq \eta_0 \Delta T_k$</span></li><li>Compute <span>$f_k = f(x_k,\pi_f)$</span></li></ol></li><li><p><strong>End If</strong></p></li></ul><ol><li>Select <span>$\pi_f^+$</span> such that <span>$\omega(c_k) \leq \eta_0 \Delta T_k$</span></li><li>Compute <span>$f_k^+ = \hat{f}(c_k,\pi_f^+)$</span></li><li>Compute <span>$\rho_k = \dfrac{f_k - f_k^+}{\Delta T_k}$</span></li></ol><ul><li><p><strong>If</strong> <span>$\rho_k \geq \eta_1$</span> # step acceptance</p><ol><li><p class="math-container">\[x_{k+1} = c_k\]</p>, <span>$f_{k+1} = f_k^+$</span></li></ol></li><li><p><strong>End If</strong></p></li></ul><ol><li>Compute $ \sigma_k = \left{ \begin{array}{lll}</li></ol><p>\gamma<em>1 \sigma</em>k &amp; \text{if} &amp; \rho<em>k \geq \eta</em>2 \
    \sigma<em>k &amp; \text{if} &amp; \rho</em>k \in  \eta<em>1,\eta</em>2  \
    \gamma<em>2 \sigma</em>k &amp; \text{if} &amp; \rho<em>k &lt; \eta</em>1   \end{array}    \right.$</p><p><strong>End While</strong>  </p><ol><li>return <span>$x_k$</span></li></ol><p>MPR2 stops either when:</p><ol><li>a point <span>$x_k$</span> satisfying <span>$\|g_k\| \leq \dfrac{\epsilon}{1+\omega_g(x_k)}$</span> has been found (see <a href="#notations"><span>$\omega_g$</span> definition</a>), which ensures that $ \|\nabla f(x_k)\| \leq \epsilon$, or </li><li>no FP format enables to achieve required precision on the objective or <span>$\mu$</span> indicator.</li></ol><p>The indicator <strong><span>$\mu_k$</span></strong> aggregates finite-precision errors due to gradient evaluation (<span>$\omega_g(x_k)$</span>), and the computation of the step (<span>$s_k$</span>), candidate (<span>$c_k$</span>) and model reduction <span>$\Delta T_k$</span> as detailed in the <a href="#rounding-error-handling">Rounding Error Handling</a> section.     </p><h2 id="**Rounding-Errors-Handling**"><a class="docs-heading-anchor" href="#**Rounding-Errors-Handling**"><strong>Rounding Errors Handling</strong></a><a id="**Rounding-Errors-Handling**-1"></a><a class="docs-heading-anchor-permalink" href="#**Rounding-Errors-Handling**" title="Permalink"></a></h2><p>Anything computed in MPR2 suffers from rounding errors since it runs on a machine, which necessarily performs computation with finite-precision arithmetic. Below is the list of rounding errors that are handled by MPR2 such that convergence and numerical stability is guaranteed. MPR2 is designed to run with FP numbers complying with the IEEE 754 norm.</p><ol><li><strong>Dot Product Error: <span>$\gamma_n$</span></strong></li></ol><p>The model for the dot product error that is used by default is     <span>$|fl(x.y) - x.y| \leq |x|.|y| \gamma(n,u),$</span> with     <span>$\gamma: n,u \rightarrow n*u$</span>     where <span>$x$</span> and <span>$y$</span> are two FP vectors (same FP format) of dimension <span>$n$</span> and <span>$u$</span> is the round-off unit of the FP format. This is a crude yet guaranteed upper bound on the error. The <span>$\gamma$</span> function is embedded in the <code>FPMPNLPModel</code> structure as a callback (see documentation and tutorial), such that the user can choose its own formula.</p><ol><li><strong>Candidate Computation</strong> Both the step and the candidate are computed inexactly.<ul><li>Inexact Step: the computed step is <span>$s_k = fl(g_k/\sigma_k) = (g_k/\sigma_k)(1+\delta) \neq g_k/\sigma_k$</span>.</li><li>Inexact Candidate: the computed candidate is <span>$c_k = fl(x_k+s_k) = (x_k+s_k)(1+\delta) \neq x_k+s_k$</span>.</li></ul>These two errors implies that <span>$c_k$</span> is not along the descent direction <span>$g_k$</span> and as such can be interpreted as additional gradient error to  <span>$\omega_g(x_k)$</span>. To handle these errors, MPR2 computes the ratio <span>$\phi_k = \|x_k\|/||s_k||$</span>. The greater <span>$\phi_k$</span>, the greater the possible deviation of <span>$c_k$</span> from the direction <span>$g_k$</span>. These errors are aggregated in the <span>$\mu_k$</span> indicator (detailed in 5.).</li><li><strong>Model Decrease Computation</strong></li></ol><p>The FP computation error for the model decrease <span>$\Delta T_k$</span> is such that,     <span>$fl(\Delta T_k) =  \Delta T_k (1+\vartheta_n)$</span>, with <span>$|\vartheta_n| \leq \gamma(n,u)$</span>     MPR2 handles this error by aggregating <span>$\alpha(n,u)$</span> in the indicator <span>$\mu_k$</span>, with <span>$\alpha(n,u) = \dfrac{1}{1-\gamma(n,u)}$</span></p><ol><li><strong>Norm Computation</strong>  </li></ol><p>The finite-precision error for norm computation of a FP vector <span>$x$</span> of dimension <span>$n$</span> is     $ |fl(\|x\|)-\|x\|| \leq fl(\|x\|)\beta(n+2,u)$ with     <span>$\beta:n,u \rightarrow \max(|\sqrt{\gamma(n,u)-1}-1|,|\sqrt{\gamma(n,u)+1}-1|)$</span>     <span>$\beta$</span> function cannot be chosen by the user, but <span>$\gamma(n,u)$</span> can be chosen via <code>γfunc</code> keyword argument when instanciating a <code>FPMPNLPModel</code> (see next sections).   The <span>$\beta$</span> function is used in MPR2 implementation to handle finite-precision error of norm computation for </p><ul><li><p class="math-container">\[\|g_k\|\]</p>norm computation: the stopping criterion implemented in MPR2 is   <span>$\|g_k\| \leq \dfrac{1}{1+\beta (n+2,u)}\dfrac{1}{1+\omega_g(x_k)}$</span> which ensures that <span>$\|\nabla f(x_k)\|\leq \epsilon$</span>. </li><li><p class="math-container">\[\phi_k\]</p>ratio computation (see 3.): <span>$\phi_k$</span> requires the norm of <span>$x_k$</span> and <span>$s_k$</span>. MPR2 actually defines </li></ul><p><span>$\phi_k =  \dfrac{fl(\|x_k\|)}{fl(\|x_k\|)}\dfrac{1+\beta(n+2,u)}{1-\beta(n+2,u)}$</span> which ensures that <span>$\phi_k \geq \|x_k\| / \|s_k\|$</span>.</p><ol><li><strong>Mu Indicator</strong>  </li></ol><p><span>$\mu_k$</span> is an indicator which aggregates</p><ul><li>the gradient evaluation error <span>$\omega_g(x_k)$</span>,</li><li>the inexact step and candidate errors (<span>$\phi_k$</span>),</li><li>the model decrease error (<span>$\gamma(n+1,u)$</span>, <span>$\alpha(n,u)$</span>).  </li></ul><p>The formula for <span>$\mu_k$</span> is     <span>$\mu_{k} = \dfrac{\alpha(n,u) \omega_g(x_k)(1+u(\phi_k +1)) + \alpha(n,u) \lambda_k + u+ \gamma(n+1,u)\alpha(n+1,u)}{1-u}$</span>.     Note that if no rounding error occurs for (<span>$u = 0$</span>), one simply has <span>$\mu_k = \omega_g(x_k)$</span>.     The implementation of line 7. of MPR2 (as described in the <a href="#mpr2-algorithm-broad-description-differs-from-package-implementation">above section</a>) consists in recomputing the step, candidate, <span>$x_k$</span> and/or <span>$s_k$</span> norm, model decrease or gradient with higher precision FP formats (therefore decreasing <span>$u$</span>) until <span>$\mu_k \leq \kappa_m$</span>. For details about default strategy, see <code>recomputeMu!</code> documentation.</p><h2 id="Conditions-on-Parameters"><a class="docs-heading-anchor" href="#Conditions-on-Parameters">Conditions on Parameters</a><a id="Conditions-on-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Conditions-on-Parameters" title="Permalink"></a></h2><p>MPR2 parameters can be chosen by the user (see Section <a href="#basic-use">Basic Use</a>) but must satisfy the following inequalities to ensure convergence:</p><ul><li><p class="math-container">\[0 \leq \eta_0 \leq \frac{1}{2}\eta_1\]</p></li><li><p class="math-container">\[0 \leq \eta_1 \leq \eta_2 &lt; 1\]</p></li><li><p class="math-container">\[\eta_0+\dfrac{\kappa_m}{2} \leq 0.5(1-\eta_2)\]</p></li><li><p class="math-container">\[\eta_2 &lt; 1\]</p></li><li><p class="math-container">\[0&lt;\gamma_1&lt;1&lt;\gamma_2\]</p></li></ul><h1 id="Basic-Use"><a class="docs-heading-anchor" href="#Basic-Use">Basic Use</a><a id="Basic-Use-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Use" title="Permalink"></a></h1><p>MPR2 solver relies on multi-precision models structure <code>FPMPNLPModels</code> (Floating Point Multi Precision Non-Linear Programming Models), that derives from <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl"><code>NLPModels.jl</code></a>. This structure embeds the problem and provides the interfaces to evaluate the objective and the gradient with several FP formats and provide error bounds <span>$\omega_f$</span> and <span>$\omega_g$</span>. For details about <code>FPMPNLPModels</code>, see related documentation and tutorial.</p><h2 id="**MPR2-Solver**"><a class="docs-heading-anchor" href="#**MPR2-Solver**"><strong>MPR2 Solver</strong></a><a id="**MPR2-Solver**-1"></a><a class="docs-heading-anchor-permalink" href="#**MPR2-Solver**" title="Permalink"></a></h2><p>MPR2 solver is run with <code>MPR2()</code> function which takes a <code>FPMPNLPModel</code> argument (see <code>FPMPNLPModel</code> documentation for details).</p><pre><code class="language-julia hljs">using MultiPrecisionR2

T = [Float16,Float32] # defines FP format used for evaluations
f(x) = sum(x.^2) # objective function
x = ones(Float32,2) # initial point
mpnlp = FPMPNLPModel(f,x,T) # instanciate a FPMPNLPModel
stats = MPR2(mpnlp) # runs MPR2</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;Execution stats: unhandled exception&quot;</code></pre><p><code>MPR2()</code> returns a <code>GenericExectutionStats</code> structure that contains information about the execution status. See <a href="https://github.com/JuliaSmoothOptimizers/SolverCore.jl">SolverCore.jl</a> package for details.</p><pre><code class="language-julia hljs">stats.iter # access number of iteration
stats.solution # accsess solution computed by MPR2
stats.dual_feas # access gradient norm at the solution</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.4142135f0</code></pre><h2 id="**Evaluation-Error-Modes**"><a class="docs-heading-anchor" href="#**Evaluation-Error-Modes**"><strong>Evaluation Error Modes</strong></a><a id="**Evaluation-Error-Modes**-1"></a><a class="docs-heading-anchor-permalink" href="#**Evaluation-Error-Modes**" title="Permalink"></a></h2><p>MPR2 can be run with interval or relative error mode for objective and gradient evaluation error estimation. This error mode is chosen when instanciating the <code>FMPMNLPModel</code>.</p><p><strong>Example</strong>: Interval Error Mode</p><pre><code class="language- hljs">using MultiPrecisionR2
using IntervalArithmetic # need this to call setrounding function

T = [Float16,Float32] 
setrounding(Interval,:accurate) # need this since Float16 is used, see warnings in the README
f(x) = sum(x.^2)
x = ones(Float32,2)
mpnlp = FPMPNLPModel(f,x,T,obj_int_eval = true, grad_err_eval = true) # instanciate a FPMPNLPModel, interval evaluation will be used for error estimation
MPR2(mpnlp) # runs MPR2 with interval estimation of the evaluation errors</code></pre><p><strong>Example</strong>: Relative Error Mode</p><pre><code class="language-julia hljs">using MultiPrecisionR2

T = [Float16,Float32]
f(x) = sum(x.^2)
omega = [0.01,0.001] # 1% and 0.1% relative error for Float16 and Float32 evaluations
x = ones(Float32,2)
mpnlp = FPMPNLPModel(f, x, T; ωfRelErr = omega, ωgRelErr = omega) # instanciate a FPMPNLPModel, relative error model will be used for error estimation
MPR2(mpnlp) # runs MPR2 with relative error model estimation</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;Execution stats: unhandled exception&quot;</code></pre><h2 id="**High-Precision-Format**"><a class="docs-heading-anchor" href="#**High-Precision-Format**"><strong>High Precision Format</strong></a><a id="**High-Precision-Format**-1"></a><a class="docs-heading-anchor-permalink" href="#**High-Precision-Format**" title="Permalink"></a></h2><p>MPR2 uses a high precision format to compute &quot;exactly&quot; the values that are <em>defined</em> (see section <a href="#mpr2-algorithm-broad-description-differs-from-package-implementation">MPR2 Algorithm Broad Description</a>). This high precision format corresponds to the type parameter <code>H</code> in <code>MPR2()</code> template. The high precision format used by MPR2 is <code>MPnlp::FPMPNLPModel</code>&#39;s <code>H</code> parameter type which can be chosen upon <code>MPnlp</code> instantiation (see <code>FPMPNLPModel</code> documentation).</p><pre><code class="language- hljs">using MultiPrecisionR2
using Quadmath

HPFormat = Float128
T = [Float32,Float64]
f(x) = sum(x.^2)
x = ones(Float32,2)
mpnlp = FPMPNLPModel(f,x,T;HPFormat = HPFormat) # instanciate a FPMPNLPModel with Float64 HPFormat
MPR2(mpnlp) # runs MPR2 with Float128 to compute &quot;define&quot; values</code></pre><h2 id="**Gamma-Function**"><a class="docs-heading-anchor" href="#**Gamma-Function**"><strong>Gamma Function</strong></a><a id="**Gamma-Function**-1"></a><a class="docs-heading-anchor-permalink" href="#**Gamma-Function**" title="Permalink"></a></h2><p>MPR2 relies on the dot product error function <span>$\gamma$</span> to handle some rounding errors for model decrease and norm computation (see <a href="#mpr2-algorithm-broad-description-differs-from-package-implementation">MPR2 Broad Description</a>). The <span>$\gamma$</span> function used by MPR2 is the one of the <code>MPR2()</code>&#39;s <code>MPnlp::FMPNLPModel</code> argument and can be chosen by the user (see <code>FPMPNLPModel</code> documentation and tutorial). The default <span>$\gamma$</span> function used is <span>$\gamma(n,u) = n*u$</span> with <span>$n$</span> the dimension of the problem and <span>$u$</span> the unit-roundoff of the FP format used for computation.</p><pre><code class="language-julia hljs">using MultiPrecisionR2

HPFormat = Float64
gamma(n,u) = HPFormat(sqrt(n)*u) # user-defined gamma funciton. Value returned  must be ::HPFormat
Formats = [Float32]
f(x) = sum(x.^2)
x0 = ones(Float32,1000)
mpmodel_ud = FPMPNLPModel(f,x0,Formats; γfunc = gamma) # build mpmodel with user defined gamma function
stats_ud = MPR2(mpmodel_ud) # run MPR2 with user define gamma function for model decrease and norm computation</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;Execution stats: unhandled exception&quot;</code></pre><h2 id="**Lack-of-Precision**"><a class="docs-heading-anchor" href="#**Lack-of-Precision**"><strong>Lack of Precision</strong></a><a id="**Lack-of-Precision**-1"></a><a class="docs-heading-anchor-permalink" href="#**Lack-of-Precision**" title="Permalink"></a></h2><p>The default implementation of MPR2 stops when the condition on the objective evaluation error or the <span>$\mu$</span> indicator fails with the highest precision evaluations (see Lines 7,8,10 of MPR2 algorithm in section <a href="#mpr2-algorithm-broad-description-differs-from-package-implementation">MPR2 Algorithm Broad Description</a>). If this happens, MPR2 returns the ad-hoc warning message. The user can tune MPR2 parameters to try to avoid such early stop via the structure <code>MPR2Params</code> and provide it as a keyword argument to <code>solve!</code>. Typically, </p><ul><li><strong>If the objective error is too big</strong>: the user should increase <span>$\eta_0$</span> parameter.</li><li><strong>If <span>$\mu_k$</span> is too big</strong>: the user should increase <span>$\kappa_m$</span> parameter.</li></ul><p>The user has to make sure that the parameters respect the convergence conditions (see section <a href="#conditions-on-parameters">Conditions on Parameters</a>).</p><p><strong>Example</strong>: Lack of Precision and Parameters Selection</p><pre><code class="language- hljs">using MultiPrecisionR2
using IntervalArithmetic

setrounding(Interval,:accurate)
FP = [Float16, Float32] # selected FP formats, max eval precision is Float64
f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
x = Float32.(1.5*ones(2)) # initial point
HPFormat = Float64
MPmodel = FPMPNLPModel(f,x,FP,HPFormat = HPFormat,obj_int_eval = true, grad_int_eval = true);
stat = MPR2(MPmodel) </code></pre><p>Running the above code block returns a warning indicating that R2 stops because the error on the objective function is too big to ensure convergence. The problem can be overcome in this example by tolerating more error on the objective by increasing <span>$\eta_0$</span>.</p><pre><code class="language- hljs">η₀ = 0.1 # greater than default value 0.01
η₁ = 0.3
η₂ = 0.7
κₘ = 0.1
γ₁ = Float16(1/2) # must be FP format of lowest evaluation precision for numerical stability
γ₂ = Float16(2) # must be FP format of lowest evaluation precision for numerical stability
param = MPR2Params(η₀,η₁,η₂,κₘ,γ₁,γ₂)
stat = MPR2(MPmodel,par = param,max_iter = 10000,obj_int_eval = true, grad_int_eval = true) </code></pre><p>Now MPR2 converges to a first order critical point since we tolerate enough error on the objective evaluation.</p><p><strong>Example</strong>: Avoiding Lack of Precision With Relative Errors</p><p>If the <code>FPMPNLPModel</code> model is instantiated with relative error model (see <code>FPMPNLPModel</code> documentation), one can simply set the error to zero with the highest FP format used for evaluation.  This avoids the algorithm to stop due to lack of precision, but MPR2 is not guaranteed to converge to a first order critical point.</p><pre><code class="language-julia hljs">using MultiPrecisionR2

FP = [Float16, Float32] # selected FP formats, max eval precision is Float64
f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
x = Float32.(1.5*ones(2)) # initial point
HPFormat = Float64
omega = [0.01,0.0] # 1% error with Float16, no error with Float32
MPmodel = FPMPNLPModel(f,x,FP; ωfRelErr = omega, ωgRelErr = omega, HPFormat = HPFormat);
stat = MPR2(MPmodel)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;Execution stats: unhandled exception&quot;</code></pre><p>Not that even if evaluation error for objective and gradient is set to zero as in the above code, early stop due to lack of precision might still occur since <span>$\mu$</span> indicator is not null even when gradient error <span>$\omega_g$</span> is null (see <a href="#mpr2-algorithm-broad-description-differs-from-package-implementation">MPR2 description section</a>).</p><p><strong>Example</strong>: Decreasing Error Bounds for Norm and Model Decrease</p><p>If early stop due to lack of precision occurs even when modifying MPR2&#39;s parameters and setting the evaluation error to zero (above 2 examples), the user can provide its own function <span>$\gamma$</span> to further decrease <span>$\mu$</span> indicator (by decreasing <span>$\gamma(n,u)$</span> and <span>$\alpha(n,u)$</span>). For example, the user can simply set <span>$\gamma$</span> to 0, i.e. supposing that model reduction and norm computation are performed exactly. This however might increase numerical instability of MPR2.</p><pre><code class="language-julia hljs">using MultiPrecisionR2

FP = [Float16, Float32] # selected FP formats, max eval precision is Float64
f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
x = Float32.(1.5*ones(2)) # initial point
HPFormat = Float64
omega = [0.01,0.0] # 1% error with Float16, no error with Float32
gamma(n,u) = HPFormat(0) # gamma function set to zero
MPmodel = FPMPNLPModel(f,x,FP; ωfRelErr = omega, ωgRelErr = omega, γfunc = gamma , HPFormat = HPFormat);
stat = MPR2(MPmodel)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;Execution stats: unhandled exception&quot;</code></pre><p><strong>Example</strong>: Letting MPR2 Run Despite Numerical Instability: <code>run_free</code> Option</p><p>The user can choose to let MPR2 run even when precision lacks with the maximum of precision levels have been reached for the objective and gradient evaluations and intermediate values computation (<span>$\phi$</span>, model decrease, ...). This can be done by providing <code>MPR2</code> keyword argument <code>run_free = true</code>, which default value is <code>false</code>.</p><pre><code class="language-julia hljs">using MultiPrecisionR2

FP = [Float16, Float32] # selected FP formats, max eval precision is Float64
f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
x = Float32.(1.5*ones(2)) # initial point
HPFormat = Float64
MPmodel = FPMPNLPModel(f,x,FP);
stat = MPR2(MPmodel,verbose = 1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;Execution stats: unhandled exception&quot;</code></pre><h3 id="**Evaluation-Counters**"><a class="docs-heading-anchor" href="#**Evaluation-Counters**"><strong>Evaluation Counters</strong></a><a id="**Evaluation-Counters**-1"></a><a class="docs-heading-anchor-permalink" href="#**Evaluation-Counters**" title="Permalink"></a></h3><p>MPR2 counts the number of objective and gradient evaluations are counted for each FP formats. They are stored in <code>counters</code> field of the <code>FPNLPModel</code> structure. The <code>counters</code> field is a <code>MPCounters</code>.</p><pre><code class="language-julia hljs">using MultiPrecisionR2

FP = [Float16, Float32, Float64]
f(x) = sum(x.^2)
x0 = ones(10)
HPFormat = Float64
MPmodel = FPMPNLPModel(f,x0,FP,HPFormat = HPFormat);
MPR2(MPmodel,verbose=1)
MPmodel.counters.neval_obj # numbers of objective evaluations
MPmodel.counters.neval_grad # numbers of gradient evaluations</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Dict{DataType, Int64} with 3 entries:
  Float16 =&gt; 0
  Float64 =&gt; 2
  Float32 =&gt; 0</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tutorial_FPMPNLPModel/">« FPMPNLPModel Tutorial</a><a class="docs-footer-nextpage" href="../tutorial_MPR2_advanced_use/">MPR2 Tutorial: Advanced Use  »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Tuesday 15 August 2023 18:31">Tuesday 15 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
