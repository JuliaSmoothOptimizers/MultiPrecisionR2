<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Disclaimer · MultiPrecisionR2.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="/tutorial_MPR2_advanced_use/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="MultiPrecisionR2.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultiPrecisionR2.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Disclaimer</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Disclaimer</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/MultiPrecisionR2/blob/main/docs/src/tutorial_MPR2_advanced_use.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Disclaimer"><a class="docs-heading-anchor" href="#Disclaimer">Disclaimer</a><a id="Disclaimer-1"></a><a class="docs-heading-anchor-permalink" href="#Disclaimer" title="Permalink"></a></h1><p>The reader is encouraged to read the <code>FPMPNLPModel</code> tutorial before this one.</p><h1 id="Advanced-Use"><a class="docs-heading-anchor" href="#Advanced-Use">Advanced Use</a><a id="Advanced-Use-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Use" title="Permalink"></a></h1><p><code>MultiPrecisionR2.jl</code> does more than implementing MPR2 algorithm as describes in the Basic Use tutorial. <code>MPR2Precision.jl</code> enables the user to define its own strategy to select evaluation precision levels and to handle evaluation errors. This is made possible by using callback functions when calling <code>MultiPrecisionR2.solve!()</code>. The default implementation of <code>MultiPrecisionR2.solve!()</code> relies on specific implementation of these callback functions, which are included in the package. The user is free to provide its own callback functions to change the behavior of the algorithm. </p><h2 id="Diving-into-MPR2-Implementation"><a class="docs-heading-anchor" href="#Diving-into-MPR2-Implementation">Diving into MPR2 Implementation</a><a id="Diving-into-MPR2-Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Diving-into-MPR2-Implementation" title="Permalink"></a></h2><p><code>MPR2Precision.jl</code> relies on callback functions that handle the objective and gradient evaluation. These callback functions are expected to compute values for the objective and the gradient and handle the evaluation precision. In the default implementation, the conditions on the error bound of objective and gradient evaluation are dealt with in these callback functions. That is why such convergence conditions (which user might choose not to implement) does not appear in the minimal implementation description of the code below.</p><h3 id="**Minimal-Implementation-Description**"><a class="docs-heading-anchor" href="#**Minimal-Implementation-Description**"><strong>Minimal Implementation Description</strong></a><a id="**Minimal-Implementation-Description**-1"></a><a class="docs-heading-anchor-permalink" href="#**Minimal-Implementation-Description**" title="Permalink"></a></h3><p>Here is a minimal description of the implementation of <code>MultiPrecisionR2.solve!()</code> to understand when and why these functions are called. Please refer to the implementation for details. Note that the callback functions templates are not respected for the sake of explanation.</p><pre><code class="language-julia hljs">function solve!(solver::MPR2solver{T},MPnlp::FPMPNLPModel{H};kwargs...)
  # ...
  # some initialization (parameters, containers,...)
  # ...
  compute_f_at_x!() # initialize objective value at x0
  # check for possible overflow 
  compute_g!() # initialize gradient value at x0
  # check for possible overflow
  # ...
  # some more initialization (gradient norm tolerance,...)
  # ...
  while # main loop
    # compute step as sk = -gk/σk
    # check for step overflow
    # compute candidate as c = x+s
    # check for candidate overflow
    # compute model decrease ΔT = -g^T s
    # check for model decrease under/overflow
    recompute_g!() # possibly recompute g(x) and ωg(x) if gradient error/mu indicator is too big
    if grad_prec_fail # not enough precision with max precision
      break
    end
    if grad_recomputed# gradient recomputed by recompute_g!() 
      # recompute step with new value of the gradient
      # check overflow
      # recompute candidate with new step
      # check overflow
      # recompute model decrease with new gradient and step
    end
    compute_f_at_x() # possibly recompute f(x) and ωf(x) if ωf(x) is too big
    if f_x_prec_fail# not enough precision with max precision (ωf(x) too big)
      break
    end
    compute_f_at_c!() # compute f(c) and ωf(c)
    if f_c_prec_fail # not enough precision with max precision (ωf(c) too big)
      break
    end
    # ...
    # compute ρ = (f(x) - f(c))/ΔT
    # update x and σ
    # ... 
    if ρ ≥ η₁
      compute_g!() # compute g(c) and  ωg(c)
      # check overflow
      # ...
      # update values for next iteration
      # ...
      selectPic!()
    end
    # ...
    # check for termination
    # ...
  end # while main loop
  return status()
end</code></pre><p>The callback functions, and what they are supposed to do, are </p><ul><li><code>compute_f_at_x!()</code>: <strong>Selects evaluation precision</strong> and <strong>computes objective function at the current point <span>$x_k$</span></strong> such that possible conditions on error bounds are ensured. In the main loop, although the objective at <span>$x_k$</span> has already been computed at a previous iteration (<span>$f(c_{j}) = f(x_k)$</span> with <span>$j$</span> the last successful iteration) it might be necessary to recompute it to achieve smaller evaluation error (<span>$\omega_f(x_k)$</span>) if necessary. This function is also <strong>called for initialization</strong> (before the main loop), where typically no bound on evaluation error is required.</li><li><code>compute_f_at_c!()</code>: <strong>Selects evaluation precision</strong> and <strong>computes objective function at the candidate <span>$c_k$</span></strong> such that possible conditions on error bounds are ensured.</li><li><code>compute_g!()</code>: <strong>Selects evaluation precision</strong> and <strong>computes gradient at the candidate <span>$c_k$</span></strong>. It is possible here to include condiditions on gradient bound error <span>$\omega_g(x_k)$</span>. In the default implementation, multiple sources of rounding error are taken into account in the <span>$\mu$</span> indicator that requires to compute the step, candidate and model decrease (see <a href="#rounding-errors-handling">Rounding Errors Handling</a>). That is why in the default implementation no conditions on gradient error are required in this callback function, but are implemented in <code>recompute_g!()</code>.</li><li><code>recompute_g!()</code>: <strong>Selects evaluation precision</strong> and <strong>recomputes gradient at the current point <span>$x_k$</span></strong>. This callback enables to recompute the gradient <strong>if necessary</strong> with a better precision if needed. In the default implementation, this callback implement the condition on the <span>$\mu$</span> indicator (see <a href="#rounding-errors-handling">Rouding Error Handling</a>) and implements strategies to achieve sufficiently low value of <span>$\mu$</span> to ensure convergence.</li><li><code>selectPic!()</code>: Select FP format for <span>$c_{k+1}$</span>, enables to lower the FP format used for evaluation at the next iteration. See <a href="#candidate-precision-selection">Candidate Precision Selection</a> section for details.</li></ul><h3 id="**Callback-Functions:-Templates**"><a class="docs-heading-anchor" href="#**Callback-Functions:-Templates**"><strong>Callback Functions: Templates</strong></a><a id="**Callback-Functions:-Templates**-1"></a><a class="docs-heading-anchor-permalink" href="#**Callback-Functions:-Templates**" title="Permalink"></a></h3><ul><li><code>prec_fail::Bool = compute_f_at_c!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T) where {H, L, E, T &lt;: Tuple}</code></li><li><code>prec_fail::Bool = compute_f_at_x_default!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T) where {H, L, E, T &lt;: Tuple}</code></li><li><code>prec_fail::Bool = compute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T, g::T) where {H, L, E, T &lt;: Tuple}</code></li><li><code>prec_fail::Bool, recompute_g::Bool = recompute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T, g::T, s::T) where {H, L, E, T &lt;: Tuple}</code></li></ul><p><strong>Arguments</strong>:</p><ul><li><code>m:FPMPNLPModel{H}</code>: multi-precision model, needed to perform objective/gradient evaluation</li><li><code>st::MPR2State{H}</code>: States or intermediate variables of MPR2 algorithm (<span>$\rho$</span>, <span>$\mu$</span>, ...), see <code>MPR2State</code> documentation</li><li><code>π::MPR2Precisions</code>: Current precision indices, see <code>MPR2Precisions</code> documentation</li><li><code>p::MPR2Params{H, L}</code>: algorithm parameters, see <code>MPR2Params</code> documentation</li><li><code>e::E</code>: user defined structure to store additional information if needed</li><li><code>x::T</code>: current point</li><li><code>s::T</code>: current step</li><li><code>c::T</code>: current candidate</li><li><code>g::T</code>: current gradient</li></ul><p><strong>Outputs</strong></p><ul><li><code>prec_fail::Bool</code>: <code>true</code> if the evaluation failed, typical reason is that not enough evaluation could be reached. If <code>prec_fail == true</code>, stops the algorithm and set <code>st.status = :exception</code>.</li><li><code>recompute_g::Bool</code>: <code>true</code> if <code>recompute_g!()</code> has recomputed the gradient. In this case, the set, candidate and model decrease are recomputed (see <a href="#minimal-implementation-description">Minimal Implementation Description</a>)</li></ul><h3 id="**Callback-Functions:-Expected-Behaviors**"><a class="docs-heading-anchor" href="#**Callback-Functions:-Expected-Behaviors**"><strong>Callback Functions: Expected Behaviors</strong></a><a id="**Callback-Functions:-Expected-Behaviors**-1"></a><a class="docs-heading-anchor-permalink" href="#**Callback-Functions:-Expected-Behaviors**" title="Permalink"></a></h3><p><strong>Modified Variables:</strong> The callback functions are expected to update all the variables that they modify. These variables are typically MPR2 states in <code>st::MPR2State{H}</code> structure, the current precisions in <code>π::MPR2Precisions</code> structure, extra values in user defined <code>e::E</code> structure. The callback functions <code>compute_g!()</code> and <code>recompute_g!()</code> are also expected to modify the gradient <code>g::T</code>.</p><p><strong>Variable that should not be modified:</strong> The callback functions <strong>should not modify x, s, and c</strong>.</p><p>Below is a table that recaps what variables each callback can/should update. |Callback| Modified Variables| | ––– | ––––––––- | |<code>compute_f_at_x!()</code>| <code>st.status</code>, <code>st.f</code>, <code>st.ωf</code>, <code>π.πf</code> |<code>compute_f_at_c!()</code>| <code>st.status</code>, <code>st.f⁺</code>, <code>st.ωf⁺</code>, <code>π.πf⁺</code> |<code>compute_g!()</code>| <code>st.status</code>, <code>g</code>, <code>st.ωg</code>, <code>π.πg</code> |<code>recompute_g()</code>| <code>st.status</code>, <code>g</code>, <code>st.ωg</code>, <code>π.πg</code>, <code>st.ΔT</code>, <code>π.ΔT</code>, <code>st.x_norm</code>, <code>π.πnx</code>, <code>st.s_norm</code>, <code>π.πns</code>, <code>st.ϕ</code>, <code>st.ϕhat</code>, <code>π.πc</code>, <code>st.μ</code></p><h3 id="**Multi-Precision-Evaluation-and-Vectors-Containers**"><a class="docs-heading-anchor" href="#**Multi-Precision-Evaluation-and-Vectors-Containers**"><strong>Multi-Precision Evaluation and Vectors Containers</strong></a><a id="**Multi-Precision-Evaluation-and-Vectors-Containers**-1"></a><a class="docs-heading-anchor-permalink" href="#**Multi-Precision-Evaluation-and-Vectors-Containers**" title="Permalink"></a></h3><p>MPR2 performs objective and gradient evaluations and error bounds estimation with different FP format. These evaluations are performed with <code>FPMPNLPModels.objerrmp()</code> and <code>FPMPNLPModels.graderrmp!()</code> functions (see <code>FPMPNLPModels</code> documentation and <a href="#fpmpnlpmodel-creating-a-multi-precision-model">FPMPNLPModel section</a>). These functions expect as input a <code>Vector{S}</code> where to evaluate the objective/gradient where <code>S</code> is the FP format that matches <code>FPMPNLPModel.FPList[id]</code> the FP format of index <code>id</code>.  That is, it is not possible to perform an evaluation in an FP format different than the FP format of the input vector. This further means that, when MPR2 is running, it is necessary to change the FP format of <span>$x_k$</span>, <span>$s_k$</span>, <span>$c_k$</span>, and <span>$g_k$</span>.   In practice, <span>$x_k$</span>, <span>$s_k$</span>, <span>$c_k$</span>, <span>$g_k$</span> are implemented as tuple of vectors of the FP formats used to perform evaluations (<em>i.e.</em> <code>FPMPNLPModel.FPList</code>).</p><p><strong>Example 1: Simple Callback</strong> Here is a simple callback function for computing the objective function that does not take evaluation error into account.</p><pre><code class="language-julia hljs">function my_compute_f_at_c!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T) where {H, L, E, T &lt;: Tuple}
  st.f⁺ = objmp(m, c[π.πc]) # FP format m.FPList[π.πc] will be used for obj evaluation.
end</code></pre><p>The implementation of MPR2 automatically updates the containers for x, s, c, and g during execution, so that the user does not have to deal with that part. The function <code>MultiPrecisionR2.umpt!()</code> (see documentation) update the containers.</p><p><strong>Warning:</strong> <code>MultiPrecisionR2.umpt!(x::Tuple, y::Vector{S})</code> updates only the vectors of x of FP format with precision greater or equal to the FP format of y. <code>umpt!</code> is implemented this to avoid rounding error due to casting into a lower precision format and overflow. This is closely related to the concept of &quot;forbidden evaluation&quot; detailed in the <a href="#forbidden-evaluations">next section</a>. If the precision <code>π.πx</code> = 2, it means that the <code>x[i]</code>s vectors are up-to-date for i&gt;=2. </p><p><strong>Example: Container Update</strong></p><pre><code class="language-julia hljs">using MultiPrecisionR2
FP = [Float16,Float32,Float64]
xini = ones(5)
x = Tuple(fp.(xini) for fp in FP)
xnew = Float32.(zeros(5))
umpt!(x,xnew)
x # only x[2] and x[3] are updated</code></pre><p><strong>Example: Lower Precision Casting Error</strong></p><pre><code class="language-julia hljs">x64 = [70000.0,1.000000001,0.000000001,-230000.0] # Vector{Float64}
x16 = Float16.(x64) # definitely not x64</code></pre><h3 id="**Forbidden-Evaluations**"><a class="docs-heading-anchor" href="#**Forbidden-Evaluations**"><strong>Forbidden Evaluations</strong></a><a id="**Forbidden-Evaluations**-1"></a><a class="docs-heading-anchor-permalink" href="#**Forbidden-Evaluations**" title="Permalink"></a></h3><p>A &quot;forbidden evaluation&quot; consists in evaluating the objective or the gradient with a FP format lower than the FP format of the point where it is evaluated. For example, if <code>x</code> is a <code>Vector{Float64}</code>, evaluating the objective in Float16 will implicitely cast <code>x</code> into a <code>Vector{Float16}</code> and then evaluate the objective with this Float16 vector. The problem is that due to rounding error, the casted vector is different than the initial Float64 <code>x</code>. The objective is therefore not evaluated at <code>x</code> but at a different point. This causes numerical instability in MPR2 and must be avoided.</p><p><strong>Example: Forbidden Evaluation</strong></p><pre><code class="language-julia hljs">f(x) = 1/(x-10000) # objective
x64 = 10001.0
f(x64) # returns 1 as expected
x16 = Float16(x64) # rounding error occurs: x16 != x64
f(x16) # definitely not the expected value for f(x)</code></pre><p>When implementing the callback functions, the user should make sure that the evaluation precision selected for objective or gradient evaluation (<code>π.πf</code>, <code>π.πg</code>) is always greater or equal than the precision of the point where the evaluation is performed (<code>π.πx</code> or <code>π.πc</code>).</p><h3 id="**Step-and-Candidate-Computation-Precision**"><a class="docs-heading-anchor" href="#**Step-and-Candidate-Computation-Precision**"><strong>Step and Candidate Computation Precision</strong></a><a id="**Step-and-Candidate-Computation-Precision**-1"></a><a class="docs-heading-anchor-permalink" href="#**Step-and-Candidate-Computation-Precision**" title="Permalink"></a></h3><p>MPR2 implementation checks for possible under/overflow when computing the step <code>s</code> and the candidate <code>c</code> and increase FP format precision if necessary to avoid that. MPR2 implementation also updates <code>π.πs</code> and <code>π.πc</code> if necessary. These careful step and candidate computations are implemented in the <code>MultiPrecisionR2.ComputeStep!()</code> and <code>MultiPrecisionR2.computeCandidate!()</code> (see documentation). It is important to note that, even if the gradient <code>g</code> has been computed with <code>π.πg</code> precision, the precision <code>π.πs</code> and <code>π.πc</code> can be greater than <code>π.πg</code> because overflow has occurred. Has a consequence, the user should not take for granted than <code>π.πc</code> == <code>π.πg</code> when selecting FP format for objective/gradient evaluation at <code>c</code> but must rely on the candidate precision <code>π.πc</code>.</p><p><strong>Example: Step Overflow</strong></p><pre><code class="language-julia hljs">g(x) = 2*x # gradient
x16 = Float16(1000)
sigma = Float16(1/2^10) # regularization parameter
g16 = g(x16)
s = g16/Float16(sigma) # overflow
s = Float32(g16)/Float32(sigma) # no overflow, s is a Float32, this is what computeStep!() does</code></pre><h3 id="**Candidate-Precision-Selection**"><a class="docs-heading-anchor" href="#**Candidate-Precision-Selection**"><strong>Candidate Precision Selection</strong></a><a id="**Candidate-Precision-Selection**-1"></a><a class="docs-heading-anchor-permalink" href="#**Candidate-Precision-Selection**" title="Permalink"></a></h3><p>In light of the &quot;forbidden evaluation&quot; concept (see section <a href="#forbidden-evaluations">Forbidden Evaluations</a>), if no particular care is given, the objective/gradient evaluation precisions can only increase from one iteration to another. To illustrate that consider that </p><ol><li>At iteration <span>$k$</span>: <span>$x_k$</span> is Float32, meaning that <span>$\hat{g}(x_k)$</span> is Float32 (or higher precision FP format), but let&#39;s say Float32 here. By extension, <span>$s_k = -g_k/\sigma_k$</span> , and <span>$c_k = x_k+s_k$</span> are both Float32. It means that <span>$f(x_k)$</span> and <span>$f(x_k+s_k)$</span> are computed with Float32 or higher because of forbidden evaluations.</li><li>At iteration <span>$k+1$</span>:<ul><li>If the iteration is successful: then <span>$x_{k+1} = c_k$</span> is Float32, meaning again that <span>$\hat{g}(x_{k+1})$</span> is Float32 or higher precision format, and <span>$s_{k+1}$</span> and <span>$c_{k+1}$</span> are also Float32 or higher precision format. It further means that <span>$f(x_{k+1})$</span> and <span>$f(c_{k+1})$</span> can only be computed with Float32 or higher percision format.</li><li>If the iteration is unsuccessful: then <span>$x_{k+1} = x_k$</span> is Float32 and we have the same than if the iteration is successful.</li></ul></li></ol><p>To overcome this issue, and enable to decrease the FP formats used for objective/gradient evaluation, the user has the freedom to chose at iteration <span>$k$</span> the FP format of <span>$c_{k+1}$</span>. Indeed, there is no restriction on how the candidate is computed. Considering the above example, at iteration <span>$k+1$</span> <span>$c_{k+1}$</span> is Float32 but we can cast it (with rounding error) into a Float16, without breaking the convergence. Casting <span>$c_{k+1}$</span> allows to compute <span>$f(c_{k+1})$</span> with Float16, and if the iteration is successful, <span>$x_{k+2} = c_{k+1}$</span> is also a Float16, meaning that the gradient and objective can be computed with Float16 at <span>$x_{k+2}$</span>.</p><p>The callback function <code>selectPif!()</code>, called at the end of the main loop (see section <a href="#minimal-implementation-description">Minimal Implementation Description</a>) update <code>π.πc</code> so that <code>MPnlp.FPList[π.πc]</code> will be the FP format of <code>c</code> at the next iteration. The function <code>ComputeCandidate!()</code>, at the begining of the main loop, handles the casting of the candidate into <code>MPnlp.FPList[π.πc]</code>.</p><p>The expected template for <code>selectPif!()</code> callback function is <code>function selectPic!(π::MPR2Precisions)</code>. Only <code>π.πc</code> is expected to be modify.</p><table><tr><th style="text-align: right">Callback</th><th style="text-align: right">Modified Variables</th></tr><tr><td style="text-align: right"><code>selectPic!()</code></td><td style="text-align: right"><code>π.πc</code></td></tr></table><h2 id="What-MultiPrecisionR2.solve!()-Handles"><a class="docs-heading-anchor" href="#What-MultiPrecisionR2.solve!()-Handles">What <code>MultiPrecisionR2.solve!()</code> Handles</a><a id="What-MultiPrecisionR2.solve!()-Handles-1"></a><a class="docs-heading-anchor-permalink" href="#What-MultiPrecisionR2.solve!()-Handles" title="Permalink"></a></h2><ul><li>Runs the main loop until a stopping condition is reached (max iteration or <span>$\|\nabla f(x)\| \leq \epsilon$</span>).</li><li>Uses the default callback functions for whichever has not been provided by the user.</li><li>Deals with error due to norm computation to ensure <span>$\|\nabla f(x)\| \leq \epsilon$</span>.</li><li>Deals with high precision format computation to simulate &quot;exactly computed&quot; values (see sections <a href="#high-precision-format">High Precision Format</a> and <a href="#high-precision-format-mpr2-solver">High Precision Format: MPR2 Solver</a>).</li><li>Update the containers <code>x</code>, <code>s</code>, <code>c</code> and <code>g</code> (see <a href="#multi-precision-evaluation-and-vector-containers">Multi-Precision Evaluation and Vector Containers</a>)</li><li>Make sure no under/overflow occurs when computing <code>s</code> and <code>c</code>, update the precision <code>π.πs</code> and <code>π.πc</code> if necessary (see <a href="#step-and-candidate-computation-precision">Step and Candidate Computation Precision</a>)</li></ul><h2 id="What-MultiPrecisionR2.solve!()-Does-not-Handle"><a class="docs-heading-anchor" href="#What-MultiPrecisionR2.solve!()-Does-not-Handle">What <code>MultiPrecisionR2.solve!()</code> Does not Handle</a><a id="What-MultiPrecisionR2.solve!()-Does-not-Handle-1"></a><a class="docs-heading-anchor-permalink" href="#What-MultiPrecisionR2.solve!()-Does-not-Handle" title="Permalink"></a></h2><ul><li><code>solve!()</code> does not check that objective/gradient evaluation are preformed with a suitable FP format in the callback functions(see sections <a href="#multi-precision-evaluation-and-vector-containers">Multi-Precision Evaluation and Vector Containers</a> and <a href="#forbidden-evaluations">Forbidden Evaluation</a>).</li><li><code>solve!()</code> does not ensure convergence if the user uses its own callback functions.</li><li><code>solve!()</code>does not update objective/gradient values and precision outside the callback functions. See <a href="#callback-functions-expected-behavior">Callback Functions: Expected Behavior</a> for proper callbacks implementation.</li><li><code>solve!()</code> does not handle overflow that might occur when evaluation the objective/gradient in the callback functions. It is up to the user to make sure no overflowed value is returned by the callbacks. See <a href="#implementation-examples">Implementation Examples</a> for dealing with overflow properly.</li><li><code>solve!()</code> does not throw error/warning if evaluations in callback have failed. It is up to the user to handle that.</li></ul><h2 id="Callback-Functions-Cheat-Sheet"><a class="docs-heading-anchor" href="#Callback-Functions-Cheat-Sheet">Callback Functions Cheat Sheet</a><a id="Callback-Functions-Cheat-Sheet-1"></a><a class="docs-heading-anchor-permalink" href="#Callback-Functions-Cheat-Sheet" title="Permalink"></a></h2><table><tr><th style="text-align: right">Callback</th><th style="text-align: right">Description</th><th style="text-align: right">Outputs</th><th style="text-align: right">Expected Modified Variables</th></tr><tr><td style="text-align: right"><code>compute_f_at_x!()</code></td><td style="text-align: right">Select obj. FP format, compute <span>$f(x_k)$</span> and <span>$ωf(x_k)$</span></td><td style="text-align: right">prec<em>fail::Bool : <code>true</code> if \omega</em>f(x_k)$ is too big, stops main loop</td><td style="text-align: right"><code>st.status</code>, <code>st.f</code>, <code>st.ωf</code>, <code>π.πf</code></td></tr><tr><td style="text-align: right"><code>compute_f_at_c!()</code></td><td style="text-align: right">Select obj. FP format and compute <span>$f(c_k)$</span> and <span>$ωf(c_k)$</span></td><td style="text-align: right">prec<em>fail::Bool: <code>true</code> if \omega</em>f(c_k)$ is too big, stops main loop</td><td style="text-align: right"><code>st.status</code>, <code>st.f⁺</code>, <code>st.ωf⁺</code>, <code>π.πf⁺</code></td></tr><tr><td style="text-align: right"><code>compute_g!()</code></td><td style="text-align: right">Select grad FP format and compute <span>$g(c_k)$</span> and <span>$ωg(c_k)$</span></td><td style="text-align: right">prec<em>fail::Bool: <code>true</code> if \omega</em>g(c_k)$ is too big, stops main loop</td><td style="text-align: right"><code>st.status</code>, <code>g</code>, <code>st.ωg</code>, <code>π.πg</code></td></tr><tr><td style="text-align: right"><code>recompute_g()</code></td><td style="text-align: right">Select grad FP format and recompute <span>$g(x_k)$</span> and <span>$ωg(x_k)$</span></td><td style="text-align: right">prec<em>fail::Bool: <code>true</code> if \omega</em>g(c<em>k)$ is too big, stops main loop, g</em>recompute::Bool: <code>true</code> if <span>$\hat{g}(x_k)$</span> was recomputed</td><td style="text-align: right"><code>st.status</code>, <code>g</code>, <code>st.ωg</code>, <code>π.πg</code>, <code>st.ΔT</code>, <code>π.ΔT</code>, <code>st.x_norm</code>, <code>π.πnx</code>, <code>st.s_norm</code>, <code>π.πns</code>, <code>st.ϕ</code>, <code>st.ϕhat</code>, <code>π.πc</code>, <code>st.μ</code></td></tr><tr><td style="text-align: right"><code>selectPic!()</code></td><td style="text-align: right"></td><td style="text-align: right">void</td><td style="text-align: right"><code>π.πc</code></td></tr></table><h2 id="Implementation-Examples"><a class="docs-heading-anchor" href="#Implementation-Examples">Implementation Examples</a><a id="Implementation-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-Examples" title="Permalink"></a></h2><h3 id="Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)"><a class="docs-heading-anchor" href="#Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)">Example 1: Precision Selection Strategy Based on Step Size (Error Free)</a><a id="Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)-1"></a><a class="docs-heading-anchor-permalink" href="#Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)" title="Permalink"></a></h3><p>This example implements a precision selection strategy for the objective and gradient based on the norm of the step size, which does not take into account evaluation errors. The strategy is to choose the FP format for evaluation such that the norm of the step is greater than the square root of the unit roundoff.</p><p>The callback functions must handles precision selection for evaluations and optionally error/warning messages if evaluation fails (typically overflow or lack of precision)</p><pre><code class="language-julia hljs">using LinearAlgebra

function my_compute_f_at_c!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T) where {H, L, E, T &lt;: Tuple}
  πmax = length(m.FPList) # get maximal allowed precision
  eval_prec = findfirst(u -&gt; sqrt(u) &lt; st.s_norm, m.UList) # select precision according to the criterion
  if eval_prec === nothing # not enough precsion
    @warn &quot; not enough precision for objective evaluation at c: ||s|| = $(st.s_norm) &lt; sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))&quot;
    st.status = :exception
    return true
  end
  π.πf⁺ = max(eval_prec,π.πc) # evaluation precision should be greater or equal to the FP format of the candidate (see forbidden evaluation)
  st.f⁺ = obj(m,c[π.πf⁺]) # eval objective only. solve!() made sure c[π.πf⁺] is up-to-date (see containers section)
  while st.f⁺ == Inf # check for overflow
    π.πf⁺ += 1
    if π.πf⁺ &gt; πmax
      @warn &quot; not enough precision for objective evaluation at c: overflow&quot;
      st.status = :exception
      return true # objective overflow with highest precision FP format: this is a fail
    end
    st.f⁺ = obj(m,c[π.πf⁺])
  end
  return false
end
  
function my_compute_f_at_x!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T) where {H, L, E, T &lt;: Tuple}
  πmax = length(m.FPList) # get maximal allowed precision
  if st.iter == 0 # initial evaluation, step = 0, choose max precision
    π.πf = πmax
  else # evaluation in main loop
    eval_prec = findfirst(u -&gt; sqrt(u) &lt; st.s_norm, m.UList) # select precision according to the criterion
    if eval_prec === nothing # not enough precsion
      @warn &quot; not enough precision for objective evaluation at x: ||s|| = $(st.s_norm) &lt; sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))&quot;
      st.status = :exception
      return true
    end
    π.πf = max(eval_prec,π.πx) # evaluation precision should be greater or equal to the FP format of the current solution (see forbidden evaluation)
  end
  st.f = obj(m,x[π.πf]) # eval objective only. solve!() made sure x[π.πf] is up-to-date (see containers section)
  while st.f == Inf # check for overflow
    π.πf += 1
    if π.πf &gt; πmax
      @warn &quot; not enough precision for objective evaluation at x: overflow&quot;
      st.status = :exception
      return true # objective overflow with highest precision FP format: this is a fail
    end
    st.f = obj(m,x[π.πf])
  end
  return false
end

function my_compute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T, g::T) where {H, L, E, T &lt;: Tuple}
  πmax = length(m.FPList) # get maximal allowed precision
  if st.iter == 0 # initial evaluation, step = 0, choose max precision
    π.πg = πmax
  else # evaluation in main loop
    eval_prec = findfirst(u -&gt; sqrt(u) &lt; st.s_norm, m.UList) # select precision according to the criterion
    if eval_prec === nothing # not enough precsion
      @warn &quot; not enough precision for gradient evaluation at c: ||s|| = $(st.s_norm) &lt; sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))&quot;
      st.status = :exception
      return true
    end
    π.πg = max(eval_prec,π.πg) # evaluation precision should be greater or equal to the FP format of the candidate (see forbidden evaluation)
  end
  grad!(m,c[π.πg],g[π.πg]) # eval gradient only. solve!() made sure x[π.πg] is up-to-date (see containers section)
  while findfirst(elem-&gt;elem == Inf,g[π.πg]) !== nothing # check for overflow, gradient vector version
    π.πg += 1
    if π.πg &gt; πmax
      @warn &quot; not enough precision for gradient evaluation at c: overflow&quot;
      st.status = :exception
      return true # objective overflow with highest precision FP format: this is a fail
    end
    grad!(m,c[π.πg])
  end
  return false
end

function my_recompute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T, g::T, s::T) where {H, L, E, T &lt;: Tuple}
  # simply update norm of the step, since recompute_g!() is called at the begining of the main loop after step computation
  πmax = length(m.FPList) # get maximum precision index
  π.πns = π.πs # select precision for step norm computation
  s_norm = norm(s[π.πs])
  while s_norm == Inf || s_norm ==0.0 # handle possible over/underflow
    π.πns = π.πns+1 # increase precision to avoid over/underflow
    if π.πns &gt; πmax
      st.status = :exception
      return true, false # overflow occurs with max precion: cannot compute s_norm with provided FP formats. Returns fail.
    end
    s_norm = norm(s[π.πns]) # compute norm with higher precision step, solve!() made sure s[π.πns] is up-to-date
  end
  st.s_norm = s_norm
  return false, false
end</code></pre><p>Let&#39;s try this implementation on a simple quadratic objective.</p><pre><code class="language-julia hljs">FP = [Float16, Float32] # selected FP formats,
f(x) = x[1]^2 + x[2]^2 # objective function
omega = [0.0,0.0]
x = Float32.(1.5*ones(2)) # initial point
MPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omega, ωgRelErr = omega); # indicates the use of relative error only to avoid interval evaluation, relative errors will not be computed with above callbacks
stat = MPR2(MPmodel;
compute_f_at_x! = my_compute_f_at_x!,
compute_f_at_c! = my_compute_f_at_c!,
compute_g! = my_compute_g!,
recompute_g! = my_recompute_g!);
stat  # first-order stationary point has been found</code></pre><p>Let&#39;s now try our implementation on the Rosenbrock function.</p><pre><code class="language-julia hljs">FP = [Float16, Float32] # selected FP formats,
f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
omega = [0.0,0.0]
x = Float32.(1.5*ones(2)) # initial point
MPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omega, ωgRelErr = omega); # indicates the use of relative error only to avoid interval evaluation, relative errors will not be computed with above callbacks
stat = MPR2(MPmodel;
compute_f_at_x! = my_compute_f_at_x!,
compute_f_at_c! = my_compute_f_at_c!,
compute_g! = my_compute_g!,
recompute_g! = my_recompute_g!); # throw lack of precision warning</code></pre><p>The strategy implemented for precision selection does not allow to find a first-order critical point for the Rosenbrock function: the step becomes too small before MPR2 converges. Although this implementation is fast since it does not bother with evaluation errors, it is not very satisfactory since Example 1 in section <a href="#lack-of-precision">Lack of Precision</a> shows that the default implementation is able to converge to a first-order critical point. This highlights that it is important to understand how rounding errors occur and affect the convergence of the algorithm (see section <a href="#mpr2-algorithm-general-description">MPR2 Algorithm: General Description</a>) and that &quot;naive&quot; strategies like the one implemented above might not be satisfactory.</p><h3 id="Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision"><a class="docs-heading-anchor" href="#Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision">Example 2: Switching to Gradient Descent When Lacking Objective Precision</a><a id="Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision-1"></a><a class="docs-heading-anchor-permalink" href="#Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision" title="Permalink"></a></h3><p>It might happen that <code>solve!()</code> stops early because the objective evaluation lacks precision. Consider for example that we use consider relative evaluation error for the objective. If MPR2 converges to a point where the objective is big, the error can be big too, and if the gradient is small the convergence condition <span>$\omega f(x_k) \leq \eta_0 \Delta T_k = \|\hat{g}(x_k)\|^2/\sigma_k$</span> is likely to fail. In that case, the user might want to continue running the algorithm without caring about the objective, that is, as a simple gradient descent. <code>solve!()</code> implementation allows enough flexibility to do so. In the implementation below, the user defined structure <code>e</code> is used to indicate what &quot;mode&quot; the algorithm is running: default mode or gradient descent. The callbacks <code>compute_f_at_x!</code> sets <code>st.f = Inf</code> and <code>compute_f_at_c!</code> sets <code>st.f⁺ = 0</code> if gradient descent mode is used. This ensures that <span>$\rho_k = Inf \geq \eta_1$</span> and the step is accepted in gradient descent mode. In the implementation below, <code>compute_f_at_x!</code> and <code>compute_f_at_c!</code> selects the precision such that <span>$\omega f(x_k) \leq \eta_0 \Delta T_k$</span> in default mode. We implement <code>compute_g!</code> to set <code>σ</code> so that <code>ComputeStep!()</code> will use the learning rate <code>1/σ</code>. We use the default <code>recompute_g</code> callback.</p><pre><code class="language-julia hljs">mutable struct my_struct
  gdmode::Bool
  learning_rate
end

function my_compute_f_at_c!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T) where {H, L, E, T &lt;: Tuple}
  if !e.gdmode
    ωfBound = p.η₀*st.ΔT
    π.πf⁺ = π.πx # basic precision selection strategy
    st.f⁺, st.ωf⁺, π.πf⁺ = objReachPrec(m, c, ωfBound, π = π.πf⁺)
    if st.f⁺ == Inf # stop algo if objective overflow
      @warn &quot;Objective evaluation overflow at x&quot;
      st.status = :exception
      return true
    end
    if st.ωf⁺ &gt; ωfBound # evaluation error too big
      @warn &quot;Objective evaluation error at x too big to ensure convergence: switching to gradient descent&quot;
      e.gdmode = true
      st.f⁺ = 0
      return false
    end
  else # gradient descent mode
    st.f⁺ = 0
  end
  return false
end

function my_compute_f_at_x!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T) where {H, L, E, T &lt;: Tuple}
  πmax = length(m.EpsList)
  if st.iter == 0 # initial evaluation before main loop
    st.f, st.ωf, π.πf = objReachPrec(m, x, m.OFList[end], π = π.πf)
  else # evaluation in the main loop
    if !e.gdmode
      ωfBound = p.η₀*st.ΔT
      if st.ωf &gt; ωfBound # need to reevaluate the objective at x
        if π.πf == πmax # already at highest precision 
          @warn &quot;Objective evaluation error at x too big to ensure convergence: switching to gradient descent&quot;
          e.gdmode = true
          st.f = Inf
          return false
        end
        π.πf += 1 # increase evaluation precision of f at x
        st.f, st.ωf, π.πf = objReachPrec(m, x, ωfBound, π = π.πf)
        if st.ωf &gt; ωfBound # error evaluation too big with max precison
          @warn &quot;Objective evaluation error at x too big to ensure convergence: switching to gradient descent&quot;
          e.gdmode = true
          st.f = Inf
          return false
        end
      end
    else # gradient descent mode
      st.f = Inf
    end
  end
  return false
end

function my_compute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T, g::T) where {H, L, E, T &lt;: Tuple}
  π.πg = π.πc # default strategy, could be a callback
  st.ωg, π.πg = gradReachPrec!(m, c, g, m.OFList[end], π = π.πg)
  if e.gdmode
    st.σ = 1/e.learning_rate
  end
  return false
end</code></pre><p>Let us first run <code>MPR2()</code> with the default implementation and relative evaluation error.</p><pre><code class="language-julia hljs">FP = [Float16, Float32] # selected FP formats,
#f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
f(x) = x[1]^2 + x[2]^2 +0.5
omegaf = Float64.([0.01,0.005])
omegag = Float64.([0.05,0.01])
x = Float32.(1.5*ones(2)) # initial point
MPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omegaf, ωgRelErr = omegag);
stat = MPR2(MPmodel,verbose=1); # stops at iteration 3, throw lack of precision warning</code></pre><p>We run <code>MPR2()</code> with the callback functions defined above and the default callbacks for <code>compute_g!()</code> and <code>recompute_g!()</code>. We use relative objective and gradient error.</p><pre><code class="language-julia hljs">FP = [Float16, Float32] # selected FP formats,
#f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function
f(x) = x[1]^2 + x[2]^2 +0.5
omegaf = Float64.([0.01,0.005])
omegag = Float64.([0.05,0.01])
x = ones(Float32,2) # initial point
MPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omegaf, ωgRelErr = omegag);
e = my_struct(false,1e-2)
stat = MPR2(MPmodel;
e = e,
compute_f_at_x! = my_compute_f_at_x!,
compute_f_at_c! = my_compute_f_at_c!,
compute_g! = my_compute_g!); # switch to gradient descent at iteration 3, converges to first order critical point</code></pre></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Tuesday 25 July 2023 22:00">Tuesday 25 July 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
