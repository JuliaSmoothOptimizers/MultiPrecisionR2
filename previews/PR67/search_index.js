var documenterSearchIndex = {"docs":
[{"location":"MPCounters/#MPCounters","page":"MPCounters","title":"MPCounters","text":"","category":"section"},{"location":"MPCounters/","page":"MPCounters","title":"MPCounters","text":"MPCounters structure is meant to extend the Counters structure from `NLPModels.jl package to multi-precision. MPCounters fields are dictionaries Dict{DataType,Int}, counting evaluations for each FP formats.","category":"page"},{"location":"MPCounters/#Fields","page":"MPCounters","title":"Fields","text":"","category":"section"},{"location":"MPCounters/","page":"MPCounters","title":"MPCounters","text":"Fields are exactly the same than Counters structure, but are dictionaries instead of Int. See Counters.jl.","category":"page"},{"location":"MPCounters/#Interface","page":"MPCounters","title":"Interface","text":"","category":"section"},{"location":"MPCounters/","page":"MPCounters","title":"MPCounters","text":"neval_field(mpnlp): returns the total number of any FP formats of field evaluations.","category":"page"},{"location":"MPCounters/","page":"MPCounters","title":"MPCounters","text":"neval_field(mpnlp,T): returns the number of field evaluations with FP format T.","category":"page"},{"location":"MPCounters/","page":"MPCounters","title":"MPCounters","text":"sum_eval(mpnlp): returns sum of all counters of any FP formats except cons, jac, jprod, jtprod. ","category":"page"},{"location":"MultiPrecisionR2/#MutiPrecisionR2","page":"MultiPrecisionR2","title":"MutiPrecisionR2","text":"","category":"section"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"MutliPrecisionR2.jl implements MPR2 (Multi-Precision Quadratic Regularization) algorithm, a multiple Floating Point (FP) precision (or formats) adaptation of the Quadratic Regularization (R2) algorithm. R2 is a first order algorithm designed to solve unconstrained minimization problems","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"min_x f(x)","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":",","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"with fmathbbR^n rightarrow mathbbR a smooth non-linear function and x in mathbbR^n.","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"MPR2 extends R2 by dynamically adapting the FP formats used for objective and gradient evaluations so that the convergence is guaranteed in spite of evaluation errors due to finite-precision computations and over/underflow are avoided.","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"MPR2 relies on FPMPNLPModel structure (see documentation) to evaluate the objective and gradient with multiple floating point format and control the evaluation errors.","category":"page"},{"location":"MultiPrecisionR2/#How-to-Run","page":"MultiPrecisionR2","title":"How to Run","text":"","category":"section"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"MPR2 algorithm is run in the same fashion than solvers from JSOSolvers.jl package, but uses a FPMPNLPModel structure which is a multi-precision extension of NLPModel structure to multi-precision.","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"MPR2 algorithm is run with MPR2() function, which returns a GenericExecutionStat structure containing useful information (nb. of iteration, termination status, etc.). The number of evaluations can be access via the FPMPNLPModel structure (see FPMPNLPModel and MPCounters documentation).","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"using MultiPrecisionR2\n\nT = [Float16,Float32] # floating point formats used for evaluation\nf(x) = sum(x.^2) # objective function\nn = 100 # problem dimension\nx0 = ones(Float32,n) # initial solution\nmpnlp = FPMPNLPModel(f,x0,T) # creates a multi-precision model of the problem\nstats = MPR2(mpnlp)\nprintln(\"Objective was evaluated $(neval_obj(mpnlp,Float32)) times with Float32 and $(neval_obj(mpnlp,Float64)) times with Float64\")\nprintln(\"Gradient was evaluated $(neval_grad(mpnlp,Float32)) times with Float32 and $(neval_grad(mpnlp,Float64)) times with Float64\")","category":"page"},{"location":"MultiPrecisionR2/#Solver-Options","page":"MultiPrecisionR2","title":"Solver Options","text":"","category":"section"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"Some parameters of the algorithm can be given as keyword arguments of MPR2(). The type parameters are S::AbstractVector, H::AbstractFloat, T::DataType, E::DataType.","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"MPR2(mpnlp::FPMPNLPModel; kwargs...)","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"kwarg description\nx₀::S = MPnlp.Model.meta.x0 initial guess, FP format must be in mpnlp.FPList\npar::MPR2Params = MPR2Params(MPnlp.FPList[1],H) MPR2 parameters, see MPR2Params for details\natol::H = H(sqrt(eps(T))) absolute tolerance on first order criterion\nrtol::H = H(sqrt(eps(T))) relative tolerance on first order criterion\nmax_eval::Int = -1 maximum number of evaluation of the objective function.\nmax_iter::Int = 1000 maximum number of iteration allowed\nσmin::T = sqrt(T(MPnlp.EpsList[end])) minimal value for regularization parameter. Value must be representable in any of the floating point formats of MPnlp.\nverbose::Int=0 display iteration information if > 0\ne::E user defined structure, used as argument for compute_f_at_x!, compute_f_at_c!, compute_g! and recompute_g! callback functions.\ncompute_f_at_x! callback function to select precision and compute objective value and error bound at the current point. Allows to reevaluate the objective at x if more precision is needed.\ncompute_f_at_c! callback function to select precision and compute objective value and error bound at candidate.\ncompute_g! callback function to select precision and compute gradient value and error bound. Called at the end of main loop.\nrecompute_g! callback function to select precision and recompute gradient value if more precision is needed. Called after step, candidate and model decrease computation in main loop.\nselectPic! callback function to select FP format of c at the next iteration","category":"page"},{"location":"MultiPrecisionR2/#Choosing-MPR2-Parameters","page":"MultiPrecisionR2","title":"Choosing MPR2 Parameters","text":"","category":"section"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"MPR2 algorithm parameters for error tolerance and step acceptance can be given as par::MPR2Params keyword argument. These parameters correspond to MPR2Params fields, with type parameters H::AbstractFloat and L::AbstractFloat.","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"Field Default value Description\nη₀::H 0.01 controls objective function error tolerance, convergence condition is ωf ≤ η₀ ΔT (see FPMPNLPModel for details on ωf)\nη₁::H 0.02 step successful if ρ ≥ η₁ (update incumbent)\nη₂::H 0.95 step very successful if ρ ≥ η₂ (decrease σ ⟹ increase step length)\nκₘ::H 0.02 tolerance on gradient evaluation error, μ ≤ κₘ (see computeMu)\nγ₁::L 2^(-2) σk+1 = σk * γ₁ if ρ ≥ η₂\nγ₂::L 2 σk+1 = σk * γ₂ if ρ < η₁","category":"page"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"These parameters must satisfy some conditions, see MPR2Params for details. These conditions can be checked with CheckMPR2ParamConditions() function.","category":"page"},{"location":"MultiPrecisionR2/#Evaluation-Error-Mode","page":"MultiPrecisionR2","title":"Evaluation Error Mode","text":"","category":"section"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"MPR2 convergence is ensured by taking into account objective and gradient evaluation errors. These errors can be evaluated with interval arithmetic of based on relative error assumption. The error evaluation mode is chosen upon the FPMPNLPModel instantiation, given as argument of MPR2(). Evaluating the objective/gradient and the error is done with the interfaces provided in MPNLPModels.jl, see FPMPNLPModel documentation. ","category":"page"},{"location":"MultiPrecisionR2/#Callbacks-for-Personalized-Implementation","page":"MultiPrecisionR2","title":"Callbacks for Personalized Implementation","text":"","category":"section"},{"location":"MultiPrecisionR2/","page":"MultiPrecisionR2","title":"MultiPrecisionR2","text":"MPR2() allows the user to define its own strategies for evaluation FP formats selection and error handling for objective and gradient evaluations. This can be done by providing the callbacks compute_f_at_x!, compute_f_at_c!, compute_g!, recompute_g! and selectPic!. See \"MPR2 advanced use\" tutorial for usage. ","category":"page"},{"location":"tutorial_FPMPNLPModel/#Evaluation-Mode","page":"FPMPNLPModel Tutorial","title":"Evaluation Mode","text":"","category":"section"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"FPMPNLPModel implements tools to evaluate the objective and the gradient and the error bounds omega_f(x_k) and omega_g(x_k) with two modes:","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"Relative Error: The error bounds omega_f(x_k) and omega_g(x_k) are estimated as fractions of hatf(x_k) and hatg(x_k), respectively. It is to the user to provide these fraction values.  ","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"By default, interval arithmetic is used for error bound computation. If relative error bounds have to be used, ωfRelErr and ωgRelErr keyword arguments must be used when calling the constructor function.","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"Interval Arithmetic: Using interval arithmetic enables to track rounding errors and to provide guaranteed bounds.MultiPrecisionR2.jl relies on IntervalArithmetic.jl library to perform interval evaluations of the gradient and the objective.  ","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"Warning: Using interval evaluation can lead to significantly long computation time. Interval evaluation might fail for some objective functions due to IntervalArithmetic.jl errors.","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"FPMPNLPModel Example 1: Interval Arithmetic for error bounds","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"using MultiPrecisionR2\nusing IntervalArithmetic\n\nsetrounding(Interval,:accurate) # add this to avoid error with Float16 interval evaluations\nFP = [Float16, Float32] # selected FP formats\nf(x) = x[1]^2 + x[2]^2 # objective function\nx = ones(2) # initial point\nx16 = Float16.(x) # initial point in Float16\nx32 = Float32.(x) # initial point in Float32\nMPmodel = FPMPNLPModel(f,x32,FP, obj_int_eval = true, grad_int_eval = true); # will use interval arithmetic for error evaluation\nf16, omega_f16 = objerrmp(MPmodel,x16) # evaluate objective and error bound at x with T[1] = Float16 FP model\nf32, omega_f32 = objerrmp(MPmodel,x32) # evaluate objective and error bound at x with T[1] = Float16 FP model\ng16, omega_g16 = graderrmp(MPmodel,x16) # evaluate gradient and error bound at x with T[2] = Float32 FP model\ng32, omega_g32 = graderrmp(MPmodel,x32) # evaluate gradient and error bound at x with T[2] = Float32 FP model","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"FPMPNLPModel Example 2: Relative error bounds","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"using MultiPrecisionR2\nusing ADNLPModels\n\nFP = [Float16, Float32] # selected FP formats\nf(x) = x[1]^2 + x[2]^2 # objective function\nx = ones(2) # initial point\nx16 = Float16.(x) # initial point in Float16\nx32 = Float32.(x) # initial point in Float32\nωfRelErr = [0.1,0.01] # objective error: 10% with Float16 and 1% with Float32\nωgRelErr = [0.05,0.02] # gradient error (norm): 5% with Float16 and 2% with Float32\nMPmodel = FPMPNLPModel(f,x32,FP;ωfRelErr=ωfRelErr,ωgRelErr=ωgRelErr);\nf32, omega_f32 = objerrmp(MPmodel,x32) # evaluate objective and error bound at x with T[1] = Float32 FP model\ng16, omega_g16 = graderrmp(MPmodel,x16) # evaluate gradient and error bound at x with T[2] = Float16 FP model","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"FPMPNLPModel Example 3: Mixed Interval/Relative Error Bounds","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"It is possible to evaluate the objective with interval mode and the gradient with relative error mode, and vice-versa.","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"using MultiPrecisionR2\nusing IntervalArithmetic\nusing ADNLPModels\n\nsetrounding(Interval,:accurate)\nFP = [Float16, Float32] # selected FP formats\nf(x) = x[1]^2 + x[2]^2 # objective function\nx = ones(2) # initial point\nx16 = Float16.(x) # initial point in Float16\nx32 = Float32.(x) # initial point in Float32\nωgRelErr = [0.05,0.02] # gradient error (norm): 5% with Float16 and 2% with Float32\nMPmodel = FPMPNLPModel(f,x32,FP; obj_int_err = true, ωgRelErr = ωgRelErr) # use interval for objective error bound and relative error bound for gradient\nf32, omega_f32 = objerrmp(MPmodel,x32) # evaluate objective and error bound with interval at x with T[1] = Float32 FP model\ng16, omega_g16 = graderrmp(MPmodel,x16) # evaluate gradient and error bound at x with T[2] = Float16 FP model","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"FPMPNLPModel Example 4: Interval evaluation is slow","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"Interval evaluation provides guaranteed bounds, but is slow compared with classical evaluation.","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"using MultiPrecisionR2\nusing IntervalArithmetic\nusing ADNLPModels\n\nsetrounding(Interval,:accurate)\nFP = [Float32] # selected FP formats\nn = 1000\nf(x) = sum([x[i]^2 for i =1:n])  # objective function\nx = ones(n) # initial point\nx32 = Float32.(x) # initial point in Float32\nMPmodelInterval = FPMPNLPModel(f,x32,FP) # use interval for objective and gradient error bounds\nMPmodelRelative = FPMPNLPModel(f,x32,FP) # will use default relative error bounds\n# precompile\nobjerrmp(MPmodelInterval,x32)\nobjerrmp(MPmodelRelative,x32)\ngraderrmp(MPmodelInterval,x32)\ngraderrmp(MPmodelRelative,x32)\n\n@time objerrmp(MPmodelInterval,x32) # interval evaluation of objective\n@time objerrmp(MPmodelRelative,x32) # classic evaluation of objective\n@time graderrmp(MPmodelInterval,x32) # interval evaluation of gradient\n@time graderrmp(MPmodelRelative,x32) # classic evaluation of gradient","category":"page"},{"location":"tutorial_FPMPNLPModel/#**High-Precision-Format**","page":"FPMPNLPModel Tutorial","title":"High Precision Format","text":"","category":"section"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"FPMPNLPModel performs some operations with a high precision FP format to provide more numerical stability. The convergence of MPR2 relies on the fact that such operations are \"exact\", as if performed with infinite precision.","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"This high precision format can be given as a keyword argument upon instantiation of FPMPNLPModel. The default value is Float64. Note that this high precision format corresponds to the type parameter H in struct FPMPNLPModel{H,F,T<:Tuple}. It is expected that FPMPNLPModel.HPFormat has at least equal or greater machine epsilon than the highest precision FP format that can be used for objective or gradient evaluation.","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"FPMPNLPModel Example 5: HPFormat value","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"using MultiPrecisionR2\n\nFP = [Float16, Float32, Float64] # selected FP formats, max eval precision is Float64\nf(x) = x[1]^2 + x[2]^2 # objective function\nx = ones(2) # initial point\nMPmodel = FPMPNLPModel(f,x,FP); # throws warning\ntry\n  MPmodel = FPMPNLPModel(f,x,FP,HPFormat = Float32); # throws error\ncatch e\n  e\nend","category":"page"},{"location":"tutorial_FPMPNLPModel/#**Gradient-and-Dot-Product-Error**:-Gamma-Callback-Function","page":"FPMPNLPModel Tutorial","title":"Gradient and Dot Product Error: Gamma Callback Function","text":"","category":"section"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"FPMPNLPModel.graderrmp() computes both the gradient and the error relative error bound omega_g such that = nabla f(x) - fl(nabla f(x))_2 leq omega_g fl(nabla f(x))_2. To do so, it is necessary to compute norm of the gradient and therefore to take the related error into account, given by the beta function: beta(nu) = max(sqrtgamma(nu)-1-1sqrtgamma(nu)+1-1) which expresses with gamma function which models the dot product error. This gamma function is a callback that can be provided to the FPMPNLPModel with the γfunc, and by default is γfunc(nu) = n*u. An implicit condition is that gamma(nu_max) leq 1, with u_max the smallest unit-roundoff among the FP formats in FPList.","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"For example, if the highest precision format Float32 is used, u_max approx 1 e^-7 which limits the size of the problems that can be solved to approx 1e^7 variables with the default implementation γ(nu) = n*u. If this is a problem, the user can provide its own callback function FPMPNLPModels.γfunc. This is illustrated in the example below.","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"FPMPNLPModel Example 5: Gradient and Dot Product Error","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"The code below returns an error at the instantiation of FPMPNLPModels indicating that the dimension of the problem is too big with respect to the highest precision FP format provided (Float16).","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"using MultiPrecisionR2\n\nFP = [Float16] # limits the size of the problem to n = 1/eps(Float16) (= 1000)\ndim = 2000 # dimension of the problem too large\nf(x) =sum([x[i]^2 for i=1:dim]) # objective function\nx = ones(Float16,dim) # initial point\ntry\n  MPmodel = FPMPNLPModel(f,x,FP); # throw error\ncatch e\n  e\nend","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"The user can provide a less pessimistic gamma function for dot product error bound. Warning: Providing your own gamma function might increase numerical instability of MPR2.","category":"page"},{"location":"tutorial_FPMPNLPModel/","page":"FPMPNLPModel Tutorial","title":"FPMPNLPModel Tutorial","text":"using MultiPrecisionR2\n\ngamma(n,u) = sqrt(n)*u # user defined γ function, less pessimistic than n*u used by default\nMPmodel = FPMPNLPModel(f,x,FP,γfunc = gamma); # no error since sqrt(dim)*eps(Float16) < 1","category":"page"},{"location":"tutorial_MPR2_basic_use/#Disclaimer","page":"MPR2 Tutorial: Basic Use ","title":"Disclaimer","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"The reader is encouraged to read the FPMPNLPModel tutorial before the present one. ","category":"page"},{"location":"tutorial_MPR2_basic_use/#Motivation","page":"MPR2 Tutorial: Basic Use ","title":"Motivation","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Here is the comparison between R2 algorithm from JSOSolvers.jl run with Float64 and MPR2 from MultiPrecisionR2.jl run with Float16, Float32 and Float64 over a set of unconstrained problems taken from OptimizationProblems.jl. The time and energy savings offered by MPR2 compared with R2 is estimated with the rule of thumb:","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Number of bits divided by two implies time computation divided by two and energy consumption divided by four.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\nusing NLPModels\nusing ADNLPModels\nusing OptimizationProblems\nusing OptimizationProblems.ADNLPProblems\nusing JSOSolvers\nusing Quadmath\n\nFP = [Float16,Float32,Float64] # MPR2 Floating Point formats\nomega = Float128.([sqrt(eps(Float16)),sqrt(eps(Float32)),0]) # MPR2 relative errors, computations assumed exact with Float64\nr2_obj_eval = [0]\nr2_grad_eval = [0]\nmpr2_obj_eval = zeros(Float64,length(FP))\nmpr2_grad_eval = zeros(Float64,length(FP))\nnvar = 99 #problem dimension (if scalable)\nmax_iter = 1000\n\nmeta = OptimizationProblems.meta\nnames_pb_vars = meta[(meta.has_bounds .== false) .& (meta.ncon .== 0), [:nvar, :name]] #select unconstrained problems\nfor pb in eachrow(names_pb_vars)\n  nlp = eval(Meta.parse(\"ADNLPProblems.$(pb[:name])(n=$nvar,type=Val(Float64),backend = :generic)\"))\n  mpmodel = FPMPNLPModel(nlp,FP,HPFormat = Float128, ωfRelErr=omega, ωgRelErr=omega);\n  statr2 = R2(nlp,max_eval=max_iter)\n  r2_obj_eval .+= nlp.counters.neval_obj\n  r2_grad_eval .+= nlp.counters.neval_grad\n  statmpr2 = MPR2(mpmodel,max_iter = max_iter)\n  mpr2_obj_eval .+= [haskey(mpmodel.counters.neval_obj,fp) ? mpmodel.counters.neval_obj[fp] : 0 for fp in FP]\n  mpr2_grad_eval .+= [haskey(mpmodel.counters.neval_grad,fp) ? mpmodel.counters.neval_grad[fp] : 0 for fp in FP]\nend\nmpr2_obj_time = sum(mpr2_obj_eval.*[1/4,1/2,1])\nobj_time_save = mpr2_obj_time/r2_obj_eval[1]\nmpr2_obj_energy = sum(mpr2_obj_eval.*[1/16,1/4,1])\nobj_energy_save = mpr2_obj_energy/r2_obj_eval[1]\nmpr2_grad_time = sum(mpr2_grad_eval.*[1/4,1/2,1])\ngrad_time_save = mpr2_grad_time/r2_grad_eval[1]\nmpr2_grad_energy = sum(mpr2_grad_eval.*[1/16,1/4,1])\ngrad_energy_save = mpr2_grad_energy/r2_grad_eval[1]\nprintln(\"Possible time saving for objective evaluation with MPR2: $(round((1-obj_time_save)*100,digits=1)) %\")\nprintln(\"Possible time saving for gradient evaluation with MPR2: $(round((1-grad_time_save)*100,digits=1)) %\")\nprintln(\"Possible energy saving for objective evaluation with MPR2: $(round((1-obj_energy_save)*100,digits=1)) %\")\nprintln(\"Possible energy saving for gradient evaluation with MPR2: $(round((1-grad_energy_save)*100,digits=1)) %\")","category":"page"},{"location":"tutorial_MPR2_basic_use/#MPR2-Algorithm:-General-Description-and-Basic-Use","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Algorithm: General Description and Basic Use","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/#**Notations**","page":"MPR2 Tutorial: Basic Use ","title":"Notations","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"FPList: List of floating point formats available\nfl\n: finite-precision computation\nu\n: unit round-off for a given FP format\ndelta\n: rounding error induced by one FP operation (+-*), for example fl(x+y) = (x+y)(1+delta). Bounded as deltaleq u.\npi\n: FP format index in FPList, also called precision\nf x rightarrow f(x)\n: objective function\nhatf xpi_f rightarrow fl(f(xpi_f))\n: finite precision counterpart of f with FP format corresponding to index pi_f in FPList  \nnabla f x rightarrow nabla f(x)\n: gradient of f\nhatg x pi_g rightarrow fl(nabla f(x)pi_g)\n: finite precision counterpart of nabla f  with FP format corresponding to index pi_g in FPList\nomega_f(x)\n: bound on finite precision evaluation error of f,  hatf(xpi_f) -f(x)  leq omega_f(x)\nomega_g(x)\n: bound on finite precision evaluation error of nabla f,  hatg(xpi_g) -nabla f(x)  leq omega_g(x)hatg(xpi_g)","category":"page"},{"location":"tutorial_MPR2_basic_use/#**MPR2-Algorithm-Broad-Description**-(differs-from-package-implementation)","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Algorithm Broad Description (differs from package implementation)","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2 is described by the following algorithm. Note that the actual implementation in the package differs slightly. For an overview of the actual implementation, see Advanced Use tutorial.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"In the algorithm, compute means compute with finite-precision machine computation, and define means \"compute exactly\", i.e. with infinite precision. Defining values is necessary to ensure the theoretical convergence of MPR2. Defining a value is not possible to perform on a (finite-precision) machine, but one can use high precision FP formats (see section High Precision Format)","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Inputs: ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Initial values: x_0, sigma_0\nTunable Parameters: 0  eta_0  eta_1  eta_2  1, 0  gamma_1  1  gamma_2, kappa_m\nGradient tolerance: epsilon\nList of FP formats (e.g [Float16, Float32, Float64])","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Outputs","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"x_k\nsuch that nabla f(x_k) leq epsilon nabla f(x_0)","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Initialization","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Compute f_0 = hatf(x_0pi_f)\nCompute g_0 = hatg(x_0pi_g)","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"While g_k  dfracepsilon1+omega_g(x_k) do","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Compute s_k = -g_ksigma_k\nCompute c_k = x_k + s_k\nCompute Delta T_k = g_k^Ts_k # model reduction\nDefine mu_k # gradient error indicator","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"If mu_k geq kappa_m\nSelect precisions such that mu_k leq kappa_m, go to 3.\nEnd If\nIf omega_f(x_k)  eta_0 Delta T_k\nSelect pi_f such that omega_f(x_k) leq eta_0 Delta T_k\nCompute f_k = f(x_kpi_f)\nEnd If","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Select pi_f^+ such that omega(c_k) leq eta_0 Delta T_k\nCompute f_k^+ = hatf(c_kpi_f^+)\nCompute rho_k = dfracf_k - f_k^+Delta T_k","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"If rho_k geq eta_1 # step acceptance\nx_k+1 = c_k\n, f_k+1 = f_k^+\nEnd If","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Compute $ \\sigma_k = \\left{ \\begin{array}{lll}","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"\\gamma1 \\sigmak & \\text{if} & \\rhok \\geq \\eta2 \\\n    \\sigmak & \\text{if} & \\rhok \\in  \\eta1,\\eta2  \\\n    \\gamma2 \\sigmak & \\text{if} & \\rhok < \\eta1   \\end{array}    \\right.$","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"End While  ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"return x_k","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2 stops either when:","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"a point x_k satisfying g_k leq dfracepsilon1+omega_g(x_k) has been found (see omega_g definition), which ensures that $ \\|\\nabla f(x_k)\\| \\leq \\epsilon$, or \nno FP format enables to achieve required precision on the objective or mu indicator.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"The indicator mu_k aggregates finite-precision errors due to gradient evaluation (omega_g(x_k)), and the computation of the step (s_k), candidate (c_k) and model reduction Delta T_k as detailed in the Rounding Error Handling section.     ","category":"page"},{"location":"tutorial_MPR2_basic_use/#**Rounding-Errors-Handling**","page":"MPR2 Tutorial: Basic Use ","title":"Rounding Errors Handling","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Anything computed in MPR2 suffers from rounding errors since it runs on a machine, which necessarily performs computation with finite-precision arithmetic. Below is the list of rounding errors that are handled by MPR2 such that convergence and numerical stability is guaranteed. MPR2 is designed to run with FP numbers complying with the IEEE 754 norm.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Dot Product Error: gamma_n","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"The model for the dot product error that is used by default is     fl(xy) - xy leq xy gamma(nu) with     gamma nu rightarrow n*u     where x and y are two FP vectors (same FP format) of dimension n and u is the round-off unit of the FP format. This is a crude yet guaranteed upper bound on the error. The gamma function is embedded in the FPMPNLPModel structure as a callback (see documentation and tutorial), such that the user can choose its own formula.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Candidate Computation Both the step and the candidate are computed inexactly.\nInexact Step: the computed step is s_k = fl(g_ksigma_k) = (g_ksigma_k)(1+delta) neq g_ksigma_k.\nInexact Candidate: the computed candidate is c_k = fl(x_k+s_k) = (x_k+s_k)(1+delta) neq x_k+s_k.\nThese two errors implies that c_k is not along the descent direction g_k and as such can be interpreted as additional gradient error to  omega_g(x_k). To handle these errors, MPR2 computes the ratio phi_k = x_ks_k. The greater phi_k, the greater the possible deviation of c_k from the direction g_k. These errors are aggregated in the mu_k indicator (detailed in 5.).\nModel Decrease Computation","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"The FP computation error for the model decrease Delta T_k is such that,     fl(Delta T_k) =  Delta T_k (1+vartheta_n), with vartheta_n leq gamma(nu)     MPR2 handles this error by aggregating alpha(nu) in the indicator mu_k, with alpha(nu) = dfrac11-gamma(nu)","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Norm Computation  ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"The finite-precision error for norm computation of a FP vector x of dimension n is     $ |fl(\\|x\\|)-\\|x\\|| \\leq fl(\\|x\\|)\\beta(n+2,u)$ with     betanu rightarrow max(sqrtgamma(nu)-1-1sqrtgamma(nu)+1-1)     beta function cannot be chosen by the user, but gamma(nu) can be chosen via γfunc keyword argument when instanciating a FPMPNLPModel (see next sections).   The beta function is used in MPR2 implementation to handle finite-precision error of norm computation for ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"g_k\nnorm computation: the stopping criterion implemented in MPR2 is   g_k leq dfrac11+beta (n+2u)dfrac11+omega_g(x_k) which ensures that nabla f(x_k)leq epsilon. \nphi_k\nratio computation (see 3.): phi_k requires the norm of x_k and s_k. MPR2 actually defines ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"phi_k =  dfracfl(x_k)fl(x_k)dfrac1+beta(n+2u)1-beta(n+2u) which ensures that phi_k geq x_k  s_k.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Mu Indicator  ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"mu_k is an indicator which aggregates","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"the gradient evaluation error omega_g(x_k),\nthe inexact step and candidate errors (phi_k),\nthe model decrease error (gamma(n+1u), alpha(nu)).  ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"The formula for mu_k is     mu_k = dfracalpha(nu) omega_g(x_k)(1+u(phi_k +1)) + alpha(nu) lambda_k + u+ gamma(n+1u)alpha(n+1u)1-u.     Note that if no rounding error occurs for (u = 0), one simply has mu_k = omega_g(x_k).     The implementation of line 7. of MPR2 (as described in the above section) consists in recomputing the step, candidate, x_k and/or s_k norm, model decrease or gradient with higher precision FP formats (therefore decreasing u) until mu_k leq kappa_m. For details about default strategy, see recomputeMu! documentation.","category":"page"},{"location":"tutorial_MPR2_basic_use/#Conditions-on-Parameters","page":"MPR2 Tutorial: Basic Use ","title":"Conditions on Parameters","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2 parameters can be chosen by the user (see Section Basic Use) but must satisfy the following inequalities to ensure convergence:","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"0 leq eta_0 leq frac12eta_1\n0 leq eta_1 leq eta_2  1\neta_0+dfrackappa_m2 leq 05(1-eta_2)\neta_2  1\n0gamma_11gamma_2","category":"page"},{"location":"tutorial_MPR2_basic_use/#Basic-Use","page":"MPR2 Tutorial: Basic Use ","title":"Basic Use","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2 solver relies on multi-precision models structure FPMPNLPModels (Floating Point Multi Precision Non-Linear Programming Models), that derives from NLPModels.jl. This structure embeds the problem and provides the interfaces to evaluate the objective and the gradient with several FP formats and provide error bounds omega_f and omega_g. For details about FPMPNLPModels, see related documentation and tutorial.","category":"page"},{"location":"tutorial_MPR2_basic_use/#**MPR2-Solver**","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Solver","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2 solver is run with MPR2() function which takes a FPMPNLPModel argument (see FPMPNLPModel documentation for details).","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\n\nT = [Float16,Float32] # defines FP format used for evaluations\nf(x) = sum(x.^2) # objective function\nx = ones(Float32,2) # initial point\nmpnlp = FPMPNLPModel(f,x,T) # instanciate a FPMPNLPModel\nstats = MPR2(mpnlp) # runs MPR2","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2() returns a GenericExectutionStats structure that contains information about the execution status. See SolverCore.jl package for details.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"stats.iter # access number of iteration\nstats.solution # accsess solution computed by MPR2 \nstats.dual_feas # access gradient norm at the solution","category":"page"},{"location":"tutorial_MPR2_basic_use/#**Evaluation-Error-Modes**","page":"MPR2 Tutorial: Basic Use ","title":"Evaluation Error Modes","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2 can be run with interval or relative error mode for objective and gradient evaluation error estimation. This error mode is chosen when instanciating the FMPMNLPModel.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Example: Interval Error Mode","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\nusing IntervalArithmetic # need this to call setrounding function\n\nT = [Float16,Float32] \nsetrounding(Interval,:accurate) # need this since Float16 is used, see warnings in the README\nf(x) = sum(x.^2)\nx = ones(Float32,2)\nmpnlp = FPMPNLPModel(f,x,T,obj_int_eval = true, grad_err_eval = true) # instanciate a FPMPNLPModel, interval evaluation will be used for error estimation\nMPR2(mpnlp) # runs MPR2 with interval estimation of the evaluation errors","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Example: Relative Error Mode","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\n\nT = [Float16,Float32] \nf(x) = sum(x.^2)\nomega = [0.01,0.001] # 1% and 0.1% relative error for Float16 and Float32 evaluations\nx = ones(Float32,2)\nmpnlp = FPMPNLPModel(f, x, T; ωfRelErr = omega, ωgRelErr = omega) # instanciate a FPMPNLPModel, relative error model will be used for error estimation\nMPR2(mpnlp) # runs MPR2 with relative error model estimation","category":"page"},{"location":"tutorial_MPR2_basic_use/#**High-Precision-Format**","page":"MPR2 Tutorial: Basic Use ","title":"High Precision Format","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2 uses a high precision format to compute \"exactly\" the values that are defined (see section MPR2 Algorithm Broad Description). This high precision format corresponds to the type parameter H in MPR2() template. The high precision format used by MPR2 is MPnlp::FPMPNLPModel's H parameter type which can be chosen upon MPnlp instantiation (see FPMPNLPModel documentation).","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\nusing Quadmath\n\nHPFormat = Float128\nT = [Float32,Float64]\nf(x) = sum(x.^2)\nx = ones(Float32,2)\nmpnlp = FPMPNLPModel(f,x,T;HPFormat = HPFormat) # instanciate a FPMPNLPModel with Float64 HPFormat\nMPR2(mpnlp) # runs MPR2 with Float128 to compute \"define\" values","category":"page"},{"location":"tutorial_MPR2_basic_use/#**Gamma-Function**","page":"MPR2 Tutorial: Basic Use ","title":"Gamma Function","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2 relies on the dot product error function gamma to handle some rounding errors for model decrease and norm computation (see MPR2 Broad Description). The gamma function used by MPR2 is the one of the MPR2()'s MPnlp::FMPNLPModel argument and can be chosen by the user (see FPMPNLPModel documentation and tutorial). The default gamma function used is gamma(nu) = n*u with n the dimension of the problem and u the unit-roundoff of the FP format used for computation.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\n\nHPFormat = Float64\ngamma(n,u) = HPFormat(sqrt(n)*u) # user-defined gamma funciton. Value returned  must be ::HPFormat\nFormats = [Float32]\nf(x) = sum(x.^2)\nx0 = ones(Float32,1000)\nmpmodel_ud = FPMPNLPModel(f,x0,Formats; γfunc = gamma) # build mpmodel with user defined gamma function\nstats_ud = MPR2(mpmodel_ud) # run MPR2 with user define gamma function for model decrease and norm computation","category":"page"},{"location":"tutorial_MPR2_basic_use/#**Lack-of-Precision**","page":"MPR2 Tutorial: Basic Use ","title":"Lack of Precision","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"The default implementation of MPR2 stops when the condition on the objective evaluation error or the mu indicator fails with the highest precision evaluations (see Lines 7,8,10 of MPR2 algorithm in section MPR2 Algorithm Broad Description). If this happens, MPR2 returns the ad-hoc warning message. The user can tune MPR2 parameters to try to avoid such early stop via the structure MPR2Params and provide it as a keyword argument to solve!. Typically, ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"If the objective error is too big: the user should increase eta_0 parameter.\nIf mu_k is too big: the user should increase kappa_m parameter.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"The user has to make sure that the parameters respect the convergence conditions (see section Conditions on Parameters).","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Example: Lack of Precision and Parameters Selection","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\nusing IntervalArithmetic\n\nsetrounding(Interval,:accurate)\nFP = [Float16, Float32] # selected FP formats, max eval precision is Float64\nf(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nx = Float32.(1.5*ones(2)) # initial point\nHPFormat = Float64\nMPmodel = FPMPNLPModel(f,x,FP,HPFormat = HPFormat,obj_int_eval = true, grad_int_eval = true);\nsolver = MPR2Solver(MPmodel);\nstat = MPR2(MPmodel) ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Running the above code block returns a warning indicating that R2 stops because the error on the objective function is too big to ensure convergence. The problem can be overcome in this example by tolerating more error on the objective by increasing eta_0.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"η₀ = 0.1 # greater than default value 0.01\nη₁ = 0.3\nη₂ = 0.7\nκₘ = 0.1\nγ₁ = Float16(1/2) # must be FP format of lowest evaluation precision for numerical stability\nγ₂ = Float16(2) # must be FP format of lowest evaluation precision for numerical stability\nparam = MPR2Params(η₀,η₁,η₂,κₘ,γ₁,γ₂)\nstat = MPR2(MPmodel,par = param,max_iter = 10000,obj_int_eval = true, grad_int_eval = true) ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Now MPR2 converges to a first order critical point since we tolerate enough error on the objective evaluation.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Example: Avoiding Lack of Precision With Relative Errors","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"If the FPMPNLPModel model is instanciated with relative error model (see FPMPNLPModel documentation), one can simply set the error to zero with the highest FP format used for evaluation.  This avoids the algorithm to stop due to lack of precision, but MPR2 is not guaranteed to converge to a first order critical point.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\n\nFP = [Float16, Float32] # selected FP formats, max eval precision is Float64\nf(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nx = Float32.(1.5*ones(2)) # initial point\nHPFormat = Float64\nomega = [0.01,0.0] # 1% error with Float16, no error with Float32\nMPmodel = FPMPNLPModel(f,x,FP; ωfRelErr = omega, ωgRelErr = omega, HPFormat = HPFormat);\nsolver = MPR2Solver(MPmodel);\nstat = MPR2(MPmodel) ","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Not that even if evaluation error for objective and gradient is set to zero as in the above code, early stop due to lack of precision might still occur since mu indicator is not null even when gradient error omega_g is null (see MPR2 description section).","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"Example: Last Resort","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"If early stop due to lack of precision occurs even when modifying MPR2's parameters and setting the evaluation error to zero (above 2 examples), the user can provide its own function gamma to further decrease mu indicator (by decreasing gamma(nu) and alpha(nu)). For example, the user can simply set gamma to 0, i.e. supposing that model reduction and norm computation are performed exactly. This however might increase numerical instability of MPR2.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\n\nFP = [Float16, Float32] # selected FP formats, max eval precision is Float64\nf(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nx = Float32.(1.5*ones(2)) # initial point\nHPFormat = Float64\nomega = [0.01,0.0] # 1% error with Float16, no error with Float32\ngamma(n,u) = HPFormat(0) # gamma function set to zero\nMPmodel = FPMPNLPModel(f,x,FP; ωfRelErr = omega, ωgRelErr = omega, γfunc = gamma , HPFormat = HPFormat);\nsolver = MPR2Solver(MPmodel);\nstat = MPR2(MPmodel)","category":"page"},{"location":"tutorial_MPR2_basic_use/#**Evaluation-Counters**","page":"MPR2 Tutorial: Basic Use ","title":"Evaluation Counters","text":"","category":"section"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"MPR2 counts the number of objective and gradient evaluations are counted for each FP formats. They are stored in counters field of the FPNLPModel structure. The counters field is a MPCounters.","category":"page"},{"location":"tutorial_MPR2_basic_use/","page":"MPR2 Tutorial: Basic Use ","title":"MPR2 Tutorial: Basic Use ","text":"using MultiPrecisionR2\n\nFP = [Float16, Float32, Float64]\nf(x) = sum(x.^2) \nx0 = ones(10)\nHPFormat = Float64\nMPmodel = FPMPNLPModel(f,x0,FP,HPFormat = HPFormat);\nMPR2(MPmodel,verbose=1)\nMPmodel.counters.neval_obj # numbers of objective evaluations \nMPmodel.counters.neval_grad # numbers of gradient evaluations","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [MultiPrecisionR2]","category":"page"},{"location":"reference/#MultiPrecisionR2.FPMPNLPModel","page":"Reference","title":"MultiPrecisionR2.FPMPNLPModel","text":"FPMPNLPModel(Model::AbstractNLPModel{D,S},FPList::Vector{K}; kwargs...) where {D,S,K<:DataType}\nFPMPNLPModel(f,x0, FPList::Vector{DataType}; kwargs...)\n\nFloating-Point Multi-Precision Non Linear Model (FPMPNLPModel) structure. This structure is intended to extend NLPModel structure to multi-precision.\n\nProvides errors on objective function and grandient evaluation (see objerrmp and graderrmp).\n\nThe error models are :\n\nojective: |fl(f(x)) - f(x)| ≤ ωf\ngradient: ||fl(∇f(x)) - ∇f(x)||₂ ≤ ||fl(∇f(x))||₂ ωg.\n\nωf and ωg are evaluated either using:\n\ninterval analysis (can be very slow)\nbased on relative error assumption (see ωfRelErr and ωgRelErr field description below)   \n\nFields\n\nModel::AbstractNLPModel : NLPModel\nFPList::Vector{DataType} : List of floating point formats\nEpsList::Vector{H} : List of machine epsilons of the floating point formats in FPList\nUList::Vector{H} : List of unit round-off of the floating point formats in FPList\nγfunc : callback function for dot product rounding error parameter |γ|, |fl(x.y) - x.y| ≤ |x|.|y| γ. Expected signature is γfunc(n::Int,u::H) and output is H. Default callback γfunc(n::Int,u::H) = n*u is implemented upon instantiation. \nωfRelErr::Vector{H} : List of relative error factor for objective function evaluation for formats in FPList. Error model is |f(x)-fl(f(x))| ≤ ωfRelErr * |fl(f(x))| \nωgRelErr::Vector{H} : List of relative error factor for gradient evaluation for formats in FPList. Error model is ||∇f(x)-fl(∇f(x))||₂ ≤ ωgRelErr * ||fl(∇f(x))||₂ \nObjEvalMode::Int : Evalutation mode for objective and error. Set automatically upon instantiation. Possible values:\nINT_ERR : interval evaluation of objective (chosen as middle of the interval) and error\nREL_ERR : classical evaluation and use relative error model (with ωfRelErr value)\nGradEvalMode::Int : Evalutation mode for gradient and error. Set automatically upon instantiation. Possible values:\nINT_ERR : interval evaluation of gradient (chosen as middle of interval vector) and error\nREL_ERR : classical evaluation and use relative error model (with ωgRelErr value)\n\nConstructors:\n\nFPMPModel(Model, FPList; kwargs...) :\nModel::AbstractNLPModel: Base model\nFPList::Vector{DataType}: List of FP formats that can be used for evaluations\nFPModels(f,x0::Vector,FPList::Vector{DataType}; kwargs...) : Instanciate a ADNLPModel with f and x0 and call above constructor\nf : objective function\nx0 : initial solution\nFPList::Vector{DataType}: List of FP formats that can be used for evaluations\n\nKeyword arguments:\n\nHPFormat=Float64 : high precision format (must be at least as accurate as FPList[end])\nγfunc=nothing : use default if not provided (see Fields section above)\nωfRelErr=HPFormat.(sqrt.(eps.(FPList))): use relative error model by default for objective evaluation\nωgRelErr=HPFormat.(sqrt.(eps.(FPList))): use relative error model by default for gradient evaluation\nobj_int_eval = false : if true, use interval arithmetic for objective value and error evaluation\ngrad_int_eval = false : if true, use interval arithmetic for gradient value and error evaluation\n\nChecks upon instantiation\n\nSome checks are performed upon instantiation. These checks include:\n\nLength consistency of vector fields:  FPList, EpsList\nHPFormat is at least as accurate as the highest precision floating point format in FPList. Ideally HPFormat is more accurate to ensure the numerical stability of MPR2 algorithm.\nInterval evaluations: it might happen that interval evaluation of objective function and/or gradient is type-unstable or returns an error. The constructor returns an error in this case. This type of error is most likely due to IntervalArithmetic.jl.\nFPList is ordered by increasing floating point format accuracy\n\nThese checks can return @warn or error. \n\nExamples\n\nusing MultiPrecisionR2\n\nT = [Float16, Float32]\nf(x) = x[1]^2 + x[2]^2\nx = zeros(2)\nmpnlp = FPMPNLPModel(f,x0,T)\n\n\n\nusing MultiPrecisionR2\nusing OptimizationProblems\nusing OptimizationProblems.ADNLPProblems\nusing ADNLPModels\nusing BFloat16s\n\nT = [BFloat16, Float16, Float32]\nnlp = woods()\nmpnlp = (nlp,T)\n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.MPCounters","page":"Reference","title":"MultiPrecisionR2.MPCounters","text":"MPCounters\n\nStruct for storing the number of function evaluations with each floating point format. The fields are the same as NLPModels.Counters, but are Dict{DataType,Int} instead of Int.\n\n\n\nMPCounters(FPformats::Vector{DataType})\n\nCreates an empty MPCounters struct for types in the vector FPformats.\n\nExample\n\nusing MultiPrecisionR2.jl\nFPformats = [Float16, Float32]\ncntrs = MPCounters(FPformats)\n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.MPR2Params","page":"Reference","title":"MultiPrecisionR2.MPR2Params","text":"MPR2Params(LPFormat::DataType, HPFormat::DataType)\n\nMPR2 parameters structure.\n\nFields\n\nη₀::H : controls objective function error tolerance, convergence condition is ωf ≤ η₀ ΔT (see FPMPNLPModel for details on ωf)\nη₁::H : step successful if ρ ≥ η₁ (update incumbent)\nη₂::H : step very successful if ρ ≥ η₂ (decrease σ ⟹ increase step length)\nκₘ::H : tolerance on gradient evaluation error, μ ≤ κₘ (see computeMu) \nγ₁::L : σk+1 = σk * γ₁ if ρ ≥ η₂\nγ₂::L : σk+1 = σk * γ₂ if ρ < η₁\n\nParameters\n\nH must correspond to MPnlp.HPFormat with MPnlp given as input of MPR2\nL must correspond to MPnlp.FPList[1], i.e the lowest precision floating point format used by MPnlp given as input of MPR2\n\nConditions\n\nParameters must statisfy the following conditions:\n\n0 ≤ η₀ ≤ 1/2*η₁\n0 ≤ η₁ ≤ η₂ < 1\nη₀+κₘ/2 ≤0.5*(1-η₂)\nη₂<1 \n0<γ₁<1<γ₂\n\nInstiates default values:\n\nη₀::H = 0.01\nη₁::H = 0.02\nη₂::H = 0.95\nκₘ::H = 0.02 \nγ₁::L = 2^(-2)\nγ₂::L = 2\n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.MPR2Precisions","page":"Reference","title":"MultiPrecisionR2.MPR2Precisions","text":"MPR2Precisions(π::Int)\n\nFP format index storage structure.\n\nStores FP formats index in FPMPNLPModel.FPList of obj, grad, model reduction and norms evaluations, and FP format index of MPR2 algorithm vector variables. \n\nFields\n\nπx::Int : Current FP format of current incumbent x\nπnx::Int : FP format used for x norm evaluation\nπs::Int : Current FP format of steps\nπns::Int : FP format used for s norm evaluation\nπc::Int : Current FP format of candidate c\nπf::Int : FP format used for objective evaluation at x\nπf⁺::Int : FP format used for objective evaluation at c\nπg::Int : FP format used for gradient evaluation at c\nπΔ::Int : FP format used for model reduction computation\n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.MPR2Precisions-Tuple{Int64}","page":"Reference","title":"MultiPrecisionR2.MPR2Precisions","text":"MPR2Precisions(π::Int)\n\nFP format index storage structure.\n\nStores FP formats index in FPMPNLPModel.FPList of obj, grad, model reduction and norms evaluations, and FP format index of MPR2 algorithm vector variables. \n\nFields\n\nπx::Int : Current FP format of current incumbent x\nπnx::Int : FP format used for x norm evaluation\nπs::Int : Current FP format of steps\nπns::Int : FP format used for s norm evaluation\nπc::Int : Current FP format of candidate c\nπf::Int : FP format used for objective evaluation at x\nπf⁺::Int : FP format used for objective evaluation at c\nπg::Int : FP format used for gradient evaluation at c\nπΔ::Int : FP format used for model reduction computation\n\n\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.MPR2Solver","page":"Reference","title":"MultiPrecisionR2.MPR2Solver","text":"MPR2Solver(MPnlp::FPMPNLPModel)\n\nSolver structure containing all the variables necessary to MRP2.\n\nFields:\n\nx::T: incumbent\ng::T : gradient\ns::T : step \nc::T : candidate\nπ::MPR2Precisions : FP format indices (precision) structure\np::MPR2Params : MPR2 parameters\nx_norm::H : norm of x\ns_norm::H : norm of s\ng_norm::H : norm of g\nΔT::H : model decrease\nρ::H : success ratio\nϕ::H : guaranteed upper bound on ||x||/||s||\nϕhat::H : computed value of ||x||/||s||\nμ::H : error indicator\nf::H : objective value at x\nf⁺::H : objective value at c\nωf::H : objective evaluation error at x, |f(x) - fl(f(x))| <= ωf\nωf⁺::H : objective evaluation error at c, |f(c) - fl(f(c))| <= ωf⁺\nωg::H : gradient evaluation error at c, ||∇f(c) - fl(∇f(c))||₂ <= ωg||fl(∇f(c))||₂\nωfBound::H : error tolerance on objective evaluation\nσ::H : regularization parameter\nπmax::Int : number of FP formats available for evaluations\ninit::Bool : initialized with true, set to false when entering main loop \n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.CheckMPR2ParamConditions-Tuple{MPR2Params}","page":"Reference","title":"MultiPrecisionR2.CheckMPR2ParamConditions","text":"CheckMPR2ParamConditions(p::MPR2Params{H})\n\nCheck if the MPR2 parameters conditions are satified. See MPR2Params for parameter conditions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.GradIntervalEval_test-Tuple{NLPModels.AbstractNLPModel, AbstractArray}","page":"Reference","title":"MultiPrecisionR2.GradIntervalEval_test","text":"GradIntervalEval_test(nlp::AbstractNLPModel,FPList::AbstractArray)\n\nTest interval evaluation of gradient for all FP formats. Test fails and return an error if interval evaluation returns an error. See [FPMPNLPModel], [AbstractNLPModel]\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.GradTypeStableTest-Tuple{NLPModels.AbstractNLPModel, AbstractArray}","page":"Reference","title":"MultiPrecisionR2.GradTypeStableTest","text":"GradTypeStableTest(nlp::AbstractNLPModel, FPList::AbstractArray)\n\nTests if objective evaluation of nlp is type stable for FP format in FPList\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.MPR2-Tuple{FPMPNLPModel}","page":"Reference","title":"MultiPrecisionR2.MPR2","text":"MPR2(MPnlp; kwargs...)\n\nAn implementation of the quadratic regularization algorithm with dynamic selection of floating point format for objective and gradient evaluation, robust against finite precision rounding errors. Type parameters are: S::AbstractVector, H::AbstractFloat, T::AbstractFloat, E::DataType \n\nArguments\n\nMPnlp::FPMPNLPModel : Multi precision model, see FPMPNLPModel\n\nKeyword agruments:\n\nx₀::S = MPnlp.Model.meta.x0 : initial guess \npar::MPR2Params = MPR2Params(MPnlp.FPList[1],H) : MPR2 parameters, see MPR2Params for details\natol::H = H(sqrt(eps(T))) : absolute tolerance on first order criterion \nrtol::H = H(sqrt(eps(T))) : relative tolerance on first order criterion\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_iter::Int = 1000 : maximum number of iteration allowed\nσmin::T = sqrt(T(MPnlp.EpsList[end])) : minimal value for regularization parameter. Value must be representable in any of the floating point formats of MPnlp. \nverbose::Int=0 : display iteration information if > 0\ne::E : user defined structure, used as argument for compute_f_at_x!, compute_f_at_c! compute_g! and recompute_g! callback functions.\ncompute_f_at_x! : callback function to select precision and compute objective value and error bound at the current point. Allows to reevaluate the objective at x if more precision is needed.\ncompute_f_at_c! : callback function to select precision and compute objective value and error bound at candidate.\ncompute_g! : callback function to select precision and compute gradient value and error bound. Called at the end of main loop.\nrecompute_g! : callback function to select precision and recompute gradient value if more precision is needed. Called after step, candidate and model decrease computation in main loop.\nselectPic! : callback function to select FP format of c at the next iteration\n\nOutputs\n\nGenericExecutionStats: execution stats containing information about algorithm execution (nb. of iteration, termination status, ...). See SolverCore.jl\n\nExample\n\nT = [Float32, Float64]\nf(x) = sum(x.^2)\nx = ones(Float32,2)\nmpnlp = FPMPNLPModel(f,x,T)\nMPR2(mpnlp)\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.ObjIntervalEval_test-Tuple{NLPModels.AbstractNLPModel, AbstractArray}","page":"Reference","title":"MultiPrecisionR2.ObjIntervalEval_test","text":"ObjIntervalEval_test(nlp::AbstractNLPModel,FPList::AbstractArray)\n\nTest interval evaluation of objective for all formats in FPList. Test fails and return an error if interval evaluation returns an error. See [FPMPNLPModel], [AbstractNLPModel]\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.ObjTypeStableTest-Tuple{NLPModels.AbstractNLPModel, AbstractArray}","page":"Reference","title":"MultiPrecisionR2.ObjTypeStableTest","text":"ObjTypeStableTest(nlp::AbstractNLPModel, FPList::AbstractArray)\n\nTests if objective evaluation of nlp is type stable for FP format in FPList.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.check_overflow-Tuple{AbstractFloat}","page":"Reference","title":"MultiPrecisionR2.check_overflow","text":"check_overflow(f)\n\nf::AbstractFloat: Returns true if f is inf or nan, false otherwise.\nf::Interval : Returns true if diam(f) is inf or nan, false otherwise.\nf::AbstractVector{AbstractFloat} : Returns true if on element of f is inf or nan, false otherwise.\nf::AbstractVector{Interval}: Returns true if on element of diam(f) is inf, false otherwise.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.computeCandidate!-Union{Tuple{T}, Tuple{T, T, T, Vector{DataType}, MPR2Precisions}} where T<:Tuple","page":"Reference","title":"MultiPrecisionR2.computeCandidate!","text":"computeCandidate!(c::T, x::T, s::T, FP::Vector{DataType}, π::MPR2Precisions) where {T <: Tuple}\n\nCompute candidate with proper FP format to avoid underflow and overflow\n\nArguments\n\nx::T : incumbent \ns::T : step\nFP::Vector{Int} : Available floating point formats\nπ::MPR2Precisions : FP format index storage structure\n\nModified arguments:\n\nc::T : updated with candidate\nπ::MPR2Precisions : π.πc updated with FP format index used for computation\n\nOutputs:\n\n::bool : false if over/underflow occur with highest precision FP format, true otherwise\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.computeModelDecrease!-Union{Tuple{T}, Tuple{H}, Tuple{T, T, MPR2Solver{T, H}, Vector{DataType}, MPR2Precisions}} where {H, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.computeModelDecrease!","text":"computeModelDecrease!(g::T,s::T,solver::MPR2Solver,FP::Vector{DataType},π::MPR2Precisions) where {T <: Tuple}\n\nCompute model decrease with FP format avoiding underflow and overflow\n\nArguments\n\ng::T : gradient \ns::T : step\nsolver::MPR2Solver : solver structure, stores intermediate variables\nFP::Vector{Int} : Available floating point formats\nπ::MPR2Precisions : FP format index storage structure\n\nModified Arguments\n\nsolver::MPR2Solver : solver.ΔT updated \nπ::MPR2Precisions : π.πΔ updated \n\nOutputs\n\n::bool : false if over/underflow occur with highest precision FP format, true otherwise\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.computeMu-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel, MPR2Solver{T, H}}} where {T, H}","page":"Reference","title":"MultiPrecisionR2.computeMu","text":"computeMu(m::FPMPNLPModel, solver::MPR2Solver{T,H}; π::MPR2Precisions = solver.π)\n\nCompute μ value for gradient error ωg, ratio ϕ = ||x||/||s|| and rounding error models\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.computeStep!-Union{Tuple{H}, Tuple{T}, Tuple{T, T, H, Vector{DataType}, MPR2Precisions}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.computeStep!","text":"computeStep!(s::T, g::T, σ::H, FP::Vector{DataType}, π::MPR2Precisions) where {T <: Tuple, H}\n\nCompute step with proper FP format to avoid underflow and overflow\n\nArguments\n\ns::T : step \ng::T : gradient \nπg::Int : g FP index\nσ::H : regularization parameter\nFP::Vector{Int} : Available floating point formats\nπ::MPR2Precisions : FP format index storage structure \n\nModified arguments :\n\ns::T : updated with computed step\nπ::MPR2Precisions : π.πs updated with FP format used for step computation\n\nOutputs\n\n::bool : false if over/underflow occurs, true otherwise\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.compute_f_at_c_default!-Union{Tuple{T}, Tuple{E}, Tuple{H}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, SolverCore.GenericExecutionStats, E}} where {H, E, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.compute_f_at_c_default!","text":"compute_f_at_c_default!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, stats::GenericExecutionStats, e::E)\n\nCompute objective function at the candidate. Updates related fields of solver.\n\nOutputs:\n\n::bool: returns false if couldn't reach sufficiently small evaluation error or overflow occured. \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.compute_f_at_x_default!-Union{Tuple{T}, Tuple{E}, Tuple{H}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, SolverCore.GenericExecutionStats, E}} where {H, E, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.compute_f_at_x_default!","text":"compute_f_at_x_default!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, stats::GenericExecutionStats, e::E)\n\nCompute objective function at the current incumbent. Updates related fields of solver.\n\nOutputs:\n\n::bool : returns false if couldn't reach sufficiently small evaluation error or overflow occured. \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.compute_g_default!-Union{Tuple{T}, Tuple{E}, Tuple{H}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, SolverCore.GenericExecutionStats, E}} where {H, E, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.compute_g_default!","text":"compute_g_default!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, stats::GenericExecutionStats, e::E)\n\nCompute gradient at x if solver.init == true (first gradient eval outside of main loop), at c otherwise.\n\nOutputs:\n\n::bool : always true (needed to comply with callback template)\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.get_id-Tuple{FPMPNLPModel, DataType}","page":"Reference","title":"MultiPrecisionR2.get_id","text":"get_id(m::FPMPNLPModel, FPFormat::DataType)\n\nReturns the index of FPFormat in m.FPList. \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.gradReachPrec!-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel{H}, T, T, H}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.gradReachPrec!","text":"gradReachPrec!(m::FPMPNLPModel{H}, x::T, g::T, err_bound::H; π::Int = 1) where {T <: Tuple, H}\ngradReachPrec(m::FPMPNLPModel{H}, x::T, err_bound::H; π::Int = 1) where {T <: Tuple, H}\n\nEvaluates gradient and increase model precision to reach necessary error bound or to avoid overflow.\n\nArguments\n\nm::FPMPNLPModel{H}: multi precision model\nx::T: tuple containing value of x in the FP formats of FPList\ng::T: gradient container\nπ: Initial ''guess'' precision level that can provide evaluation error lower than err_bound, use 1 by default (lowest precision)\n\nOutputs\n\n(g): gradient, returned only with gradReachPrec call  \nωg: objective evaluation error\nid: precision level used for evaluation\n\nModified\n\n(g): updated with the gradient value, only with gradReachPrec! call\n\nThere is no guarantee that ωg ≤ err_bound. This case happens if the highest precision FP format is not accurate enough.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.gradReachPrec-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel{H}, T, H}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.gradReachPrec","text":"gradReachPrec!(m::FPMPNLPModel{H}, x::T, g::T, err_bound::H; π::Int = 1) where {T <: Tuple, H}\ngradReachPrec(m::FPMPNLPModel{H}, x::T, err_bound::H; π::Int = 1) where {T <: Tuple, H}\n\nEvaluates gradient and increase model precision to reach necessary error bound or to avoid overflow.\n\nArguments\n\nm::FPMPNLPModel{H}: multi precision model\nx::T: tuple containing value of x in the FP formats of FPList\ng::T: gradient container\nπ: Initial ''guess'' precision level that can provide evaluation error lower than err_bound, use 1 by default (lowest precision)\n\nOutputs\n\n(g): gradient, returned only with gradReachPrec call  \nωg: objective evaluation error\nid: precision level used for evaluation\n\nModified\n\n(g): updated with the gradient value, only with gradReachPrec! call\n\nThere is no guarantee that ωg ≤ err_bound. This case happens if the highest precision FP format is not accurate enough.\n\n\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.graderrmp!-Union{Tuple{V}, Tuple{S}, Tuple{H}, Tuple{FPMPNLPModel{H}, V, V}} where {H, S, V<:AbstractVector{S}}","page":"Reference","title":"MultiPrecisionR2.graderrmp!","text":"graderrmp(m::FPMPNLPModel, x::V)\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V) where {H, S, V<:AbstractVector{S}}\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V, ::Val{INT_ERR}) where {H, S, V<:AbstractVector{S}}\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V, ::Val{REL_ERR}) where {H, S, V<:AbstractVector{S}}\n\nEvaluates the gradient g and the relative evaluation error ωg. The two functions with the extra argument ::Val{INT_ERR} and ::Val{REL_ERR} handles the interval and \"classic\" evaluation of the objective and the error, respectively.\n\nArguments\n\nm::FPMPNLPModel : multi-precision model\nx::V: where the gradient is evaluated\ng::V: container for gradient\n\nOutputs\n\ng::Vector{S}: gradient value, only returned with graderrmp.\nωg <: AbstractFloat: evaluation error satisfying: ||∇f(x) - fl(∇f(x))||₂ ≤ ωg||g||₂ with fl() the floating point evaluation.\n\nNote: ωg FP format may be different than S\n\nModified\n\ng::V: updated with gradient value\n\nOverflow cases:\n\nInterval evaluation: if at least one element of g has infinite diameter, returns [0]ⁿ, Inf\nClassical evaluation: if at least one element of g overflows, returns g, Inf \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.graderrmp-Union{Tuple{V}, Tuple{S}, Tuple{FPMPNLPModel, V}} where {S, V<:AbstractVector{S}}","page":"Reference","title":"MultiPrecisionR2.graderrmp","text":"graderrmp(m::FPMPNLPModel, x::V)\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V) where {H, S, V<:AbstractVector{S}}\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V, ::Val{INT_ERR}) where {H, S, V<:AbstractVector{S}}\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V, ::Val{REL_ERR}) where {H, S, V<:AbstractVector{S}}\n\nEvaluates the gradient g and the relative evaluation error ωg. The two functions with the extra argument ::Val{INT_ERR} and ::Val{REL_ERR} handles the interval and \"classic\" evaluation of the objective and the error, respectively.\n\nArguments\n\nm::FPMPNLPModel : multi-precision model\nx::V: where the gradient is evaluated\ng::V: container for gradient\n\nOutputs\n\ng::Vector{S}: gradient value, only returned with graderrmp.\nωg <: AbstractFloat: evaluation error satisfying: ||∇f(x) - fl(∇f(x))||₂ ≤ ωg||g||₂ with fl() the floating point evaluation.\n\nNote: ωg FP format may be different than S\n\nModified\n\ng::V: updated with gradient value\n\nOverflow cases:\n\nInterval evaluation: if at least one element of g has infinite diameter, returns [0]ⁿ, Inf\nClassical evaluation: if at least one element of g overflows, returns g, Inf \n\n\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.hess_coord_of_mp!-Union{Tuple{T}, Tuple{FPMPNLPModel, T, T, T}} where T<:Tuple","page":"Reference","title":"MultiPrecisionR2.hess_coord_of_mp!","text":"hess_coord_of_mp(m::FPMPNLPModel, x::T; obj_weight::Real = 1.0, π::Int = 1) where {T <: Tuple}\nhess_coord_of_mp(m::FPMPNLPModel, x::T, y::T; obj_weight::Real = 1.0, π::Int = 1) where {T <: Tuple}\nhess_coord_of_mp!(m::FPMPNLPModel, x::T, vals::T; obj_weight::Real = 1.0, π::Int = 1) where {T <: Tuple}\nhess_coord_of_mp!(m::FPMPNLPModel, x::T, y::T, vals::T; obj_weight::Real = 1.0, π::Int = 1) where {T <: Tuple}\n\nCall hess_coord! recursively from the π-th element of the T <: Tuple arguments until vals does not overflow.\n\nModified argument\n\nvals::T\n\nOutputs\n\nvals::T: only returned with hess_coord_of_mp\nid::Int : index of updated vals element\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.hprod_of_mp!-Union{Tuple{T}, Tuple{FPMPNLPModel, Vararg{T, 4}}} where T<:Tuple","page":"Reference","title":"MultiPrecisionR2.hprod_of_mp!","text":"hprod_of_mp(m::FPMPNLPModel, x::T, v::T; obj_weight::Real = 1.0, π::Int = 1) where {T <: Tuple}\nhprod_of_mp(m::FPMPNLPModel, x::T, y::T, v::T; obj_weight::Real = 1.0, π::Int = 1) where {T <: Tuple}\nhprod_of_mp!(m::FPMPNLPModel, x::T, v::T, Hv::T; obj_weight::Real = 1.0, π::Int = 1) where {T <: Tuple}\nhprod_of_mp!(m::FPMPNLPModel, x::T, y::T, v::T, Hv::T; obj_weight::Real = 1.0, π::Int = 1) where {T <: Tuple}\n\nCall hprod! recursively from the π-th element of the tuple arguments until Hv does not overflow.\n\nModified argument\n\nHv::T\n\nOutputs\n\nHv::T: only returned with hprod_of_mp\nid::Int : index of updated Hv element\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.increment!-Tuple{AbstractMPNLPModel, Symbol, DataType}","page":"Reference","title":"MultiPrecisionR2.increment!","text":"increment!(nlp, s)\n\nIncrement counter s of problem nlp.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.objReachPrec-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel{H}, T, H}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.objReachPrec","text":"objReachPrec(m::FPMPNLPModel{H}, x::T, err_bound::H; π::Int = 1) where {T <: Tuple, H}\n\nEvaluates objective and increase model precision to reach necessary error bound or to avoid overflow.\n\nArguments\n\nm::FPMPNLPModel{H}: multi precision model\nx::T: tuple containing value of x in the FP formats of FPList\nerr_bound::H : evaluation error tolerance\nπ::Int: Initial ''guess'' for FP format that can provide evaluation error lower than err_bound, use 1 by default (lowest precision FP format)\n\nOutputs\n\nf: objective value at x\nωf: objective evaluation error\nid: precision level used for evaluation\n\nThere is no guarantee that ωf ≤ err_bound, happens if highest precision FP format is not accurate enough.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.objerrmp-Union{Tuple{S}, Tuple{FPMPNLPModel, AbstractVector{S}}} where S","page":"Reference","title":"MultiPrecisionR2.objerrmp","text":"objerrmp(m::FPMPNLPModel, x::AbstractVector{T})\nobjerrmp(m::FPMPNLPModel, x::AbstractVector{S}, ::Val{INT_ERR})\nobjerrmp(m::FPMPNLPModel, x::AbstractVector{S}, ::Val{REL_ERR})\n\nEvaluates the objective and the evaluation error. The two functions with the extra argument ::Val{INT_ERR} and ::Val{REL_ERR} handles the interval and \"classic\" evaluation of the objective and the error, respectively.\n\nArguments\n\nx::Vector{S}: where to evaluate the objective, can be either a vector of AbstractFloat or a vector of Intervals.\n\nOutputs\n\nfl(f(x)): finite-precision evaluation of the objective at x\nωf <: AbstractFloat: evaluation error, |f(x)-fl(f(x))| ≤ ωf with fl() the floating point evaluation.\n\nOverflow cases:\n\nInterval evaluation: overflow occurs if the diameter of the interval enclosing f(x) is Inf. Returns 0, Inf\nClassical evaluation:\nIf obj(x) = Inf: returns: Inf, Inf\nIf obj(x) != Inf and ωf = Inf, returns: obj(x), Inf  \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.recomputeMu!-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, MPR2Precisions}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.recomputeMu!","text":"recomputeMu!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, πr::MPR2Precisions) where {T <: Tuple, H}\n\nRecompute mu based on new precision levels.  Performs only necessary operations to recompute mu.  Possible operations are:\n\nrecompute candidate with higher prec FP format to decrease u\nrecompute ϕhat and ϕ with higher FP format for norm computation of x and s\nrecompute step with greater precision: decrease μ denominator\nrecompute gradient with higher precision to decrease ωg\nrecompute model reduction with higher precision to decrease αfunc(n,U[π.πΔ])\n\nDoes not make the over/underflow check as in main loop, since it is a repetition of the main loop with higher precisions and these issue shouldn't occur\n\nOutputs:\n\ng_recompute::Bool : true if gradient has been modified, false otherwise\n\nSee recomputeMuPrecSelection!\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.recomputeMuPrecSelection!-Tuple{MPR2Precisions, MPR2Precisions, Any}","page":"Reference","title":"MultiPrecisionR2.recomputeMuPrecSelection!","text":"recomputeMuPrecSelection!(π::MPR2Precisions, πr::MPR2Precisions, πmax)\n\nDefault strategy to select new precisions to recompute μ in the case where μ > κₘ. Return false if no precision can be increased.\n\nModified arguments:\n\nπr: contains new precision that will be used to recompute mu, see recomputeMu!\n\nOuptputs\n\nmax_prec::bool : return true if maximum precision levels have been reached\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.recompute_g_default!-Union{Tuple{T}, Tuple{E}, Tuple{H}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, SolverCore.GenericExecutionStats, E}} where {H, E, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.recompute_g_default!","text":"recompute_g_default!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, stats::GenericExecutionStats, e::E)\n\nIncrease operation precision levels until sufficiently small μ indicator is achieved. See also computeMu, recomputeMuPrecSelection!, recomputeMu!\n\nOutputs:\n\n::bool : returns false if couldn't reach sufficiently small evaluation error or overflow occured. \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.reset!-Tuple{AbstractMPNLPModel}","page":"Reference","title":"MultiPrecisionR2.reset!","text":"reset!(mpnlp::AbstractMPNLPModel)\n\nReset evaluation count and model data (if appropriate) in mpnlp.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.reset!-Tuple{MPCounters}","page":"Reference","title":"MultiPrecisionR2.reset!","text":"reset!(counters::MPCounters)\n\nReset evaluation counters\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.selectPic_default!-Tuple{MPR2Solver}","page":"Reference","title":"MultiPrecisionR2.selectPic_default!","text":"selectPic_default!(π::MPR2Precisions)\n\nDefault strategy for selecting FP format of candidate for the next evaluation. Updates solver.π.πf⁺.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.selectPif!-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, H}} where {T, H}","page":"Reference","title":"MultiPrecisionR2.selectPif!","text":"selectPif!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, ωfBound::H)\n\nSelect a precision for objective evaluation for candidate based on predicted evaluation error. Evaluation is predicted as:\n\nRelative error:\nPredicted value of objective at c: f(c) ≈ f(x) - ΔTk\nRelative error model: ωf(c) = |f(c)| * RelErr\nInterval error:\nPredicted value of objective at c: f(c) ≈ f(x) - ΔTk\nInterval evaluation error is proportional to f(x)\nInterval evaluation error depends linearly with unit-roundoff \nOther: Lowest precision that does not cast candidate in a lower prec FP format and f(c) predicted does not overflow\n\nModified arguments:\n\nsolver.π.πf⁺: updated with FP format index chosen for objective evaluation\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.sum_counters-Tuple{AbstractMPNLPModel}","page":"Reference","title":"MultiPrecisionR2.sum_counters","text":"sum_counters(mpnlp)\n\nSum all counters of problem mpnlp except cons, jac, jprod and jtprod.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.sum_counters-Tuple{MPCounters}","page":"Reference","title":"MultiPrecisionR2.sum_counters","text":"sum_counters(c::MPCounters)\n\nSum all counters of c except cons, jac, jprod and jtprod.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.umpt!-Union{Tuple{S}, Tuple{Tuple, Vector{S}}} where S","page":"Reference","title":"MultiPrecisionR2.umpt!","text":"umpt!(x::Tuple, y::Vector{S})\n\nUpdate the elements of the multi precision containers x with the value y. Only the elements of x of FP formats with precision greater or equal than y are updated (avoid rounding error).\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.update_struct!-Tuple{MPR2Precisions, MPR2Precisions}","page":"Reference","title":"MultiPrecisionR2.update_struct!","text":"update_struct!(str,other_str)\n\nUpdate the fields of str with the fields of other_str.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.γfunc_test_error_bound-Tuple{Int64, AbstractFloat, Any}","page":"Reference","title":"MultiPrecisionR2.γfunc_test_error_bound","text":"γfunc_test_error_bound(n::Int,eps::AbstractFloat,γfunc)\n\nTests if γfunc callback provides strictly less than 100% error for dot product error of vector of size the dimension of the problem and the lowest machine epsilon.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.γfunc_test_template-Tuple{Any}","page":"Reference","title":"MultiPrecisionR2.γfunc_test_template","text":"γfunc_test_template(γfunc)\n\nTests if γfunc callback function is properly implemented. Expected template: γfunc(n::Int,u::Float) -> Float\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_cons-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_cons","text":"neval_cons(nlp)\nneval_cons(nlp,T)\n\nGet the total number (all FP formats) of cons evaluations. If extra argument T is provided, returns the number of cons evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_cons_lin-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_cons_lin","text":"neval_cons_lin(nlp)\nneval_cons_lin(nlp,T)\n\nGet the total number (all FP formats) of cons evaluations. If extra argument T is provided, returns the number of cons evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_cons_nln-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_cons_nln","text":"neval_cons_nln(nlp)\nneval_cons_nln(nlp,T)\n\nGet the total number (all FP formats) of cons evaluations. If extra argument T is provided, returns the number of cons evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_grad-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_grad","text":"neval_grad(nlp)\nneval_grad(nlp,T)\n\nGet the total number (all FP formats) of grad evaluations. If extra argument T is provided, returns the number of grad evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_hess-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_hess","text":"neval_hess(nlp)\nneval_hess(nlp,T)\n\nGet the total number (all FP formats) of hess evaluations. If extra argument T is provided, returns the number of hess evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_hprod-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_hprod","text":"neval_hprod(nlp)\nneval_hprod(nlp,T)\n\nGet the total number (all FP formats) of hprod evaluations. If extra argument T is provided, returns the number of hprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jac-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jac","text":"neval_jac(nlp)\nneval_jac(nlp,T)\n\nGet the total number (all FP formats) of jac evaluations. If extra argument T is provided, returns the number of jac evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jac_lin-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jac_lin","text":"neval_jac_lin(nlp)\nneval_jac_lin(nlp,T)\n\nGet the total number (all FP formats) of jac evaluations. If extra argument T is provided, returns the number of jac evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jac_nln-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jac_nln","text":"neval_jac_nln(nlp)\nneval_jac_nln(nlp,T)\n\nGet the total number (all FP formats) of jac evaluations. If extra argument T is provided, returns the number of jac evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jcon-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jcon","text":"neval_jcon(nlp)\nneval_jcon(nlp,T)\n\nGet the total number (all FP formats) of jcon evaluations. If extra argument T is provided, returns the number of jcon evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jgrad-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jgrad","text":"neval_jgrad(nlp)\nneval_jgrad(nlp,T)\n\nGet the total number (all FP formats) of jgrad evaluations. If extra argument T is provided, returns the number of jgrad evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jhess-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jhess","text":"neval_jhess(nlp)\nneval_jhess(nlp,T)\n\nGet the total number (all FP formats) of jhess evaluations. If extra argument T is provided, returns the number of jhess evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jhprod-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jhprod","text":"neval_jhprod(nlp)\nneval_jhprod(nlp,T)\n\nGet the total number (all FP formats) of jhprod evaluations. If extra argument T is provided, returns the number of jhprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jprod-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jprod","text":"neval_jprod(nlp)\nneval_jprod(nlp,T)\n\nGet the total number (all FP formats) of jprod evaluations. If extra argument T is provided, returns the number of jprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jprod_lin-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jprod_lin","text":"neval_jprod_lin(nlp)\nneval_jprod_lin(nlp,T)\n\nGet the total number (all FP formats) of jprod evaluations. If extra argument T is provided, returns the number of jprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jprod_nln-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jprod_nln","text":"neval_jprod_nln(nlp)\nneval_jprod_nln(nlp,T)\n\nGet the total number (all FP formats) of jprod evaluations. If extra argument T is provided, returns the number of jprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jtprod-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jtprod","text":"neval_jtprod(nlp)\nneval_jtprod(nlp,T)\n\nGet the total number (all FP formats) of jtprod evaluations. If extra argument T is provided, returns the number of jtprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jtprod_lin-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jtprod_lin","text":"neval_jtprod_lin(nlp)\nneval_jtprod_lin(nlp,T)\n\nGet the total number (all FP formats) of jtprod evaluations. If extra argument T is provided, returns the number of jtprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jtprod_nln-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jtprod_nln","text":"neval_jtprod_nln(nlp)\nneval_jtprod_nln(nlp,T)\n\nGet the total number (all FP formats) of jtprod evaluations. If extra argument T is provided, returns the number of jtprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_obj-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_obj","text":"neval_obj(nlp)\nneval_obj(nlp,T)\n\nGet the total number (all FP formats) of obj evaluations. If extra argument T is provided, returns the number of obj evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"FPMPNLPModel/#FPMPNLPModel:-Multi-Precision-Models","page":"FPMPNLPModel","title":"FPMPNLPModel: Multi-Precision Models","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"FPMPNLPModel (Floating Point Multi Precision Non Linear Model) is a subtype of AbstractNLPModel and implements the NLPModel API defined in NLPModels.jl to deal with multiple floating point formats and handle evaluation errors.","category":"page"},{"location":"FPMPNLPModel/#Fields","page":"FPMPNLPModel","title":"Fields","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"FPMPNLPModel structure contains a NLPModel field and additional field related to multiple precision. The types are:","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"D < AbstractFloat\nS < AbstractVector\nH < AbstractFloat\nB < Tuple}","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Field Type Notes\nModel AbstractNLPModel{D, S} Base model\nmeta NLPModelMeta meta data\ncounters MPCounters multi-precision counters\nFPList Vector{DataType} List Floating Point formats used\nEpsList Vector{H} List of epsilon machine corresponding to FPList\nUList Vector{H} List of unit roundoff corresponding to FPList\nOFList Vector{H} List of largest representable numbers corresponding to FPList\nγfunc  dot product error model function callback.\nωfRelErr Vector{H} List of relative error factor for objective function evaluation corresponding to FP formats in FPList\nωgRelErr Vector{H} List of relative error factor for gradient evaluation corresponding to FP formats in FPList\nObjEvalMode Int objective function error evaluation mode\nGradEvalMode Int gradient error evaluation mode\nX B Container used for interval evaluation, memory pre-allocation\nG B Container used for interval evaluation, memory pre-allocation","category":"page"},{"location":"FPMPNLPModel/#γfunc","page":"FPMPNLPModel","title":"γfunc","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"γfunc callback function provides the error on dot product: xy - fl(xy)leq xygamma func(nu) with n the dimension of x and y vector and u the unit-roundoff of the FP format used to perform the dot product operation.  The expected template is: γfunc(n::Int,u::AbstractFloat) with n the dimension of the problem and u the unit roundoff of the considered FPFormat.  By default, γfunc(n,u) = n*u is used. This function is used to take finite-precision norm computation error into account, to further guarantee gradient error bounds (details below).","category":"page"},{"location":"FPMPNLPModel/#Constructors","page":"FPMPNLPModel","title":"Constructors","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"FPMPNLPModel(Model::AbstractNLPModel{D,S},FPList::Vector{K}; kwargs...) where {D,S,K<:DataType}\nFPMPNLPModel(f,x0, FPList::Vector{DataType}; kwargs...)","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Build a ADNLPModel from f and x0 and call constructor 1.","category":"page"},{"location":"FPMPNLPModel/#Keyword-arguments","page":"FPMPNLPModel","title":"Keyword arguments","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"HPFormat=Float64 : high precision format (must be at least as accurate as FPList[end]), corrensponds to H parameter after instantiation\nγfunc=nothing : use default callback if not provided (see Fields section above)\nωfRelErr=HPFormat.(sqrt.(eps.(FPList))): use relative error model by default for objective evaluation\nωgRelErr=HPFormat.(sqrt.(eps.(FPList))): use relative error model by default for gradient evaluation\nobj_int_eval = false : if true, use interval arithmetic for objective value and error evaluation\ngrad_int_eval = false : if true, use interval arithmetic for gradient value and error evaluation","category":"page"},{"location":"FPMPNLPModel/#Checks-upon-instantiation","page":"FPMPNLPModel","title":"Checks upon instantiation","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Some checks are performed upon instanctiation. These checks include:","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Length consistency of vector fields:  FPList, EpsList, UList;\nHPFormat is at least as accurate as the highest precision floating point format in FPList. Ideally HPFormat is more accurate to ensure the numerical stability;\nInterval evaluations: it might happen that interval evaluation of objective function and/or gradient is type-unstable or returns an error. The constructor returns an error in this case. This type of error is most likely due to IntervalArithmetic.jl package;\nFPList is ordered by increasing floating point format accuracy.","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"This checks can return @warn or error.","category":"page"},{"location":"FPMPNLPModel/#Evaluation-Errors","page":"FPMPNLPModel","title":"Evaluation Errors","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"FPMPNLPModel provides interfaces to evaluate the objective function and gradient and evaluation errors due to finite-precision computations. The evaluation errors on the objective function and the gradient, omega_f and omega_g, are such that","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Objective function : f(x) - fl(f(x))leq omega_f\nGradient : nabla f(x) - fl(nabla f(x))_2 leq omega_g fl(nabla f(x))_2","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"where fl() denotes the finite-precision computation with one of the FP formats in FPList.","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"FPNLPModel enables to determine evaluation errors omega_f and omega_g either:","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"based on relative error model with ωfRelErr and ωgRelErr,\nin a guaranteed way based on interval evaluation with IntervalArithmetic.jl package.","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"By default, the relative error model is used. Interval evaluation can be selected upon instantiation of FPMPNLPModel with obj_int_eval and grad_int_eval kwargs.","category":"page"},{"location":"FPMPNLPModel/#Taking-Norm-Computation-Error-Into-Account","page":"FPMPNLPModel","title":"Taking Norm Computation Error Into Account","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"For the gradient error, the 2-norm computation error due to finite-precision computations is taken into account via γfunc, such that omega_g is guaranteed. The norm computation error is given by  x_2 - fl(x_2)  leq beta(n+2u) fl(x_2), with n the dimension of the problem, u the unit-roundoff of the FP format used to perform the norm computation, and","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"beta(nu) = max(sqrtgamma func(nu)-1-1sqrtgamma func(nu)+1-1)","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"defined from γfunc.","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"The high precision format H is used to compute beta(n+2u).","category":"page"},{"location":"FPMPNLPModel/#Relative-Error-Evaluation","page":"FPMPNLPModel","title":"Relative Error Evaluation","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"By default, evaluation errors for objective and gradient are estimated with the relative error model.","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"The error models are: ","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Objective: f(x) - fl(f(x)) leqωfRelErr[id]*fl(f(x)) where id is the index of the FP format of x in FPList. objerrmp returns the value of the classic evaluation of the objective as the value of the objective, and ωfRelErr[id]*fl(f(x)) as the evaluation error omega_f, where id is the index of the FP format of x in FPList\nGradient: nabla f(x) - fl(nabla f(x))_2 leqωgRelErr[id]fl(nabla f(x))_2 where where id is the index of the FP format of x in FPList. graderrmp returns the value of the classic evaluation of the gradient as the value of the gradient, and ωgRelErr[id] as the value of the evaluation error.","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"See keyword arguments section for ωfRelErr and ωgRelErr default values. ","category":"page"},{"location":"FPMPNLPModel/#Interval-Error-Evaluation","page":"FPMPNLPModel","title":"Interval Error Evaluation","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Error of the objective (resp. gradient) evaluation can be determined with interval arithmetic. This evaluation mode can be set upon FPMPNLPModel instantiation with obj_int_eval and grad_int_eval.","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Objective: evaluating the objective with interval arithmetic provides the interval underlinefoverlinef such that underlinefleq fl(f) leq overlinef. objerrmp returns the middle of the interval as the value of the objective, that is (underlinef+overlinef)2, and returns the diameter of the interval as omega_f, that is, omega_f = (underlinef-overlinef)2\nGradient: evaluating the gradient with interval arithmetic provides the interval vector G = underlineg_1overlineg_1 times  times underlineg_noverlineg_n such that nabla f(x) in G. graderrmp returns g the middle of the interval vector as the value of the gradient. The evaluation error nabla f(x)-g is the vector of the diameters of the element of G. The error omega_g returned by graderrmp is nabla f(x)-g_2g_2 * (1+beta(n+2u))(1-beta(n+2u)), the second term takes norm computation error into account.","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Warning","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"Interval evaluation is slow compared with \"classic\" evaluation.\nAlthough guaranteed, interval bounds can be quite pessimistic.\nInterval evaluation might fail with rounding mode other than :accurate for FP formats other than Float32 and Float64. When using interval evaluation, it is recommended to call ","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"using IntervalArithmetic\nsetrounding(Interval,:accurate)","category":"page"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"before instanciating a FPMPNLPModel.","category":"page"},{"location":"FPMPNLPModel/#HPFormat","page":"FPMPNLPModel","title":"HPFormat","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"FPMPNLPModel requires a high-precision FP format, given by HPFormat constructor's keyword argument. This format is used to compute accurately a bound on finite-precision norm evaluation error, to further guaranteed the bound omega_g in interval evaluation context. The bound on norm error is computed via γfunc.","category":"page"},{"location":"FPMPNLPModel/#Interface","page":"FPMPNLPModel","title":"Interface","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"function desctiption\nFPMPNLPModel constructor (see Constructor section)\nget_id Returns index of FP format in FPList\nobjerrmp Compute objective function value and evaluation error omega_f\ngraderrmp! Compute gradient and evaluation error omega_g (no memory allocation for gradient)\ngraderrmp Compute gradient and evaluation error omega_g (memory allocation for gradient)\nobjReachPrec Compute objective and evaluation error omega_f, increases FP format precision until bound on omega_f is reached\ngradReachPrec! Compute gradient and evaluation error omega_g, increases FP format precision until bound on omega_g is reached (no mem. allocation for gradient)\ngradReachPrec Compute gradient and evaluation error omega_g, increases FP format precision until bound on omega_g is reached (mem. allocation for gradient)","category":"page"},{"location":"FPMPNLPModel/#Examples","page":"FPMPNLPModel","title":"Examples","text":"","category":"section"},{"location":"FPMPNLPModel/#Interval-Evaluations","page":"FPMPNLPModel","title":"Interval Evaluations","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"using MultiPrecisionR2\nusing IntervalArithmetic\n\nsetrounding(Interval,:accurate)\nFormats = [Float16,Float32,Float64] # FP formats\nf(x) = sum(x.^2) # objective function\ndim = 100 # problem dimentsion\nx0 = ones(100)\nmpmodel = FPMPNLPModel(f,x0,Formats) # create multi-precision model, will use interval arithmetic for evaluation error.\n\nx16 = ones(Float16,dim)\nx32 = Float32.(x16)\nx64 = x0\n\n# objective evaluation \nf16, ωf16 = objerrmp(mpmodel,x16) # Float16 objective evaluation\nf32, ωf32 = objerrmp(mpmodel,x32) # Float32 objective evaluation\nf64, ωf64 = objerrmp(mpmodel,x64) # Float32 objective evaluation\n\nx = (x16,x32,x64) # element of the tuple should refer to the same vector in different FP formats\nbound = ωf32*1.1 # error bound reachable with Float32 precision\nfx, ωfx, fid = objReachPrec(mpmodel,x,bound; π=1) # evaluate objective with increasing precision, starting with mpnlpmodel.FPFormat[π] = Float16, until evaluation error is lower than bound (satisfied with Float32)\n\n\n# gradient evaluation\ng16, ωg16 = graderrmp(mpmodel,x16) # Float16 gradient evaluation\ng32, ωg32 = graderrmp(mpmodel,x32) # Float32 gradient evaluation\ng64, ωg64 = graderrmp(mpmodel,x64) # Float32 gradient evaluation\n\nx = (x16,x32,x64) # element of the tuple should refer to the same vector in different FP formats\nbound = ωg64*1.1 # error bound reachable with Float64 precision\ngx, ωgx, gid = gradReachPrec(mpmodel,x,bound; π=1) # evaluate gradient with increasing precision, starting with mpnlpmodel.FPFormat[π] = Float16, until evaluation error is lower than bound (satisfied with Float64)","category":"page"},{"location":"FPMPNLPModel/#Relative-error","page":"FPMPNLPModel","title":"Relative error","text":"","category":"section"},{"location":"FPMPNLPModel/","page":"FPMPNLPModel","title":"FPMPNLPModel","text":"using MultiPrecisionR2\n\nFormats = [Float16,Float32,Float64] # FP formats\nf(x) = sum(x.^2) # objective function\ndim = 100 # problem dimentsion\nx0 = ones(100)\nω = Float64.([sqrt(eps(t)) for t in Formats]) # relative errors, have to be H format (Float64)\nmpmodel = FPMPNLPModel(f,x0,Formats; ωfRelErr = ω, ωgRelErr = ω) # create multi-precision model, will use relative error model base on ωfRelErr and ωgRelErr.\n\nx16 = ones(Float16,dim)\nx32 = Float32.(x16)\nx64 = x0\n\n# objective evaluation \nf16, ωf16 = objerrmp(mpmodel,x16) # Float16 objective evaluation\nf32, ωf32 = objerrmp(mpmodel,x32) # Float32 objective evaluation\nf64, ωf64 = objerrmp(mpmodel,x64) # Float64 objective evaluation\n\n# gradient evaluation\ng16, ωg16 = graderrmp(mpmodel,x16) # Float16 gradient evaluation\ng32, ωg32 = graderrmp(mpmodel,x32) # Float32 gradient evaluation\ng64, ωg64 = graderrmp(mpmodel,x64) # Float64 gradient evaluation","category":"page"},{"location":"#MultiPrecisionR2","page":"Home","title":"MultiPrecisionR2","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionR2.jl is a package that implements a multi-precision version of the Quadratic Regularization (R2) algorithm, a first-order algorithm, for solving non-convex, continuous, smooth optimization problems. The Floating Point (FP) format is adapted dynamically during algorithm execution to use low precision FP formats as much as possible while ensuring convergence and numerical stability.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package also implements multi-precision models FPMPNLPModel structure that derives from NLPModel implemented in NLPModels.jl. The interfaces for objective and gradient evaluations are extended to provide evaluation errors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionR2 can ensure numerical stability by using interval evaluations of the objective function and gradient. Interval evaluation relies on IntervalArithmetic.jl package to perform the interval evaluations.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"MultiPrecisionR2\")","category":"page"},{"location":"#Minimal-examples","page":"Home","title":"Minimal examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\n\nFP = [Float16,Float32] # define floating point formats used by the algorithm for objective and gradient evaluation\nf(x) = x[1]^2 + x[2]^2 # some objective function\nx0 = ones(Float32,2) # initial point\nmpmodel = FPMPNLPModel(f,x0,FP); # instanciate a Floating Point Multi Precision NLPModel (FPMPNLPModel)\nstat = MPR2(mpmodel) # run the algorithm","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing ADNLPModels\nusing OptimizationProblems\nusing OptimizationProblems.ADNLPProblems\n\nFP = [Float16,Float32] # define floating point formats used by the algorithm for objective and gradient evaluation\ns = :woods # select problem\nnlp = eval(s)(n=12,type = Val(FP[end]), backend = :generic)\nmpmodel = FPMPNLPModel(nlp,FP); # instanciate a Floating Point Multi Precision NLPModel (FPMPNLPModel)\nstat = MPR2(mpmodel) # run the algorithm","category":"page"},{"location":"#Warnings","page":"Home","title":"Warnings","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionR2 is designed to work with FP formats. Other format, such as fix point, might break the convergence property of the algorithm.\nInterval evaluation might fail with rounding mode other than :accurate for FP formats other than Float32 and Float64. When using interval evaluation, it is recommended to call ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using IntervalArithmetic\nsetrounding(Interval,:accurate)","category":"page"},{"location":"","page":"Home","title":"Home","text":"before instanciating a FPMPNLPModel.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If interval evaluation mode is used, interval evaluations of the objective and the gradient are automatically tested upon FPMPNLPModel instantiation.  An error is thrown if the evaluation fails. This might happen for several reasons related to IntervalArithmetic.jl package.","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers organization, so questions about any of our packages are welcome.","category":"page"},{"location":"tutorial_MPR2_advanced_use/#Disclaimer","page":"MPR2 Tutorial: Advanced Use ","title":"Disclaimer","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"The reader is encouraged to read the FPMPNLPModel and MPR2 Basic Use tutorial before this one.","category":"page"},{"location":"tutorial_MPR2_advanced_use/#Advanced-Use","page":"MPR2 Tutorial: Advanced Use ","title":"Advanced Use","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"MultiPrecisionR2.jl does more than implementing MPR2 algorithm as describes in the Basic Use tutorial. MPR2Precision.jl enables the user to define its own strategy to select evaluation precision levels and to handle evaluation errors. This is made possible by using callback functions when calling MultiPrecisionR2.solve!(). The default implementation of MultiPrecisionR2.solve!() relies on specific implementation of these callback functions, which are included in the package. The user is free to provide its own callback functions to change the behavior of the algorithm. ","category":"page"},{"location":"tutorial_MPR2_advanced_use/#Diving-into-MPR2-Implementation","page":"MPR2 Tutorial: Advanced Use ","title":"Diving into MPR2 Implementation","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"MPR2Precision.jl relies on callback functions that handle the objective and gradient evaluation. These callback functions are expected to compute values for the objective and the gradient and handle the evaluation precision. In the default implementation, the conditions on the error bound of objective and gradient evaluation are dealt with in these callback functions. That is why such convergence conditions (which user might choose not to implement) does not appear in the minimal implementation description of the code below.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Note that the function MPR2() use to run the algorithm in the Basic Use tutorial is just an interface to call the solver with convenience. The algorithm is actually implemented in solve!() which takes as arguments","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"m::FPMPNLPModel : the multi-precision model\nsolver::MPR2Solver: a structure that contains the algorithm variables (current solution, gradient, evaluation errors, ...)\nstats::GeneriExecutionStats: a structure containing algorithm status (nb. iteration, elapsed time, termination status,...), see SolverCore.jl package.","category":"page"},{"location":"tutorial_MPR2_advanced_use/#**Minimal-Implementation-Description**","page":"MPR2 Tutorial: Advanced Use ","title":"Minimal Implementation Description","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Here is a minimal description of the implementation of solve!() to understand when and why these functions are called. Please refer to the actual implementation for details. Note that the callback functions templates are not respected for the sake of explanation.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"function solve!(solver::MPR2solver{T},MPnlp::FPMPNLPModel{H},stats::GenericExecutionStats;kwargs...)\n  # ...\n  # some initialization (parameters, containers,...)\n  # ...\n  compute_f_at_x!() # initialize objective value at x0\n  # check for possible overflow \n  compute_g!() # initialize gradient value at x0\n  # check for possible overflow\n  # ...\n  # some more initialization (gradient norm tolerance,...)\n  # ...\n  while # main loop\n    # compute step as sk = -gk/σk\n    # check for step overflow\n    # compute candidate as c = x+s\n    # check for candidate overflow\n    # compute model decrease ΔT = -g^T s\n    # check for model decrease under/overflow\n    recompute_g!() # possibly recompute g(x) and ωg(x) if gradient error/mu indicator is too big\n    if grad_prec_fail # not enough precision with max precision\n      break\n    end\n    if grad_recomputed# gradient recomputed by recompute_g!() \n      # recompute step with new value of the gradient\n      # check overflow\n      # recompute candidate with new step\n      # check overflow\n      # recompute model decrease with new gradient and step\n    end\n    compute_f_at_x() # possibly recompute f(x) and ωf(x) if ωf(x) is too big\n    if f_x_prec_fail# not enough precision with max precision (ωf(x) too big)\n      break\n    end\n    compute_f_at_c!() # compute f(c) and ωf(c)\n    if f_c_prec_fail # not enough precision with max precision (ωf(c) too big)\n      break\n    end\n    # ...\n    # compute ρ = (f(x) - f(c))/ΔT\n    # update x and σ\n    # ... \n    if ρ ≥ η₁\n      compute_g!() # compute g(c) and  ωg(c)\n      # check overflow\n      # ...\n      # update values for next iteration\n      # ...\n      selectPic!()\n    end\n    # ...\n    # check for termination\n    # ...\n  end # while main loop\n  return status()\nend","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"The callback functions, and what they are supposed to do, are ","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"compute_f_at_x!(): Selects evaluation precision and computes objective function at the current point x_k such that possible conditions on error bounds are ensured. In the main loop, although the objective at x_k has already been computed at a previous iteration (f(c_j) = f(x_k) with j the last successful iteration) it might be necessary to recompute it to achieve smaller evaluation error (omega_f(x_k)) if necessary. This function is also called for initialization (before the main loop), where typically no bound on evaluation error is required.\ncompute_f_at_c!(): Selects evaluation precision and computes objective function at the candidate c_k such that possible conditions on error bounds are ensured.\ncompute_g!(): Selects evaluation precision and computes gradient at the candidate c_k. It is possible here to include condiditions on gradient bound error omega_g(x_k). In the default implementation, multiple sources of rounding error are taken into account in the mu indicator that requires to compute the step, candidate and model decrease (see Rounding Errors Handling of Basic Use tutorial). That is why in the default implementation no conditions on gradient error are required in this callback function, but are implemented in recompute_g!().\nrecompute_g!(): Selects evaluation precision and recomputes gradient at the current point x_k. This callback enables to recompute the gradient if necessary with a better precision if needed. In the default implementation, this callback implement the condition on the mu indicator (see Rouding Error Handling section of the Basic Use tutorial) and implements strategies to achieve sufficiently low value of mu to ensure convergence.\nselectPic!(): Select FP format for c_k+1, enables to lower the FP format used for evaluation at the next iteration. See Candidate Precision Selection section below for details.","category":"page"},{"location":"tutorial_MPR2_advanced_use/#**Callback-Functions:-Templates**","page":"MPR2 Tutorial: Advanced Use ","title":"Callback Functions: Templates","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"The callback functions for objective and gradient evaluation share the same template.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"success::Bool = compute_f_at_c!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\nsuccess::Bool = compute_f_at_x_default!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\nsuccess::Bool = compute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\nsuccess::Bool, recompute_g::Bool = recompute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Arguments:","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"m:FPMPNLPModel{H}: multi-precision model, needed to perform objective/gradient evaluation\nsolver::MPR2Solver : MPR2 solver structure, contains algorithm variables\nstats::GenericExecutionStats : Algorithm stats, can be used to access elapsed time, iteration, ...\ne::E: user defined structure to store additional information if needed","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Outputs","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"success::Bool: false if the evaluation failed, typical reason is that not enough evaluation precision could be reached. If success == false, stops the algorithm and set stats.status = :exception.\nrecompute_g::Bool: true if recompute_g!() has recomputed the gradient. In this case, the step, candidate and model decrease are recomputed (see Minimal Implementation Description)","category":"page"},{"location":"tutorial_MPR2_advanced_use/#**Callback-Functions:-Expected-Behaviors**","page":"MPR2 Tutorial: Advanced Use ","title":"Callback Functions: Expected Behaviors","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Modified Variables: The callback functions are expected to update all the variables that they modify. These variables are typically the solver::MPR2Solver fields, and possibly extra fields in user defined e structure. The callback functions compute_g!() and recompute_g!() are also expected to modify solver.g which stores the gradient.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Variable that should not be modified: The callback functions should not modify x, s, and c.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Below is a table that recaps what variables each callback can/should update. |Callback| Modified Variables| | ––– | ––––––––- | |compute_f_at_x!()| solver.f, solver.ωf, solver.π.πf |compute_f_at_c!()| solver.f⁺, solver.ωf⁺, solver.π.πf⁺ |compute_g!()| solver.g, solver..ωg, solver.π.πg |recompute_g()| solver.g, solver..ωg, solver.π.πg, solver.ΔT, solver.π.ΔT, solver.x_norm, solver.π.πnx, solver.s_norm, solver.π.πns, solver.ϕ, solver.ϕhat, solver.π.πc, solver.μ","category":"page"},{"location":"tutorial_MPR2_advanced_use/#**Multi-Precision-Evaluation-and-Vectors-Containers**","page":"MPR2 Tutorial: Advanced Use ","title":"Multi-Precision Evaluation and Vectors Containers","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"MPR2 performs objective and gradient evaluations and error bounds estimation with different FP formats. These evaluations are performed with FPMPNLPModels.objerrmp() and FPMPNLPModels.graderrmp!() functions (see FPMPNLPModels documentation).","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"x_k","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":", s_k, c_k, g_k are implemented as tuple of vectors of the FP formats used to perform evaluations (i.e. FPMPNLPModel.FPList), and are stored in the MPR2Solver structure given as argument of the callbacks. To evaluation the objective or the gradient in a given FP format, one has to call the appropriate function with the vector of the corresponding FP format, see example below","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Example 1: Simple Callback","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Here is a simple callback function for computing the objective function that does not take evaluation error into account.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"using MultiPrecisionR2\n\nfunction my_compute_f_at_c!(m::FPMPNLPModel, solver::MPR2Solver)\n  solver.f⁺ = obj(m, solver.c[solver.π.πc]) # FP format m.FPList[π.πc] will be used for obj evaluation, since obj() is called with the correspond FP format vector.\nend","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"The implementation of MPR2 automatically updates the containers for x, s, c, and g during execution, so that the user does not have to deal with that part. The function umpt!() (see documentation) update the containers.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Warning: MultiPrecisionR2.umpt!(x::Tuple, y::Vector{S}) updates only the vectors of x of FP format with precision greater or equal to the FP format of y. umpt! is implemented this to avoid rounding error due to casting into a lower precision format and overflow. This is closely related to the concept of \"forbidden evaluation\" detailed in the next section. If the precision solver.π.πx == 2, it means that the x[i]s vectors are up-to-date for i>=2. ","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Example: Container Update","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"using MultiPrecisionR2\n\nFP = [Float16,Float32,Float64]\nxini = ones(5)\nx = Tuple(fp.(xini) for fp in FP)\nxnew = Float32.(zeros(5))\numpt!(x,xnew)\nx # only x[2] and x[3] are updated","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Example: Lower Precision Casting Error","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"x64 = [70000.0,1.000000001,0.000000001,-230000.0] # Vector{Float64}\nx16 = Float16.(x64) # definitely not x64","category":"page"},{"location":"tutorial_MPR2_advanced_use/#**Forbidden-Evaluations**","page":"MPR2 Tutorial: Advanced Use ","title":"Forbidden Evaluations","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"A \"forbidden evaluation\" consists in evaluating the objective or the gradient with a FP format lower than the FP format of the point where it is evaluated. For example, if x is a Vector{Float64}, evaluating the objective with Float16 implicitly casts x into a Vector{Float16} and then evaluate the objective with this Float16 vector. The problem is that due to rounding error, the cast vector is different than the initial Float64 vector x. The objective is therefore not evaluated at x but at a different point. This causes numerical instability in MPR2 and must be avoided.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Example: Forbidden Evaluation","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"f(x) = 1/(x-10000) # objective\nx64 = 10001.0\nf(x64) # returns 1 as expected\nx16 = Float16(x64) # rounding error occurs: x16 != x64\nf(x16) # definitely not the expected value for f(x)","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"When implementing the callback functions, the user should make sure that the evaluation precision selected for objective or gradient evaluation (solver.π.πf, solver.π.πg) is always greater or equal than the precision of the point where the evaluation is performed (solver.π.πx or solver.π.πc).","category":"page"},{"location":"tutorial_MPR2_advanced_use/#**Step-and-Candidate-Computation-Precision**","page":"MPR2 Tutorial: Advanced Use ","title":"Step and Candidate Computation Precision","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"MPR2 implementation checks for possible under/overflow when computing the step solver.s and the candidate solver.c and increase FP format precision if necessary to avoid that. MPR2 implementation also updates solver.π.πs and solver.π.πc if necessary. These careful step and candidate computations are implemented in the MultiPrecisionR2.ComputeStep!() and MultiPrecisionR2.computeCandidate!() (see documentation). It is important to note that, even if the gradient solver.g has been computed with solver.π.πg precision, the precision solver.π.πs and solver.π.πc can be greater than solver.π.πg because overflow has occurred. As a consequence, the user should not take for granted than solver.π.πc == solver.π.πg when selecting FP format for objective/gradient evaluation at solver.c but must rely on the candidate precision solver.π.πc.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Example: Step Overflow","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"g(x) = 2*x # gradient\nx16 = Float16(1000)\nsigma = Float16(1/2^10) # regularization parameter\ng16 = g(x16)\ns = g16/Float16(sigma) # overflow\ns = Float32(g16)/Float32(sigma) # no overflow, s is a Float32, this is what computeStep!() does","category":"page"},{"location":"tutorial_MPR2_advanced_use/#**Candidate-Precision-Selection**","page":"MPR2 Tutorial: Advanced Use ","title":"Candidate Precision Selection","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"In light of the \"forbidden evaluation\" concept (see section Forbidden Evaluations), if no particular care is given, the objective/gradient evaluation precision can only increase from one iteration to another. To illustrate that consider that ","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"At iteration k: x_k is Float32, meaning that hatg(x_k) is Float32 (or higher precision FP format), but let's say Float32 here. By extension, s_k = -g_ksigma_k , and c_k = x_k+s_k are both Float32. It means that f(x_k) and f(x_k+s_k) are computed with Float32 or higher because of forbidden evaluations.\nAt iteration k+1:\nIf the iteration is successful: then x_k+1 = c_k is Float32, meaning again that hatg(x_k+1) is Float32 or higher precision format, and s_k+1 and c_k+1 are also Float32 or higher precision format. It further means that f(x_k+1) and f(c_k+1) can only be computed with Float32 or higher percision format.\nIf the iteration is unsuccessful: then x_k+1 = x_k is Float32 and we have the same than if the iteration is successful.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"To overcome this issue, and enable to decrease the FP formats used for objective/gradient evaluation, the user has the freedom to chose at iteration k the FP format of c_k+1. Indeed, there is no restriction on how the candidate is computed. Considering the above example, at iteration k+1, c_k+1 is Float32 but we can cast it (with rounding error) into a Float16, without breaking the convergence. Casting c_k+1 allows to compute f(c_k+1) with Float16, and if the iteration is successful, x_k+2 = c_k+1 is also a Float16, meaning that the gradient and objective can be computed with Float16 at x_k+2.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"The callback function selectPif!(), called at the end of the main loop (see section Minimal Implementation Description) update solver.π.πc so that MPnlp.FPList[π.πc] will be the FP format of c at the next iteration. The function ComputeCandidate!(), at the begining of the main loop, handles the casting of the candidate into MPnlp.FPList[π.πc].","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"The expected template for selectPif!() callback function is function selectPic!(π::MPR2Precisions). Only π.πc is expected to be modify.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Callback Modified Variables\nselectPic!() solver.π.πc","category":"page"},{"location":"tutorial_MPR2_advanced_use/#What-MultiPrecisionR2.solve!()-Handles","page":"MPR2 Tutorial: Advanced Use ","title":"What MultiPrecisionR2.solve!() Handles","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Runs the main loop until a stopping condition is reached (max iteration or nabla f(x) leq epsilon).\nUses the default callback functions for whichever has not been provided by the user.\nDeals with error due to norm computation to ensure nabla f(x) leq epsilon.\nDeals with high precision format computation to simulate \"exactly computed\" values (see Basic Use tutorial).\nUpdate the containers solver.x, solver.s, solver.c and solver.g (see Multi-Precision Evaluation and Vector Containers)\nMake sure no under/overflow occurs when computing solver.s and solver.c, update the precision solver.π.πs and solver.π.πc if necessary (see Step and Candidate Computation Precision)","category":"page"},{"location":"tutorial_MPR2_advanced_use/#What-MultiPrecisionR2.solve!()-Does-not-Handle","page":"MPR2 Tutorial: Advanced Use ","title":"What MultiPrecisionR2.solve!() Does not Handle","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"solve!() does not check that objective/gradient evaluation are preformed with a suitable FP format in the callback functions(see sections Multi-Precision Evaluation and Vector Containers and Forbidden Evaluation).\nsolve!() does not ensure convergence if the user uses its own callback functions.\nsolve!()does not update objective/gradient values and precision outside the callback functions. See Callback Functions: Expected Behavior for proper callback implementation.\nsolve!() does not handle overflow that might occur when evaluating the objective/gradient in the callback functions. It is up to the user to make sure avoid overflows (if possible). See Implementation Examples for dealing with overflow properly.","category":"page"},{"location":"tutorial_MPR2_advanced_use/#Callback-Functions-Cheat-Sheet","page":"MPR2 Tutorial: Advanced Use ","title":"Callback Functions Cheat Sheet","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Callback Description Outputs Expected Modified Variables\ncompute_f_at_x!() Select obj. FP format, compute f(x_k) and ωf(x_k) success::Bool : false if omega_f(x_k) is too big solver.f, solver.ωf, solver.π.πf\ncompute_f_at_c!() Select obj. FP format and compute f(c_k) and ωf(c_k) success::Bool : false if omega_f(c_k) is too big solver.f⁺, solver.ωf⁺, solver.π.πf⁺\ncompute_g!() Select grad FP format and compute g(c_k) and ωg(c_k) success::Bool : false if omega_g(c_k) is too big solver.g, solver.ωg, solver.π.πg\nrecompute_g() Select grad FP format and recompute g(x_k) and ωg(x_k) success::Bool : false if omega_g(c_k) is too big, grecompute::Bool: true if \\hat{g}(xk)$ was recomputed solver.g, solver.ωg, solver.π.πg, solver.ΔT, solver.π.ΔT, solver.x_norm, solver.π.πnx, solver.s_norm, solver.π.πns, solver.ϕ, solver.ϕhat, solver.π.πc, solver.μ\nselectPic!() Select candidate FP format for next iteration void solver.π.πc","category":"page"},{"location":"tutorial_MPR2_advanced_use/#Implementation-Examples","page":"MPR2 Tutorial: Advanced Use ","title":"Implementation Examples","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/#Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)","page":"MPR2 Tutorial: Advanced Use ","title":"Example 1: Precision Selection Strategy Based on Step Size (Error Free)","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"This example implements a precision selection strategy for the objective and gradient based on the norm of the step size, which does not take into account evaluation errors. The strategy is to choose the FP format for evaluation such that the norm of the step is greater than the square root of the unit roundoff.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"The callback functions must handle precision selection for evaluations and optionally error/warning messages if evaluation fails (typically overflow or lack of precision)","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"using MultiPrecisionR2\nusing LinearAlgebra\nusing SolverCore\n\nfunction my_compute_f_at_c!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\n  πmax = length(m.FPList) # get maximal allowed precision\n  eval_prec = findfirst(u -> sqrt(u) < solver.s_norm, m.UList) # select precision according to the criterion\n  if eval_prec === nothing # not enough precsion\n    @warn \" not enough precision for objective evaluation at c: ||s|| = $(solver.s_norm) < sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))\"\n    return false\n  end\n  solver.π.πf⁺ = max(eval_prec,solver.π.πc) # evaluation precision should be greater or equal to the FP format of the candidate (see forbidden evaluation)\n  solver.f⁺ = obj(m,solver.c[solver.π.πf⁺]) # eval objective only. solve!() made sure c[π.πf⁺] is up-to-date (see containers section)\n  while isinf(solver.f⁺) # check for overflow\n    solver.π.πf⁺ += 1\n    if solver.π.πf⁺ > πmax\n      @warn \" not enough precision for objective evaluation at c: overflow\"\n      return false # objective overflow with highest precision FP format: this is a fail\n    end\n    solver.f⁺ = obj(m,solver.c[solver.π.πf⁺])\n  end\n  return true\nend\n  \nfunction my_compute_f_at_x!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\n  πmax = length(m.FPList) # get maximal allowed precision\n  if solver.init # initial evaluation, step = 0, choose max precision\n    solver.π.πf = πmax\n  else # evaluation in main loop\n    eval_prec = findfirst(u -> sqrt(u) < solver.s_norm, m.UList) # select precision according to the criterion\n    if eval_prec === nothing # not enough precsion\n      @warn \" not enough precision for objective evaluation at x: ||s|| = $(solver.s_norm) < sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))\"\n      return false\n    end\n    solver.π.πf = max(eval_prec,solver.π.πx) # evaluation precision should be greater or equal to the FP format of the current solution (see forbidden evaluation)\n  end\n  solver.f = obj(m,solver.x[solver.π.πf]) # eval objective only. solve!() made sure x[π.πf] is up-to-date (see containers section)\n  while isinf(solver.f) # check for overflow\n    solver.π.πf += 1\n    if solver.π.πf > πmax\n      @warn \" not enough precision for objective evaluation at x: overflow\"\n      return false # objective overflow with highest precision FP format: this is a fail\n    end\n    solver.f = obj(m,solver.x[solver.π.πf])\n  end\n  return true\nend\n\nfunction my_compute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\n  πmax = length(m.FPList) # get maximal allowed precision\n  if solver.init # initial evaluation, step = 0, choose max precision\n    solver.π.πg = πmax\n  else # evaluation in main loop\n    eval_prec = findfirst(u -> sqrt(u) < solver.s_norm, m.UList) # select precision according to the criterion\n    if eval_prec === nothing # not enough precsion\n      @warn \" not enough precision for gradient evaluation at c: ||s|| = $(solver.s_norm) < sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))\"\n      return false\n    end\n    solver.π.πg = max(eval_prec,solver.π.πg) # evaluation precision should be greater or equal to the FP format of the candidate (see forbidden evaluation)\n  end\n  grad!(m,solver.c[solver.π.πg],solver.g[solver.π.πg]) # eval gradient only. solve!() made sure x[π.πg] is up-to-date (see containers section)\n  while findfirst(elem->isinf(elem),solver.g[solver.π.πg]) !== nothing # check for overflow, gradient vector version\n    solver.π.πg += 1\n    if solver.π.πg > πmax\n      @warn \" not enough precision for gradient evaluation at c: overflow\"\n      return false # objective overflow with highest precision FP format: this is a fail\n    end\n    grad!(m,solver.c[solver.π.πg])\n  end\n  return true\nend\n\nfunction my_recompute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\n  # simply update norm of the step, since recompute_g!() is called at the begining of the main loop after step computation\n  πmax = length(m.FPList) # get maximum precision index\n  solver.π.πns = solver.π.πs # select precision for step norm computation\n  s_norm = norm(solver.s[solver.π.πs])\n  while isinf(s_norm)|| s_norm ==0.0 # handle possible over/underflow\n    solver.π.πns = solver.π.πns+1 # increase precision to avoid over/underflow\n    if solver.π.πns > πmax\n      return false, false # overflow occurs with max precion: cannot compute s_norm with provided FP formats. Returns fail. Gradient has not been recomputed, return false.\n    end\n    s_norm = norm(solver.s[solver.π.πns]) # compute norm with higher precision step, solve!() made sure s[π.πns] is up-to-date\n  end\n  solver.s_norm = s_norm\n  return false, true # gradient not recomputed, successful evaluation\nend","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Let's try this implementation on a simple quadratic objective.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"FP = [Float16, Float32] # selected FP formats,\nf(x) = x[1]^2 + x[2]^2 # objective function\nomega = [0.0,0.0] # no evaluation error\nx = Float32.(1.5*ones(2)) # initial point\nMPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omega, ωgRelErr = omega); # indicates the use of relative error only to avoid interval evaluation, relative errors will not be computed with above callbacks\nstat = MPR2(MPmodel;\ncompute_f_at_x! = my_compute_f_at_x!,\ncompute_f_at_c! = my_compute_f_at_c!,\ncompute_g! = my_compute_g!,\nrecompute_g! = my_recompute_g!);\nstat  # first-order stationary point has been found","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Let's now try our implementation on the Rosenbrock function.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"FP = [Float16, Float32] # selected FP formats,\nf(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nomega = [0.0,0.0]\nx = Float32.(1.5*ones(2)) # initial point\nMPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omega, ωgRelErr = omega); # indicates the use of relative error only to avoid interval evaluation, relative errors will not be computed with above callbacks\nstat = MPR2(MPmodel;\ncompute_f_at_x! = my_compute_f_at_x!,\ncompute_f_at_c! = my_compute_f_at_c!,\ncompute_g! = my_compute_g!,\nrecompute_g! = my_recompute_g!); # throw lack of precision warning","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"The strategy implemented for precision selection does not allow to find a first-order critical point for the Rosenbrock function: the step becomes too small before MPR2 converges. Although this implementation is fast since it does not bother with evaluation errors, it is not very satisfactory since the Example in section \"Lack of Precision\" in the Basic tutorial shows that the default implementation is able to converge to a first-order critical point. This highlights that it is important to understand how rounding errors occur and affect the convergence of the algorithm (see MPR2 algorithm description in Basic Use tutorial) and that \"naive\" strategies like the one implemented above might not be satisfactory.","category":"page"},{"location":"tutorial_MPR2_advanced_use/#Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision","page":"MPR2 Tutorial: Advanced Use ","title":"Example 2: Switching to Gradient Descent When Lacking Objective Precision","text":"","category":"section"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"It might happen that solve!() stops early because the objective evaluation lacks precision. Consider for example that we use consider relative evaluation error for the objective. If MPR2 converges to a point where the objective is big, the error can be big too, and if the gradient is small the convergence condition omega f(x_k) leq eta_0 Delta T_k = hatg(x_k)^2sigma_k is likely to fail. In that case, the user might want to continue running the algorithm without caring about the objective, that is, as a simple gradient descent. solve!() implementation allows enough flexibility to do so. In the implementation below, the user defined structure e is used to indicate what \"mode\" the algorithm is running: default mode or gradient descent. The callbacks compute_f_at_x! sets st.f = Inf and compute_f_at_c! sets st.f⁺ = 0 if gradient descent mode is used. This ensures that rho_k = Inf geq eta_1 and the step is accepted in gradient descent mode. In the implementation below, compute_f_at_x! and compute_f_at_c! selects the precision such that omega f(x_k) leq eta_0 Delta T_k in default mode. We implement compute_g! to set σ so that ComputeStep!() will use the learning rate 1/σ. We use the default recompute_g callback.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"using MultiPrecisionR2\nusing LinearAlgebra\nusing SolverCore\n\nmutable struct my_struct\n  gdmode::Bool\n  learning_rate\nend\n\nfunction my_compute_f_at_c!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\n  if !e.gdmode # classic mode\n    ωfBound = solver.p.η₀*solver.ΔT\n    solver.π.πf⁺ = solver.π.πx # basic precision selection strategy\n    solver.f⁺, solver.ωf⁺, solver.π.πf⁺ = objReachPrec(m, solver.c, ωfBound, π = solver.π.πf⁺)\n    if isinf(solver.f⁺) # stop algo if objective overflow\n      @warn \"Objective evaluation overflow at x\"\n      return false\n    end\n    if solver.ωf⁺ > ωfBound # evaluation error too big\n      @warn \"Objective evaluation error at x too big to ensure convergence: switching to gradient descent\"\n      e.gdmode = true\n      solver.f⁺ = 0\n      return true\n    end\n  else # gradient descent mode\n    solver.f⁺ = 0.0\n  end\n  return true\nend\n\nfunction my_compute_f_at_x!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\n  πmax = length(m.EpsList)\n  if solver.init # initial evaluation before main loop\n    solver.f, solver.ωf, solver.π.πf = objReachPrec(m, solver.x, m.OFList[end], π = solver.π.πf)\n  else # evaluation in the main loop\n    if !e.gdmode\n      ωfBound = solver.p.η₀*solver.ΔT\n      if solver.ωf > ωfBound # need to reevaluate the objective at x\n        if solver.π.πf == πmax # already at highest precision \n          @warn \"Objective evaluation error at x too big to ensure convergence: switching to gradient descent\"\n          e.gdmode = true\n          solver.f = Inf\n          return true\n        end\n        solver.π.πf += 1 # increase evaluation precision of f at x\n        solver.f, solver.ωf, solver.π.πf = objReachPrec(m, solver.x, ωfBound, π = solver.π.πf)\n        if isinf(solver.f) # stop algo if objective overflow\n          @warn \"Objective evaluation overflow at x\"\n          return false\n        end\n        if solver.ωf > ωfBound # error evaluation too big with max precison\n          @warn \"Objective evaluation error at x too big to ensure convergence: switching to gradient descent\"\n          e.gdmode = true\n          solver.f = Inf\n          return true\n        end\n      end\n    else # gradient descent mode\n      solver.f = Inf\n    end\n  end\n  return true\nend\n\nfunction my_compute_g!(m::FPMPNLPModel, solver::MPR2Solver, stats::GenericExecutionStats, e)\n  solver.π.πg = solver.π.πc # default strategy, could be a callback\n  solver.ωg, solver.π.πg = gradReachPrec!(m, solver.c, solver.g, m.OFList[end], π = solver.π.πg)\n  if e.gdmode\n    solver.σ = 1/e.learning_rate\n  end\n  return true\nend","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"Let us first run MPR2() with the default implementation and relative evaluation error.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"FP = [Float16, Float32] # selected FP formats,\n#f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nf(x) = x[1]^2 + x[2]^2 +0.5\nomegaf = Float64.([0.01,0.005])\nomegag = Float64.([0.05,0.01])\nx = Float32.(1.5*ones(2)) # initial point\nMPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omegaf, ωgRelErr = omegag);\nstat = MPR2(MPmodel,verbose=1); # stops at iteration 3, throw lack of precision warning","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"We run MPR2() with the callback functions defined above and the default callbacks for compute_g!() and recompute_g!(). We use relative objective and gradient error.","category":"page"},{"location":"tutorial_MPR2_advanced_use/","page":"MPR2 Tutorial: Advanced Use ","title":"MPR2 Tutorial: Advanced Use ","text":"FP = [Float16, Float32] # selected FP formats,\n#f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nf(x) = x[1]^2 + x[2]^2 +0.5\nomegaf = Float64.([0.01,0.005])\nomegag = Float64.([0.05,0.01])\nx = ones(Float32,2) # initial point\nMPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omegaf, ωgRelErr = omegag);\ne = my_struct(false,1e-2)\nstat = MPR2(MPmodel;\ne = e,\ncompute_f_at_x! = my_compute_f_at_x!,\ncompute_f_at_c! = my_compute_f_at_c!,\ncompute_g! = my_compute_g!); # switch to gradient descent at iteration 3, converges to first order critical point","category":"page"}]
}
