var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [MultiPrecisionR2]","category":"page"},{"location":"reference/#MultiPrecisionR2.FPMPNLPModel","page":"Reference","title":"MultiPrecisionR2.FPMPNLPModel","text":"FPMPNLPModel(Model::AbstractNLPModel{D,S},FPList::Vector{K}; kwargs...) where {D,S,K<:DataType}\nFPMPNLPModel(f,x0, FPList::Vector{DataType})\n\nFloating-Point Multi-Precision Non Linear Model structure. This structure is intended to be used as MPR2Solver input.\n\nPrimairly stores NLPmodels instanciated with different Floating Point formats and provide errors on objective function and grandient evaluation (see objerrmp and graderrmp). The error models are :\n\nojective: |fl(f(x)) - f(x)| ≤ ωf\ngradient: |fl(∇f(x)) - ∇f(x)| ≤ ||fl(∇f(x))||₂ ωg.\n\nωf and ωg are needed for MPR2Solver. They are evaluated either using:\n\ninterval analysis (can be very slow)\nbased on relative error assumption (see ωfRelErr and ωgRelErr field description below)   \n\nFields\n\nModel::AbstractNLPModel : NLPModel\nFPList::Vector{DataType} : List of floating point formats\nEpsList::Vector{H} : List of machine epsilons of the floating point formats\nHPFormat::DataType : High precision floating point format, used for intermediate value computation in MPR2Solver. Is H parameter.\nγfunc : callback function for dot product rounding error parameter |γ|, |fl(x.y) - x.y| ≤ |x|.|y| γ. Expected signature is γfunc(n::Int,u::H) and output is H. Default callback γfunc(n::Int,u::H) = n*u is implemented upon instanciation. \nωfRelErr::Vector{H} : List of relative error factor for objective function evaluation for formats in FPList. Error model is |f(x)-fl(f(x))| ≤ ωfRelErr * |fl(f(x))| \nωgRelErr::Vector{H} : List of relative error factor for gradient evaluation for formats in FPList. Error model is |∇f(x)-fl(∇f(x))| ≤ ωgRelErr * ||fl(∇f(x))||₂ \nObjEvalMode::Int : Evalutation mode for objective and error. Set automatically upon instanciation. Possible values:\nINT_ERR : interval evaluation of objective (chosen as middle of the interval) and error\nREL_ERR : classical evaluation and use relative error model (with ωfRelErr value)\nGradEvalMode::Int : Evalutation mode for gradient and error. Set automatically upon instanciation. Possible values:\nINT_ERR : interval evaluation of gradient (chosen as middle of interval vector) and error\nREL_ERR : classical evaluation and use relative error model (with ωgRelErr value)\n\nConstructors:\n\nFPMPModel(Model::AbstractNLPModel, FPList::Vector{DataType}; nvar=100, kwargs...) : create a FPMPModel from Model with FPList precisions\nFPMPModel(s::symbol,FPList::Vector{DataType}; nvar=100, kwargs...) : create a FPMPModel from a symbol linked to an AbstractNLPModel.\nFPModels(f,x0::Vector,FPList::Vector{DataType}; nvar=100, kwargs...)\n\nKeyword arguments: \n\nnvar: dimension of the problem (if scalable)\nkwargs: \nHPFormat=Float64 : high precision format (must be at least as accurate as FPList[end])\nγfunc=nothing : use default if not provided (see Fields section above)\nωfRelErr=nothing : use interval evaluation if not provided\nωgRelErr=nothing : use interval evaluation if not provided\n\nChecks upon instanciation\n\nSome checks are performed upon instanciation. These checks include:\n\nLength consistency of vector fields:  FPList, EpsList\nHPFormat is at least as accurate as the highest precision floating point format in FPList`. Ideally HPFormat is more accurate to ensure the numerical stability of MPR2 algorithm.\nInterval evaluations: it might happen that interval evaluation of objective function and/or gradient is type-unstable or returns an error. The constructor returns an error in this case. This type of error is most likely due to IntervalArithmetic.jl.\nFPList is ordered by increasing floating point format accuracy\n\nThis checks can return @warn or error. \n\nExamples\n\nT = [Float16, Float32]\nf(x) = x[1]^2 + x[2]^2\nx = zeros(2)\nMPmodel = FPMPNLPModel(f,x0,T)\n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.MPCounters","page":"Reference","title":"MultiPrecisionR2.MPCounters","text":"MPCounters\n\nStruct for storing the number of function evaluations with each floating point format. The fields are the same as NLPModels.Counters, but contains a Dict{DataType,Int}.\n\n\n\nMPCounters(FPformats::Vector{DataType})\n\nCreates an empty MPCounters struct for types in the vector FPformats.\n\nusing MultiPrecisionR2.jl\nFPformats = [Float16, Float32]\ncntrs = MPCounters(FPformats)\n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.MPR2Params","page":"Reference","title":"MultiPrecisionR2.MPR2Params","text":"MPR2Params(LPFormat::DataType, HPFormat::DataType) MPR2 parameters.\n\nFields\n\nη₀::H : controls objective function error tolerance, convergence condition is ωf ≤ η₀ ΔT (see FPMPNLPModel for details on ωf)\nη₁::H : step successful if ρ ≥ η₁ (update incumbent)\nη₂::H : step very successful if ρ ≥ η₂ (decrease σ ⟹ increase step length)\nκₘ::H : tolerance on gradient evaluation error, μ ≤ κₘ (see computeMu) \nγ₁::L : σk+1 = σk * γ₁ if ρ ≥ η₂\nγ₂::L : σk+1 = σk * γ₂ if ρ < η₁\n\nParameters\n\nH must correspond to MPnlp.HPFormat with MPnlp given as input of MPR2\nL must correspond to MPnlp.FPList[1], i.e the lowest precision floating point format used by MPnlp given as input of MPR2\n\nConditions\n\nParameters must statisfy the following conditions:\n\n0 ≤ η₀ ≤ 1/2*η₁\n0 ≤ η₁ ≤ η₂ < 1\nη₀+κₘ/2 ≤0.5*(1-η₂)\nη₂<1 \n0<γ₁<1<γ₂\n\nInstiates default values:\n\nη₀::H = 0.01\nη₁::H = 0.02\nη₂::H = 0.95\nκₘ::H = 0.02 \nγ₁::L = 2^(-2)\nγ₂::L = 2\n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.MPR2Precisions","page":"Reference","title":"MultiPrecisionR2.MPR2Precisions","text":"function MPR2Precisions(π::Int)\n\nPrecision  of variables and precision evaluation of obj, grad, model reduction and norms. Precisions are represented by integers, and correspond to FP format of corresponding index in FPMPNLPModel.FPList. i.e., precision i correpsonds to FP format FPMPNLPModel.FPList[i] See FPMPNLPModel.\n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.MPR2Precisions-Tuple{Int64}","page":"Reference","title":"MultiPrecisionR2.MPR2Precisions","text":"function MPR2Precisions(π::Int)\n\nPrecision  of variables and precision evaluation of obj, grad, model reduction and norms. Precisions are represented by integers, and correspond to FP format of corresponding index in FPMPNLPModel.FPList. i.e., precision i correpsonds to FP format FPMPNLPModel.FPList[i] See FPMPNLPModel.\n\n\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.MPR2Solver","page":"Reference","title":"MultiPrecisionR2.MPR2Solver","text":"MPR2(MPnlp; kwargs...)\n\nAn implementation of the quadratic regularization algorithm with dynamic selection of floating point format for objective and gradient evaluation, robust against finite precision rounding errors.\n\nArguments\n\nMPnlp::FPMPNLPModel : Multi precision model, see FPMPNLPModel\n\nKeyword agruments:\n\nx₀::S = MPnlp.Model.meta.x0 : initial guess \npar::MPR2Params = MPR2Params(MPnlp.FPList[1],H) : MPR2 parameters, see MPR2Params for details\natol::H = H(sqrt(eps(T))) : absolute tolerance on first order criterion \nrtol::H = H(sqrt(eps(T))) : relative tolerance on first order criterion\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_iter::Int = 1000 : maximum number of iteration allowed\nσmin::T = sqrt(T(MPnlp.EpsList[end])) : minimal value for regularization parameter. Value must be representable in any of the floating point formats of MPnlp. \nverbose::Int=0 : display iteration information if > 0\ne::E : user defined structure, used as argument for compute_f_at_x!, compute_f_at_c! compute_g! and recompute_g! callback functions.\ncompute_f_at_x! : callback function to select precision and compute objective value and error bound at the current point. Allows to reevaluate the objective at x if more precision is needed.\ncompute_f_at_c! : callback function to select precision and compute objective value and error bound at candidate.\ncompute_g! : callback function to select precision and compute gradient value and error bound. Called at the end of main loop.\nrecompute_g! : callback function to select precision and recompute gradient value if more precision is needed. Called after step, candidate and model decrease computation in main loop.\nselectPic! : callback function to select FP format of c at the next iteration\n\nOutputs\n\nReturns a GenericExecutionStats, see SolverCore.jl GenericExecutionStats.status is set to :exception \n\nExample\n\nT = [Float16, Float32]\nf(x) = x[1]^2+x[2]^2\nx = ones(2)\nnlp_list = [ADNLPModel(f,t.(x)) for t in T ]\nMPnlp = FPMPNLPModel(nlp_list)\nmpr2s(MPnlp) \n\n\n\n\n\n","category":"type"},{"location":"reference/#MultiPrecisionR2.CheckMPR2ParamConditions-Union{Tuple{MPR2Params{H}}, Tuple{H}} where H","page":"Reference","title":"MultiPrecisionR2.CheckMPR2ParamConditions","text":"CheckMPR2ParamConditions(p::MPR2Params{H})\n\nCheck if the MPR2 parameters conditions are satified. See MPR2Params for parameter conditions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.GradIntervalEval_test-Tuple{NLPModels.AbstractNLPModel, AbstractArray}","page":"Reference","title":"MultiPrecisionR2.GradIntervalEval_test","text":"GradIntervalEval_test(nlp::AbstractNLPModel,FPList::AbstractArray)\n\nTest interval evaluation of gradient for all FP formats. Test fails and return an error if:\n\nInterval evaluation returns an error\nInterval evaluation is not type stable\n\nSee [FPMPNLPModel], [AbstractNLPModel]\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.MPR2-Tuple{FPMPNLPModel}","page":"Reference","title":"MultiPrecisionR2.MPR2","text":"MPR2(MPnlp; kwargs...)\n\nAn implementation of the quadratic regularization algorithm with dynamic selection of floating point format for objective and gradient evaluation, robust against finite precision rounding errors.\n\nArguments\n\nMPnlp::FPMPNLPModel : Multi precision model, see FPMPNLPModel\n\nKeyword agruments:\n\nx₀::S = MPnlp.Model.meta.x0 : initial guess \npar::MPR2Params = MPR2Params(MPnlp.FPList[1],H) : MPR2 parameters, see MPR2Params for details\natol::H = H(sqrt(eps(T))) : absolute tolerance on first order criterion \nrtol::H = H(sqrt(eps(T))) : relative tolerance on first order criterion\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_iter::Int = 1000 : maximum number of iteration allowed\nσmin::T = sqrt(T(MPnlp.EpsList[end])) : minimal value for regularization parameter. Value must be representable in any of the floating point formats of MPnlp. \nverbose::Int=0 : display iteration information if > 0\ne::E : user defined structure, used as argument for compute_f_at_x!, compute_f_at_c! compute_g! and recompute_g! callback functions.\ncompute_f_at_x! : callback function to select precision and compute objective value and error bound at the current point. Allows to reevaluate the objective at x if more precision is needed.\ncompute_f_at_c! : callback function to select precision and compute objective value and error bound at candidate.\ncompute_g! : callback function to select precision and compute gradient value and error bound. Called at the end of main loop.\nrecompute_g! : callback function to select precision and recompute gradient value if more precision is needed. Called after step, candidate and model decrease computation in main loop.\nselectPic! : callback function to select FP format of c at the next iteration\n\nOutputs\n\nReturns a GenericExecutionStats, see SolverCore.jl GenericExecutionStats.status is set to :exception \n\nExample\n\nT = [Float16, Float32]\nf(x) = x[1]^2+x[2]^2\nx = ones(2)\nnlp_list = [ADNLPModel(f,t.(x)) for t in T ]\nMPnlp = FPMPNLPModel(nlp_list)\nmpr2s(MPnlp) \n\n\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.ObjIntervalEval_test-Tuple{NLPModels.AbstractNLPModel, AbstractArray}","page":"Reference","title":"MultiPrecisionR2.ObjIntervalEval_test","text":"ObjIntervalEval_test(nlp::AbstractNLPModel,FPList::AbstractArray)\n\nTest interval evaluation of objective for all formats in FPList. Test fails and return an error if:\n\nInterval evaluation returns an error\nInterval evaluation is not type stable\n\nSee [FPMPNLPModel], [AbstractNLPModel]\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.check_overflow-Tuple{AbstractFloat}","page":"Reference","title":"MultiPrecisionR2.check_overflow","text":"check_overflow(f)\n\nf::AbstractFloat: Returns true if f is inf or nan, false otherwise.\nf::Interval : Returns true if diam(f) is inf or nan, false otherwise.\nf::AbstractVector{AbstractFloat} : Returns true if on element of f is inf or nan, false otherwise.\nf::AbstractVector{Interval} : Returns true if on element of diam(f) is inf, false otherwise.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.computeCandidate!-Union{Tuple{T}, Tuple{T, T, T, Vector{DataType}, MPR2Precisions}} where T<:Tuple","page":"Reference","title":"MultiPrecisionR2.computeCandidate!","text":"computeCandidate!(c::T, x::T, s::T, FP::Vector{DataType}, π::MPR2Precisions) where {T <: Tuple}\n\nCompute candidate with FP format avoiding underflow and overflow\n\nArguments\n\nx::Vector{T} : incumbent \ns::Vector{T} : step\nFP::Vector{Int} : Available floating point formats\nπ::MPR2Precisions\n\nModified arguments:\n\nc::Vector{T} : candidate\nπ::MPR2Precisions : π.πc updated\n\nOutputs\n\n::bool : false if over/underflow occur with highest precision FP format, true otherwise\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.computeModelDecrease!-Union{Tuple{T}, Tuple{H}, Tuple{T, T, MPR2Solver{T, H}, Vector{DataType}, MPR2Precisions}} where {H, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.computeModelDecrease!","text":"computeModelDecrease!(g::T,s::T,solver::MPR2Solver,FP::Vector{DataType},π::MPR2Precisions) where {T <: Tuple}\n\nCompute model decrease with FP format avoiding underflow and overflow\n\nArguments\n\ng::Vector{T} : gradient \ns::Vector{T} : step\nsolver::MPR2Solver: algo status\nFP::Vector{Int} : Available floating point formats\nπ::MPR2Precisions : hold the FP format indices\n\nModified Arguments\n\nsolver::MPR2Solver : solver.ΔT updated \nπ::MPR2Precisions : π.πΔ updated \n\nOutputs\n\n::bool : false if over/underflow occur with highest precision FP format, true otherwise\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.computeMu-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel, MPR2Solver{T, H}}} where {T, H}","page":"Reference","title":"MultiPrecisionR2.computeMu","text":"computeMu(m::FPMPNLPModel, solver::MPR2Solver{T,H}; π::MPR2Precisions = solver.π)\n\nCompute μ value for gradient error ωg, ratio ϕ = ||x||/||s|| and rounding error models\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.computeStep!-Union{Tuple{H}, Tuple{T}, Tuple{T, T, H, Vector{DataType}, MPR2Precisions}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.computeStep!","text":"computeStep!(s::T, g::T, σ::H, FP::Vector{DataType}, π::MPR2Precisions) where {T <: Tuple, H}\n\nCompute step with FP format avoiding underflow and overflow\n\nArguments\n\ns::Vector{T} : step \ng::Vector{T} : gradient \nπg::Int : g FP index\nσ::H : regularization parameter\nFP::Vector{Int} : Available floating point formats\n\nModified arguments :\n\ns::Vector : step vector container\nπ::MPR2Precisions : hold the FP format indices\n\nOutput\n\n::bool : false if over/underflow occur, true otherwise\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.compute_f_at_c_default!-Union{Tuple{T}, Tuple{E}, Tuple{H}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, SolverCore.GenericExecutionStats, E}} where {H, E, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.compute_f_at_c_default!","text":"compute_f_at_c_default!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, stats::GenericExecutionStats, e::E)\n\nCompute objective function at the candidate. Updates related fields of solver.\n\nOutputs:\n\n*bool : returns false if couldn't reach sufficiently small evaluation error or overflow occured. \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.compute_f_at_x_default!-Union{Tuple{T}, Tuple{E}, Tuple{H}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, SolverCore.GenericExecutionStats, E}} where {H, E, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.compute_f_at_x_default!","text":"compute_f_at_x_default!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, stats::GenericExecutionStats, e::E)\n\nCompute objective function at the current incumbent. Updates related fields of solver.\n\nOutputs:\n\n*bool : returns false if couldn't reach sufficiently small evaluation error or overflow occured. \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.compute_g_default!-Union{Tuple{T}, Tuple{E}, Tuple{H}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, SolverCore.GenericExecutionStats, E}} where {H, E, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.compute_g_default!","text":"compute_g_default!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, stats::GenericExecutionStats, e::E)\n\nCompute gradient at x if solver.init == true (first gradient eval outside of main loop), at c otherwise.\n\nOutputs:\n\n*bool : always returns true (comply with callback template)\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.gradReachPrec!-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel{H}, T, T, H}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.gradReachPrec!","text":"gradReachPrec!(m::FPMPNLPModel{H}, x::T, g::T, err_bound::H; π::Int = 1) where {T <: Tuple, H}\n\nEvaluates gradient and increase model precision to reach necessary error bound or to avoid overflow.\n\nInputs\n\nπ: Initial ''gess'' for precision level that can provide evaluation error lower than err_bound, uses 1 by default (lowest precision)\n\nOutputs\n\nωg: objective evaluation error\nid: precision level used for evaluation\n\nThere is no guarantee that ωg ≤ err_bound. This case happens if the highest precision FP format is not accurate enough.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.gradReachPrec-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel{H}, T, H}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.gradReachPrec","text":"gradReachPrec!(m::FPMPNLPModel{H}, x::T, g::T, err_bound::H; π::Int = 1) where {T <: Tuple, H}\n\nEvaluates gradient and increase model precision to reach necessary error bound or to avoid overflow.\n\nInputs\n\nπ: Initial ''gess'' for precision level that can provide evaluation error lower than err_bound, uses 1 by default (lowest precision)\n\nOutputs\n\nωg: objective evaluation error\nid: precision level used for evaluation\n\nThere is no guarantee that ωg ≤ err_bound. This case happens if the highest precision FP format is not accurate enough.\n\n\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.graderrmp!-Union{Tuple{V}, Tuple{S}, Tuple{H}, Tuple{FPMPNLPModel{H}, V, V}} where {H, S, V<:AbstractVector{S}}","page":"Reference","title":"MultiPrecisionR2.graderrmp!","text":"graderrmp!(m::FPMPNLPModel{H}, x::V, g::V) where {H, S, V<:AbstractVector{S}}\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V, ::Val{INT_ERR}) where {H, S, V<:AbstractVector{S}}\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V, ::Val{REL_ERR}) where {H, S, V<:AbstractVector{S}}\n\nEvaluates the gradient g and the relative evaluation error ωg. The two functions with the extra argument ::Val{INTERR} and ::Val{RELERR} handles the interval and \"classic\" evaluation of the objective and the error, respectively. Inputs: x::Vector{S} with S in m.FPList Outputs: g::Vector{S}, ωg <: AbstractFloat satisfying: ||∇f(x) - fl(∇f(x))||₂ ≤ ωg||g||₂ with fl() the floating point evaluation. Note: ωg FP format may be different than S Overflow cases:\n\nInterval evaluation: if at least one element of g has infinite diameter, returns [0]ⁿ, Inf\nClassical evaluation: if one element of g overflow, returns g, Inf \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.graderrmp-Union{Tuple{V}, Tuple{S}, Tuple{FPMPNLPModel, V}} where {S, V<:AbstractVector{S}}","page":"Reference","title":"MultiPrecisionR2.graderrmp","text":"graderrmp!(m::FPMPNLPModel{H}, x::V, g::V) where {H, S, V<:AbstractVector{S}}\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V, ::Val{INT_ERR}) where {H, S, V<:AbstractVector{S}}\ngraderrmp!(m::FPMPNLPModel{H}, x::V, g::V, ::Val{REL_ERR}) where {H, S, V<:AbstractVector{S}}\n\nEvaluates the gradient g and the relative evaluation error ωg. The two functions with the extra argument ::Val{INTERR} and ::Val{RELERR} handles the interval and \"classic\" evaluation of the objective and the error, respectively. Inputs: x::Vector{S} with S in m.FPList Outputs: g::Vector{S}, ωg <: AbstractFloat satisfying: ||∇f(x) - fl(∇f(x))||₂ ≤ ωg||g||₂ with fl() the floating point evaluation. Note: ωg FP format may be different than S Overflow cases:\n\nInterval evaluation: if at least one element of g has infinite diameter, returns [0]ⁿ, Inf\nClassical evaluation: if one element of g overflow, returns g, Inf \n\n\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.increment!-Tuple{AbstractMPNLPModel, Symbol, DataType}","page":"Reference","title":"MultiPrecisionR2.increment!","text":"increment!(nlp, s)\n\nIncrement counter s of problem nlp.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.objReachPrec-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel{H}, T, H}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.objReachPrec","text":"objReachPrec(m::FPMPNLPModel{H}, x::T, err_bound::H; π::Int = 1) where {T <: Tuple, H}\n\nEvaluates objective and increase model precision to reach necessary error bound or to avoid overflow.\n\nInputs\n\nπ: Initial ''guess'' precision level that can provide evaluation error lower than err_bound, use 1 by default (lowest precision)\n\nOutputs\n\nf: objective value at x\nωf: objective evaluation error\nid: precision level used for evaluation\n\nThere is no guarantee that ωf ≤ err_bound, happens if highest precision FP format is not accurate enough.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.objerrmp-Union{Tuple{S}, Tuple{FPMPNLPModel, AbstractVector{S}}} where S","page":"Reference","title":"MultiPrecisionR2.objerrmp","text":"objerrmp(m::FPMPNLPModel, x::AbstractVector{T})\nobjerrmp(m::FPMPNLPModel, x::AbstractVector{S}, ::Val{INT_ERR})\nobjerrmp(m::FPMPNLPModel, x::AbstractVector{S}, ::Val{REL_ERR})\n\nEvaluates the objective and the evaluation error. The two functions with the extra argument ::Val{INTERR} and ::Val{RELERR} handles the interval and \"classic\" evaluation of the objective and the error, respectively. Inputs: x::Vector{S}, can be either a vector of AbstractFloat or a vector of Intervals. Outputs: fl(f(x)), ωf <: AbstractFloat, where |f(x)-fl(f(x))| ≤ ωf with fl() the floating point evaluation. Overflow cases:\n\nInterval evaluation: overflow occurs if the diameter of the interval enclosing f(x) is Inf. Returns 0, Inf\nClassical evaluation:\nIf obj(x) = Inf: returns: Inf, Inf\nIf obj(x) != Inf and ωf = Inf, returns: obj(x), Inf  \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.recomputeMu!-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, MPR2Precisions}} where {T<:Tuple, H}","page":"Reference","title":"MultiPrecisionR2.recomputeMu!","text":"recomputeMu!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, πr::MPR2Precisions) where {T <: Tuple, H}\n\nRecompute mu based on new precision levels.  Performs only necessary operations to recompute mu.  Possible operations are:\n\nrecompute candidate with higher prec FP format to decrease u\nrecompute ϕhat and ϕ with higher FP format for norm computation of x and s\nrecompute step with greater precision: decrease μ denominator\nrecompute gradient with higher precision to decrease ωg\nrecompute model reduction with higher precision to decrease αfunc(n,U[π.πΔ])\n\nDoes not make the over/underflow check as in main loop, since it is a repetition of the main loop with higher precisions and these issue shouldn't occur\n\nOutputs:\n\ng_recompute::Bool : true if gradient has been modified, false otherwise\n\nSee recomputeMuPrecSelection!\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.recomputeMuPrecSelection!-Tuple{MPR2Precisions, MPR2Precisions, Any}","page":"Reference","title":"MultiPrecisionR2.recomputeMuPrecSelection!","text":"recomputeMuPrecSelection!(π::MPR2Precisions, πr::MPR2Precisions, πmax)\n\nDefault strategy to select new precisions to recompute μ in the case where μ > κₘ. Return false if no precision can be increased.\n\nModified arguments:\n\nπr: contains new precision that will be used to recompute mu, see recomputeMu!\n\nOuptputs\n\nmax_prec::bool : return true if maximum precision levels have been reached\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.recompute_g_default!-Union{Tuple{T}, Tuple{E}, Tuple{H}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, SolverCore.GenericExecutionStats, E}} where {H, E, T<:Tuple}","page":"Reference","title":"MultiPrecisionR2.recompute_g_default!","text":"recompute_g_default!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, stats::GenericExecutionStats, e::E)\n\nIncrease operation precision levels until sufficiently small μ indicator is achieved. See also computeMu, recomputeMuPrecSelection!, recomputeMu!\n\nOutputs:\n\n*bool : returns false if couldn't reach sufficiently small evaluation error or overflow occured. \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.reset!-Tuple{AbstractMPNLPModel}","page":"Reference","title":"MultiPrecisionR2.reset!","text":"reset!(mpnlp::AbstractMPNLPModel)\n\nReset evaluation count and model data (if appropriate) in mpnlp.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.reset!-Tuple{MPCounters}","page":"Reference","title":"MultiPrecisionR2.reset!","text":"reset!(counters::MPCounters)\n\nReset evaluation counters\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.selectPic_default!-Tuple{MPR2Solver}","page":"Reference","title":"MultiPrecisionR2.selectPic_default!","text":"selectPic_default!(π::MPR2Precisions)\n\nDefault strategy for selecting FP format of candidate for the next evaluation. Updates solver.π.πf⁺.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.selectPif!-Union{Tuple{H}, Tuple{T}, Tuple{FPMPNLPModel, MPR2Solver{T, H}, H}} where {T, H}","page":"Reference","title":"MultiPrecisionR2.selectPif!","text":"selectPif!(m::FPMPNLPModel, solver::MPR2Solver{T,H}, ωfBound::H)\n\nSelect a precision for objective evaluation for candidate based on predicted evaluation error. Evaluation is predicted as:\n\nRelative error:\nPredicted value of objective at c: f(c) ≈ f(x) - ΔTk\nRelative error model: ωf(c) = |f(c)| * RelErr\nInterval error:\nPredicted value of objective at c: f(c) ≈ f(x) - ΔTk\nInterval evaluation error is proportional to f(x)\nInterval evaluation error depends linearly with unit-roundoff \nOther: Lowest precision that does not cast candidate in a lower prec FP format and f(c) predicted does not overflow\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.sum_counters-Tuple{AbstractMPNLPModel}","page":"Reference","title":"MultiPrecisionR2.sum_counters","text":"sum_counters(mpnlp)\n\nSum all counters of problem nlp except cons, jac, jprod and jtprod.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.sum_counters-Tuple{MPCounters}","page":"Reference","title":"MultiPrecisionR2.sum_counters","text":"sum_counters(c::MPCounters)\n\nSum all counters of counters except cons, jac, jprod and jtprod.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.umpt!-Union{Tuple{S}, Tuple{Tuple, Vector{S}}} where S","page":"Reference","title":"MultiPrecisionR2.umpt!","text":"umpt!(x::Tuple, y::Vector{S})\n\nUpdate multi precision containers.  Update is occuring only if precision input vector y is lower or equal to the one of the element of the container (vector) to avoid rounding error due to conversion. \n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.update_struct!-Tuple{MPR2Precisions, MPR2Precisions}","page":"Reference","title":"MultiPrecisionR2.update_struct!","text":"update_struct!(str,other_str)\n\nUpdate the fields of str with the fields of other_str.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.γfunc_test_error_bound-Tuple{Int64, AbstractFloat, Any}","page":"Reference","title":"MultiPrecisionR2.γfunc_test_error_bound","text":"γfunc_test_error_bound(n::Int,eps::AbstractFloat,γfunc)\n\nTests if γfunc callback provides strictly less than 100% error for dot product error of vector of size the dimension of the problem and the lowest machine epsilon.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MultiPrecisionR2.γfunc_test_template-Tuple{Any}","page":"Reference","title":"MultiPrecisionR2.γfunc_test_template","text":"γfunc_test_template(γfunc)\n\nTests if γfunc callback function is properly implemented. Expected template: γfunc(n::Int,u::Float) -> Float\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_cons-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_cons","text":"neval_cons(nlp)\nneval_cons(nlp,T)\n\nGet the total number (all FP formats) of cons evaluations. If extra argument T is provided, returns  turn number of cons evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_cons_lin-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_cons_lin","text":"neval_cons_lin(nlp)\nneval_cons_lin(nlp,T)\n\nGet the total number (all FP formats) of cons evaluations. If extra argument T is provided, returns  turn number of cons evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_cons_nln-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_cons_nln","text":"neval_cons_nln(nlp)\nneval_cons_nln(nlp,T)\n\nGet the total number (all FP formats) of cons evaluations. If extra argument T is provided, returns  turn number of cons evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_grad-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_grad","text":"neval_grad(nlp)\nneval_grad(nlp,T)\n\nGet the total number (all FP formats) of grad evaluations. If extra argument T is provided, returns  turn number of grad evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_hess-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_hess","text":"neval_hess(nlp)\nneval_hess(nlp,T)\n\nGet the total number (all FP formats) of hess evaluations. If extra argument T is provided, returns  turn number of hess evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_hprod-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_hprod","text":"neval_hprod(nlp)\nneval_hprod(nlp,T)\n\nGet the total number (all FP formats) of hprod evaluations. If extra argument T is provided, returns  turn number of hprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jac-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jac","text":"neval_jac(nlp)\nneval_jac(nlp,T)\n\nGet the total number (all FP formats) of jac evaluations. If extra argument T is provided, returns  turn number of jac evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jac_lin-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jac_lin","text":"neval_jac_lin(nlp)\nneval_jac_lin(nlp,T)\n\nGet the total number (all FP formats) of jac evaluations. If extra argument T is provided, returns  turn number of jac evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jac_nln-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jac_nln","text":"neval_jac_nln(nlp)\nneval_jac_nln(nlp,T)\n\nGet the total number (all FP formats) of jac evaluations. If extra argument T is provided, returns  turn number of jac evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jcon-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jcon","text":"neval_jcon(nlp)\nneval_jcon(nlp,T)\n\nGet the total number (all FP formats) of jcon evaluations. If extra argument T is provided, returns  turn number of jcon evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jgrad-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jgrad","text":"neval_jgrad(nlp)\nneval_jgrad(nlp,T)\n\nGet the total number (all FP formats) of jgrad evaluations. If extra argument T is provided, returns  turn number of jgrad evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jhess-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jhess","text":"neval_jhess(nlp)\nneval_jhess(nlp,T)\n\nGet the total number (all FP formats) of jhess evaluations. If extra argument T is provided, returns  turn number of jhess evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jhprod-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jhprod","text":"neval_jhprod(nlp)\nneval_jhprod(nlp,T)\n\nGet the total number (all FP formats) of jhprod evaluations. If extra argument T is provided, returns  turn number of jhprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jprod-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jprod","text":"neval_jprod(nlp)\nneval_jprod(nlp,T)\n\nGet the total number (all FP formats) of jprod evaluations. If extra argument T is provided, returns  turn number of jprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jprod_lin-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jprod_lin","text":"neval_jprod_lin(nlp)\nneval_jprod_lin(nlp,T)\n\nGet the total number (all FP formats) of jprod evaluations. If extra argument T is provided, returns  turn number of jprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jprod_nln-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jprod_nln","text":"neval_jprod_nln(nlp)\nneval_jprod_nln(nlp,T)\n\nGet the total number (all FP formats) of jprod evaluations. If extra argument T is provided, returns  turn number of jprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jtprod-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jtprod","text":"neval_jtprod(nlp)\nneval_jtprod(nlp,T)\n\nGet the total number (all FP formats) of jtprod evaluations. If extra argument T is provided, returns  turn number of jtprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jtprod_lin-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jtprod_lin","text":"neval_jtprod_lin(nlp)\nneval_jtprod_lin(nlp,T)\n\nGet the total number (all FP formats) of jtprod evaluations. If extra argument T is provided, returns  turn number of jtprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_jtprod_nln-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_jtprod_nln","text":"neval_jtprod_nln(nlp)\nneval_jtprod_nln(nlp,T)\n\nGet the total number (all FP formats) of jtprod evaluations. If extra argument T is provided, returns  turn number of jtprod evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.neval_obj-Tuple{AbstractMPNLPModel}","page":"Reference","title":"NLPModels.neval_obj","text":"neval_obj(nlp)\nneval_obj(nlp,T)\n\nGet the total number (all FP formats) of obj evaluations. If extra argument T is provided, returns  turn number of obj evaluations for the given FP format T.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MultiPrecisionR2","category":"page"},{"location":"#MultiPrecisionR2","page":"Home","title":"MultiPrecisionR2","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionR2.jl implements MPR2, a multi precision extension of the Quadratic Regularization (R2) algorithm, for solving continuous unconstrained non-convex problems with several Floating Point (FP) formats, for example Float16, Float32 and Float64. MPR2 aims for evaluating the objective and the gradient with the lowest precision FP format to save computational time and energy while controlling computation error to maintain convergence.","category":"page"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionR2.jl implements MPR2 in such a way that it allows the user some flexibility. By default, rounding errors and precision selection for the objective function and the gradient evaluation are handled such that the convergence is guaranteed (easy use, see Basic Use chapter). The user can choose to implement its own strategies for precision selection via callback functions (see Advanced Use chapter), but in that case it is up to the user to ensure the convergence of the algorithm. ","category":"page"},{"location":"#Table-of-Contents","page":"Home","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Quick Start\nMotivation\nMPR2 Algorithm: General Description  \nNotations  \nMPR2 Algorithm Broad Description  \nRounding Error Handling  \nConditions on Parameters  \nBasic Use\nFPMPNLPModels: Creating a Multi-Precision Model\nEvaluation Mode\nHigh Precision Format\nGradient and Dot Product Error: Gamma Callback Function\nMPR2Solver\nGamma Function\nHigh Precsion Format: MPR2 Solver\nLack of Precision\nAdvanced Use\nDiving into MPR2 Implementation\nMinimal Implementation Description\nCallback Functions: Templates\nCallback Functions: Expected Behaviors\nMulti-Precision Evaluation and Vectors Containers\nForbidden Evaluations\nStep and Candidate Computation Precision\nCandidate Precision Selection\nWhat MPR2Solver Handles\nWhat MPR2Solver Does not Handle\nImplementation Examples","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing ADNLPModels\nusing IntervalArithmetic\n\nsetrounding(Interval,:accurate)\nFP = [Float16,Float32] # define floating point formats used by the algorithm for objective and gradient evaluation\nf(x) = x[1]^2 + x[2]^2 # some objective function\nx0 = ones(Float32,2) # initial point\nmpmodel = FPMPNLPModel(f,x0,FP); # instanciate a Floating Point Multi Precision NLPModel (FPMPNLPModel)\nstat = MPR2(mpmodel) # run the algorithm","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing ADNLPModels\nusing IntervalArithmetic\nusing OptimizationProblems\nusing OptimizationProblems.ADNLPProblems\n\nsetrounding(Interval,:accurate)\nFP = [Float16,Float32] # define floating point formats used by the algorithm for objective and gradient evaluation\ns = :woods # select problem\nnlp = eval(s)(n=12,type = Val(FP[end]), gradient_backend = ADNLPModels.GenericForwardDiffADGradient)\nmpmodel = FPMPNLPModel(nlp,FP); # instanciate a Floating Point Multi Precision NLPModel (FPMPNLPModel)\nstat = MPR2(mpmodel) # run the algorithm","category":"page"},{"location":"","page":"Home","title":"Home","text":"Warnings","category":"page"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionR2.jl works only with Floating Point formats (BFloat16,Float16,Float32,Float64,Float128)\nUnfortunately, other modes than :accurate for interval evaluation are not guaranteed to work with FP formats different from Float32 and Float64. \nIf interval evaluation mode is used, interval evaluation for the objective and the gradient is automatically tested upon FPMPNLPModel instanciation.  An error is thrown if the evaluation fails. This might happen for several reasons related to IntervalArithmetic.jl package.","category":"page"},{"location":"#Motivation","page":"Home","title":"Motivation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here is the comparison between R2 algorithm from JSOSolvers.jl run with Float64 and MPR2 from MultiPrecisionR2.jl run with Float16, Float32 and Float64 over a set of unconstrained problems from OptimizationProblems.jl. For a fair comparison, we set MPR2 evaluation errors for objective function and gradient to zero with FLoat64. The time and energy savings offer by MPR2 compared to R2 is estimated with the rules of thumb:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Number of bits divided by two -> time computation divided by two and energy consumption divided by four.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing NLPModels\nusing ADNLPModels\nusing OptimizationProblems\nusing OptimizationProblems.ADNLPProblems\nusing JSOSolvers\n\nFP = [Float16,Float32,Float64] # MPR2 Floating Point formats\nomega = Float64.([sqrt(eps(Float16)),sqrt(eps(Float32)),0]) # MPR2 relative errors, computations assumed exact with Float64\nr2_obj_eval = [0]\nr2_grad_eval = [0]\nmpr2_obj_eval = zeros(Float64,length(FP))\nmpr2_grad_eval = zeros(Float64,length(FP))\nnvar = 100 #problem dimension (if scalable)\nmax_iter = 1000\n\nmeta = OptimizationProblems.meta\nnames_pb_vars = meta[(meta.has_bounds .== false) .& (meta.ncon .== 0), [:nvar, :name]] #select unconstrained problems\nfor pb in eachrow(names_pb_vars)\n  nlp = eval(Meta.parse(\"ADNLPProblems.$(pb[:name])(n=$nvar,type=Val(Float64),gradient_backend = ADNLPModels.GenericForwardDiffADGradient)\"))\n  mpmodel = FPMPNLPModel(nlp,FP,ωfRelErr=omega,ωgRelErr=omega);\n  statr2 = R2(nlp,max_eval=max_iter)\n  r2_obj_eval .+= nlp.counters.neval_obj\n  r2_grad_eval .+= nlp.counters.neval_grad\n  statmpr2 = MPR2(mpmodel,max_iter = max_iter)\n  mpr2_obj_eval .+= [haskey(mpmodel.counters.neval_obj,fp) ? mpmodel.counters.neval_obj[fp] : 0 for fp in FP]\n  mpr2_grad_eval .+= [haskey(mpmodel.counters.neval_grad,fp) ? mpmodel.counters.neval_grad[fp] : 0 for fp in FP]\nend\nmpr2_obj_time = sum(mpr2_obj_eval.*[1/4,1/2,1])\nobj_time_save = mpr2_obj_time/r2_obj_eval[1]\nmpr2_obj_energy = sum(mpr2_obj_eval.*[1/16,1/4,1])\nobj_energy_save = mpr2_obj_energy/r2_obj_eval[1]\nmpr2_grad_time = sum(mpr2_grad_eval.*[1/4,1/2,1])\ngrad_time_save = mpr2_grad_time/r2_grad_eval[1]\nmpr2_grad_energy = sum(mpr2_grad_eval.*[1/16,1/4,1])\ngrad_energy_save = mpr2_grad_energy/r2_grad_eval[1]\nprintln(\"Possible time saving for objective evaluation with MPR2: $(round((1-obj_time_save)*100,digits=1)) %\")\nprintln(\"Possible time saving for gradient evaluation with MPR2: $(round((1-grad_time_save)*100,digits=1)) %\")\nprintln(\"Possible energy saving for objective evaluation with MPR2: $(round((1-obj_energy_save)*100,digits=1)) %\")\nprintln(\"Possible energy saving for gradient evaluation with MPR2: $(round((1-grad_energy_save)*100,digits=1)) %\")","category":"page"},{"location":"#MPR2-Algorithm:-General-Description","page":"Home","title":"MPR2 Algorithm: General Description","text":"","category":"section"},{"location":"#**Notations**","page":"Home","title":"Notations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"fl\n: finite-precision computation\nu\n: unit round-off for a given FP format\ndelta\n: rounding error induced by one FP operation (+-*), for example fl(x+y) = (x+y)(1+delta). Bounded as deltaleq u.\npi\n: FP format index, also called precision\nf x rightarrow f(x)\n: objective function\nhatf xpi_f rightarrow fl(f(xpi_f))\n: finite precision counterpart of f with FP format corresponding to index pi_f  \nnabla f x rightarrow nabla f(x)\n: gradient of f\nhatg x pi_g rightarrow fl(nabla f(x)pi_g)\n: finite precision counterpart of nabla f  with FP format corresponding to index pi_g\nomega_f(x)\n: bound on finite precision evaluation error of f,  hatf(xpi_f) -f(x)  leq omega_f(x)\nomega_g(x)\n: bound on finite precision evaluation error of nabla f,  hatg(xpi_g) -nabla f(x)  leq omega_g(x)hatg(xpi_g)","category":"page"},{"location":"#**MPR2-Algorithm-Broad-Description**-(differs-from-package-implementation)","page":"Home","title":"MPR2 Algorithm Broad Description (differs from package implementation)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MPR2 is described by the following algorithm. Note that the actual implementation in the package differs slightly. For an overview of the actual implementation, see section Diving Into MPR2 Implementation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the algorithm, compute means compute with finite-precision machine computation, and define means \"compute exactly\", i.e with infinite precision. Defining a value is therefore not possible to perform on a (finite-precision) machine. This point is discussed later. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Inputs: ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Initial values: x_0, sigma_0\nTunable Parameters: 0  eta_0  eta_1  eta_2  1, 0  gamma_1  1  gamma_2, kappa_m\nGradient tolerance: epsilon\nList of FP formats (e.g [Float16, Float32, Float64])","category":"page"},{"location":"","page":"Home","title":"Home","text":"Outputs","category":"page"},{"location":"","page":"Home","title":"Home","text":"x_k\nsuch that nabla f(x_k) leq epsilon nabla f(x_0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Initialization","category":"page"},{"location":"","page":"Home","title":"Home","text":"Compute f_0 = hatf(x_0pi_f)\nCompute g_0 = hatg(x_0pi_g)","category":"page"},{"location":"","page":"Home","title":"Home","text":"While g_k  dfracepsilon1+omega_g(x_k) do","category":"page"},{"location":"","page":"Home","title":"Home","text":"Compute s_k = -g_ksigma_k\nCompute c_k = x_k + s_k\nCompute Delta T_k = g_k^Ts_k # model reduction\nDefine mu_k # gradient error indicator","category":"page"},{"location":"","page":"Home","title":"Home","text":"If mu_k geq kappa_m\nSelect precisions such that mu_k leq kappa_m, go to 3.\nEnd If\nIf omega_f(x_k)  eta_0 Delta T_k\nSelect pi_f such that omega_f(x_k) leq eta_0 Delta T_k\nCompute f_k = f(x_kpi_f)\nEnd If","category":"page"},{"location":"","page":"Home","title":"Home","text":"Select pi_f^+ such that omega(c_k) leq eta_0 Delta T_k\nCompute f_k^+ = hatf(c_kpi_f^+)\nCompute rho_k = dfracf_k - f_k^+Delta T_k","category":"page"},{"location":"","page":"Home","title":"Home","text":"If rho_k geq eta_1 # step acceptance\nx_k+1 = c_k\n, f_k+1 = f_k^+\nEnd If","category":"page"},{"location":"","page":"Home","title":"Home","text":"Compute $ \\sigma_k = \\left{ \\begin{array}{lll}","category":"page"},{"location":"","page":"Home","title":"Home","text":"\\gamma1 \\sigmak & \\text{if} & \\rhok \\geq \\eta2 \\\n    \\sigmak & \\text{if} & \\rhok \\in  \\eta1,\\eta2  \\\n    \\gamma2 \\sigmak & \\text{if} & \\rhok < \\eta1   \\end{array}    \\right.$","category":"page"},{"location":"","page":"Home","title":"Home","text":"End While  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"return x_k","category":"page"},{"location":"","page":"Home","title":"Home","text":"MPR2 stops either when:","category":"page"},{"location":"","page":"Home","title":"Home","text":"a point x_k satisfying g_k leq dfracepsilon1+omega_g(x_k) has been found (see omega_g definition), which ensures that $ \\|\\nabla f(x_k)\\| \\leq \\epsilon$, or \nno FP format enables to achieve required precision on the objective or mu indicator.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The indicator mu_k aggregates finite-precision errors due to gradient evaluation (omega_g(x_k)), and the computation of the step (s_k), candidate (c_k) and model reduction Delta T_k as detailed in the Rounding Error Handling section.     ","category":"page"},{"location":"#**Rounding-Errors-Handling**","page":"Home","title":"Rounding Errors Handling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Anything computed in MPR2 suffers from rounding errors since it runs on a machine, which necessarily performs computation with finite-precision arithmetic. Below is the list of rounding errors that are handled by MPR2 such that convergence and numerical stability is guaranteed.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Dot Product Error: gamma_n","category":"page"},{"location":"","page":"Home","title":"Home","text":"The model for the dot product error that is used by default is     fl(xy) - xy leq xy gamma(nu) with     gamma nu rightarrow n*u     where x and y are two FP vectors (same FP format) of dimension n and u is the round-off unit of the FP format. This is a crude yet guaranteed upper bound on the error. The user can choose its own formula for gamma, see FPMPNLPModel: Creating a Multi-Precision Model section for practical use.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Candidate Computation Both the step and the candidate are computed inexactly.\nInexact Step: the computed step is s_k = fl(g_ksigma_k) = (g_ksigma_k)(1+delta) neq g_ksigma_k.\nInexact Candidate: the computed candidate is c_k = fl(x_k+s_k) = (x_k+s_k)(1+delta) neq x_k+s_k.\nThese two errors implies that c_k is not along the descent direction g_k and as such can be interpreted as additional gradient error to  omega_g(x_k). To handle these errors, MPR2 computes the ratio phi_k = x_ks_k. The greater phi_k, the greater the possible deviation of c_k from the direction g_k. These errors are aggregated in the mu_k indicator (detailed in 5.).\nModel Decrease Computation","category":"page"},{"location":"","page":"Home","title":"Home","text":"The FP computation error for the model decrease Delta T_k is such that,     fl(Delta T_k) =  Delta T_k (1+vartheta_n), with vartheta_n leq gamma(nu)     MPR2 handles this error by aggregating alpha(nu) in the indicator mu_k, with alpha(nu) = dfrac11-gamma(nu)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Norm Computation  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The finite-precision error for norm computation of a FP vector x of dimension n is     $ |fl(\\|x\\|)-\\|x\\|| \\leq fl(\\|x\\|)\\beta(n+2,u)$ with     betanu rightarrow max(sqrtgamma(nu)-1-1sqrtgamma(nu)+1-1)     beta function cannot be chosen by the user.     The beta function is used in MPR2 implementation to handle finite-precision error of norm computation for ","category":"page"},{"location":"","page":"Home","title":"Home","text":"g_k\nnorm computation: the stopping criterion implemented in MPR2 is   g_k leq dfrac11+beta (n+2u)dfrac11+omega_g(x_k) which ensures that nabla f(x_k)leq epsilon. \nphi_k\nratio computation (see 3.): phi_k requires the norm of x_k and s_k. MPR2 actually defines ","category":"page"},{"location":"","page":"Home","title":"Home","text":"phi_k =  dfracfl(x_k)fl(x_k)dfrac1+beta(n+2u)1-beta(n+2u) which ensures that phi_k geq x_k  s_k.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Mu Indicator  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"mu_k is an indicator which aggregates","category":"page"},{"location":"","page":"Home","title":"Home","text":"the gradient evaluation error omega_g(x_k),\nthe inexact step and candidate errors (phi_k),\nthe model decrease error (gamma(n+1u), alpha(nu)).  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The formula for mu_k is     mu_k = dfracalpha(nu) omega_g(x_k)(1+u(phi_k +1)) + alpha(nu) lambda_k + u+ gamma(n+1u)alpha(n+1u)1-u.     Note that if no rounding error occurs for (u = 0), one simply has mu_k = omega_g(x_k).     The implementation of line 7. of MPR2 (as described in the above section) consists in recomputing the step, candidate, x_k and/or s_k norm, model decrease or gradient with higher precision FP formats (therefore decreasing u) until mu_k leq kappa_m. For details about default strategy, see recomputeMu!() documentation.","category":"page"},{"location":"#Conditions-on-Parameters","page":"Home","title":"Conditions on Parameters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MPR2 parameters can be chosen by the user (see Section Basic Use) but must satisfy the following inequalities:","category":"page"},{"location":"","page":"Home","title":"Home","text":"0 leq eta_0 leq frac12eta_1\n0 leq eta_1 leq eta_2  1\neta_0+dfrackappa_m2 leq 05(1-eta_2)\neta_2  1\n\n0gamma_11gamma_2","category":"page"},{"location":"#Basic-Use","page":"Home","title":"Basic Use","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MPR2 solver relies on multi-precision models structure FPMPNLPModels (Floating Point Multi Precision Non Linear Programming Models), that derives from NLPModels.jl. This structure embeds the problem and provides the functions to evaluate the objective and the gradient with several FP formats. These evaluation functions are used by MPR2Solver to evaluate the objective and gradient with different FP format.","category":"page"},{"location":"#**FPMPNLPModel**:-Creating-a-Multi-Precision-Model","page":"Home","title":"FPMPNLPModel: Creating a Multi-Precision Model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See FPMPNLPModel documentation.","category":"page"},{"location":"#**Evaluation-Mode**","page":"Home","title":"Evaluation Mode","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FPMPNLPModel implements tools to evaluate the objective and the gradient and the error bounds omega_f(x_k) and omega_g(x_k) with two modes:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Interval Arithmetic: Using interval arithmetic enables to track rounding errors and to provide guaranteed bounds.MultiPrecisionR2.jl relies on IntervalArithmetic.jl library to perform interval evaluations of the gradient and the objective.  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Warning: Using interval evaluation can lead to significantly long computation time. Interval evaluation might fail for some objective functions due to IntervalArithmetic.jl errors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Relative Error: The error bounds omega_f(x_k) and omega_g(x_k) are estimated as fractions of hatf(x_k) and hatg(x_k), respectively. It is to the user to provide these fraction values.  ","category":"page"},{"location":"","page":"Home","title":"Home","text":"By default, interval arithmetic is used for error bound computation. If relative error bounds have to be used, ωfRelErr and ωgRelErr keyword arguments must be used when calling the constructor function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"FPMPNLPModel Example 1: Interval Arithmetic for error bounds","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing IntervalArithmetic\nusing ADNLPModels\n\nsetrounding(Interval,:accurate)\nFP = [Float16, Float32] # selected FP formats\nf(x) = x[1]^2 + x[2]^2 # objective function\nx = ones(2) # initial point\nx16 = Float16.(x) # initial point in Float16\nx32 = Float32.(x) # initial point in Float32\nMPmodel = FPMPNLPModel(f,x32,FP); # will use interval arithmetic for error evaluation\nf16, omega_f16 = objerrmp(MPmodel,x16) # evaluate objective and error bound at x with T[1] = Float16 FP model\nf32, omega_f32 = objerrmp(MPmodel,x32) # evaluate objective and error bound at x with T[1] = Float16 FP model\ng16, omega_g16 = graderrmp(MPmodel,x16) # evaluate gradient and error bound at x with T[2] = Float32 FP model\ng32, omega_g32 = graderrmp(MPmodel,x32) # evaluate gradient and error bound at x with T[2] = Float32 FP model","category":"page"},{"location":"","page":"Home","title":"Home","text":"FPMPNLPModel Example 2: Relative error bounds","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing ADNLPModels\n\nFP = [Float16, Float32] # selected FP formats\nf(x) = x[1]^2 + x[2]^2 # objective function\nx = ones(2) # initial point\nx16 = Float16.(x) # initial point in Float16\nx32 = Float32.(x) # initial point in Float32\nωfRelErr = [0.1,0.01] # objective error: 10% with Float16 and 1% with Float32\nωgRelErr = [0.05,0.02] # gradient error (norm): 5% with Float16 and 2% with Float32\nMPmodel = FPMPNLPModel(f,x32,FP;ωfRelErr=ωfRelErr,ωgRelErr=ωgRelErr);\nf32, omega_f32 = objerrmp(MPmodel,x32) # evaluate objective and error bound at x with T[1] = Float32 FP model\ng16, omega_g16 = graderrmp(MPmodel,x16) # evaluate gradient and error bound at x with T[2] = Float16 FP model","category":"page"},{"location":"","page":"Home","title":"Home","text":"FPMPNLPModel Example 3: Mixed Interval/Relative Error Bounds","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing IntervalArithmetic\nusing ADNLPModels\n\nsetrounding(Interval,:accurate)\nFP = [Float16, Float32] # selected FP formats\nf(x) = x[1]^2 + x[2]^2 # objective function\nx = ones(2) # initial point\nx16 = Float16.(x) # initial point in Float16\nx32 = Float32.(x) # initial point in Float32\nωgRelErr = [0.05,0.02] # gradient error (norm): 5% with Float16 and 2% with Float32\nMPmodel = FPMPNLPModel(f,x32,FP; ωgRelErr = ωgRelErr) # use interval for objective error bound and relative error bound for gradient\nf32, omega_f32 = objerrmp(MPmodel,x32) # evaluate objective and error bound with interval at x with T[1] = Float32 FP model\ng16, omega_g16 = graderrmp(MPmodel,x16) # evaluate gradient and error bound at x with T[2] = Float16 FP model","category":"page"},{"location":"","page":"Home","title":"Home","text":"FPMPNLPModel Example 4: Interval evaluation is slow","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing IntervalArithmetic\nusing ADNLPModels\n\nsetrounding(Interval,:accurate)\nFP = [Float32] # selected FP formats\nn = 1000\nf(x) = sum([x[i]^2 for i =1:n])  # objective function\nx = ones(n) # initial point\nx32 = Float32.(x) # initial point in Float32\nMPmodelInterval = FPMPNLPModel(f,x32,FP) # use interval for objective and gradient error bounds\nωfRelErr = [Float64(sqrt(eps(Float32)))] # objective error\nωgRelErr = [Float64(sqrt(eps(Float32)))] # gradient error (norm)\nMPmodelRelative = FPMPNLPModel(f,x32,FP,ωfRelErr = ωfRelErr, ωgRelErr = ωgRelErr) # will use relative error bounds\n# precompile\nobjerrmp(MPmodelInterval,x32)\nobjerrmp(MPmodelRelative,x32)\ngraderrmp(MPmodelInterval,x32)\ngraderrmp(MPmodelRelative,x32)\n\n@time objerrmp(MPmodelInterval,x32) # interval evaluation of objective\n@time objerrmp(MPmodelRelative,x32) # classic evaluation of objective\n@time graderrmp(MPmodelInterval,x32) # interval evaluation of gradient\n@time graderrmp(MPmodelRelative,x32) # classic evaluation of gradient","category":"page"},{"location":"#**High-Precision-Format**","page":"Home","title":"High Precision Format","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FPMPNLPModel performs some operations with a high precision FP format to provide more numerical stability. The convergence of MPR2 relies on the fact that such operations are \"exact\", as if performed with infinite precision. This high precision format is also used by MPR2solve in a similar way and for the same reasons (see section High Precision Format: MPR2 Solver)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This high precision format is FPMPNLPModel.HPFormat and can be given as a keyword argument upon instanciation. The default value is Float64. Note that this high precision format also correspond to the type parameter H in struct FPMPNLPModel{H,F,T<:Tuple}. It is expected that FPMPNLPModel.HPFormat has at least equal or greater machine epsilon than the highest precision FP format that can be used for objective or gradient evaluation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"FPMPNLPModel Example 5: HPFormat value","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing IntervalArithmetic\nusing ADNLPModels\n\nsetrounding(Interval,:accurate)\nFP = [Float16, Float32, Float64] # selected FP formats, max eval precision is Float64\nf(x) = x[1]^2 + x[2]^2 # objective function\nx = ones(2) # initial point\nMPmodel = FPMPNLPModel(f,x,FP); # throws warning\nMPmodel = FPMPNLPModel(f,x,FP,HPFormat = Float32); # throws error","category":"page"},{"location":"#**Gradient-and-Dot-Product-Error**:-Gamma-Callback-Function","page":"Home","title":"Gradient and Dot Product Error: Gamma Callback Function","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FPMPNLPModel.graderrmp() computes both the gradient and the error bound omega_g as in the Notations section. To do so, it is necessary to compute norm of the gradient and therefore to take the related error into account, given by the beta function (see section Rounding Errors Handling). Note that beta expresses with gamma function which models the dot product error. An implicit condition is that gamma(nu_max) leq 1, with u_max the smallest unit-roundoff among the provided FP formats.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For example, if the highest precision format Float32 is used, u_max approx 1 e^-7 which limits the size of the problems that can be solve to approx 1e^7 variables with the default implementation γ(nu) = n*u. If this is a problem, the user can provide its own callback function FPMPNLPModels.γfunc. This is illustrated in the example below.","category":"page"},{"location":"","page":"Home","title":"Home","text":"FPMPNLPModel Example 5: Gradient and Dot Product Error The code below returns an error at the instanciation of FPMPNLPModels indicating that the dimension of the problem is too big with respect to the highest precision FP format provided (Float16).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing IntervalArithmetic\nusing ADNLPModels\n\nsetrounding(Interval,:accurate)\nFP = [Float16] # limits the size of the problem to n = 1/eps(Float16) (= 1000)\ndim = 2000 # dimension of the problem too large\nf(x) =sum([x[i]^2 for i=1:dim]) # objective function\nx = ones(Float16,dim) # initial point\nMPmodel = FPMPNLPModel(f,x,FP); # throw error","category":"page"},{"location":"","page":"Home","title":"Home","text":"The user can provide a less pressimistic gamma function for dot product error bound. Warning: Providing your own gamma function can break the convergence properties of MPR2.","category":"page"},{"location":"","page":"Home","title":"Home","text":"gamma(n,u) = sqrt(n)*u # user defined γ function, less pessimistic than n*u used by default\nMPmodel = FPMPNLPModel(f,x,FP,γfunc = gamma); # no error since sqrt(dim)*eps(Float16) < 1","category":"page"},{"location":"#**MPR2Solver**","page":"Home","title":"MPR2Solver","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See MPR2Solver documentation.","category":"page"},{"location":"#**Gamma-Function**","page":"Home","title":"Gamma Function","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"As mentionned in section Rounding Errors Handling, MPR2 relies on the dot product error function gamma to handle some rounding errors (norm and model decrease). The gamma function used by MPR2 is the one of the solve!() function's FPMPNLPModel argument. ","category":"page"},{"location":"#**High-Precision-Format:-MPR2-Solver**","page":"Home","title":"High Precision Format: MPR2 Solver","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MPR2 uses a high precision format to compute \"exactly\" the values that are defined (see section MPR2 Algorithm Broad Description). This high precision format corresponds to the type parameter H in MultiPrecisionR2.solve!(). The high precision format used by MPR2 is FPMPNLPModel.HPFormat of MPnlp argument of MultiPrecisionR2.solve!() (see section High Precision Format).","category":"page"},{"location":"#**Lack-of-Precision**","page":"Home","title":"Lack of Precision","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The default implementation of MPR2 stops when the condition on the objective evaluation error or the mu indicator fails with the highest precision evaluations (see Lines 7,8,10 of MPR2 algorithm in section MPR2 Algorithm Broad Description). If this happens, MPR2 returns the ad-hoc warning message. The user can tune MPR2 parameters to try to avoid such early stop via the structure MPR2Params and provide it as a keyword argument to solve!. Typically, ","category":"page"},{"location":"","page":"Home","title":"Home","text":"If the objective error is too big: the user should increase eta_0 parameter.\nIf mu_k is too big: the user should increase kappa_m parameter.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The user has to make sure that the parameters respect the convergence conditions (see section Conditions on Parameters).","category":"page"},{"location":"","page":"Home","title":"Home","text":"MPR2Solver Example 1: Lack of Precision and Parameters Selection","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nusing IntervalArithmetic\nusing ADNLPModels\n\nsetrounding(Interval,:accurate)\nFP = [Float16, Float32] # selected FP formats, max eval precision is Float64\nf(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nx = Float32.(1.5*ones(2)) # initial point\nHPFormat = Float64\nMPmodel = FPMPNLPModel(f,x,FP,HPFormat = HPFormat);\nsolver = MPR2Solver(MPmodel);\nstat = MPR2(MPmodel) ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Running the above code block returns a warning indicating that R2 stops because the error on the objective function is too big to ensure convergence. The problem can be overcome in this example by tolerating more error on the objective by increasing eta_0.","category":"page"},{"location":"","page":"Home","title":"Home","text":"η₀ = 0.1 # greater than default value 0.01\nη₁ = 0.3\nη₂ = 0.7\nκₘ = 0.1\nγ₁ = Float16(1/2) # must be FP format of lowest evaluation precision for numerical stability\nγ₂ = Float16(2) # must be FP format of lowest evaluation precision for numerical stability\nparam = MPR2Params(η₀,η₁,η₂,κₘ,γ₁,γ₂)\nstat = MPR2(MPmodel,par = param) ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Now MPR2 converges to a first order critical point since we tolerate enough error on the objective evaluation.","category":"page"},{"location":"#**Evaluation-Counters**","page":"Home","title":"Evaluation Counters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MPR2 counts the number of objective and gradient evaluations are counted for each FP formats. They are stored in counters field of the FPNLPModel structure. The counters field is a MPCounters.","category":"page"},{"location":"","page":"Home","title":"Home","text":"setrounding(Interval,:accurate)\nFP = [Float16, Float32, Float64]\nf(x) = sum(x.^2) \nx0 = ones(10)\nHPFormat = Float64\nMPmodel = FPMPNLPModel(f,x0,FP,HPFormat = HPFormat);\nMPR2(MPmodel,verbose=1)\n@show MPmodel.counters.neval_obj # numbers of objective evaluations \n@show MPmodel.counters.neval_grad # numbers of gradient evaluations","category":"page"},{"location":"#Advanced-Use","page":"Home","title":"Advanced Use","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionR2.jl does more than implementing MPR2 algorithm as describes in Section MPR2 Algorithm General Description. MPR2Precision.jl enables the user to define its own strategy to select evaluation precisions and to handle evaluation errors. This is made possible by using callback functions when calling MultiPrecisionR2.solve!(). The default implementation of MultiPrecisionR2.solve!() relies on specific implementation of these callback functions, which are included in the package. The user is free to provide its own callback functions to change the behavior of the algorithm. ","category":"page"},{"location":"#Diving-into-MPR2-Implementation","page":"Home","title":"Diving into MPR2 Implementation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MPR2Precision.jl relies on callback functions that handle the objective and gradient evaluation. These callback functions are expected to compute values for the objective and the gradient and handle the evaluation precision. In the default implementation, the conditions on the error bound of objective and gradient evaluation are dealt with in these callback functions. That is why such convergence conditions (which user might choose not to implement) does not appear in the minimal implementation description of the code below.","category":"page"},{"location":"#**Minimal-Implementation-Description**","page":"Home","title":"Minimal Implementation Description","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here is a minimal description of the implementation of MultiPrecisionR2.solve!() to understand when and why these functions are called. Please refer to the implementation for details. Note that the callback functions templates are not respected for the sake of explanation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"solve!(solver::MPR2solver{T},MPnlp::FPMPNLPModel{H};kwargs...)\n  # ...\n  # some initialization (parameters, containers,...)\n  # ...\n  compute_f_at_x!() # initialize objective value at x0\n  # check for possible overflow \n  compute_g!() # initialize gradient value at x0\n  # check for possible overflow\n  # ...\n  # some more initialization (gradient norm tolerance,...)\n  # ...\n  while # main loop\n    # compute step as sk = -gk/σk\n    # check for step overflow\n    # compute candidate as c = x+s\n    # check for candidate overflow\n    # compute model decrease ΔT = -g^T s\n    # check for model decrease under/overflow\n    recompute_g!() # possibly recompute g(x) and ωg(x) if gradient error/mu indicator is too big\n    if # not enough precision with max precision\n      break\n    end\n    if # gradient recomputed by recompute_g!() \n      # recompute step with new value of the gradient\n      # check overflow\n      # recompute candidate with new step\n      # check overflow\n      # recompute model decrease with new gradient and step\n    end\n    compute_f_at_x() # possibly recompute f(x) and ωf(x) if ωf(x) is too big\n    if # not enough precision with max precision (ωf(x) too big)\n      break\n    end\n    compute_f_at_c!() # compute f(c) and ωf(c)\n    if # not enough precision with max precision (ωf(c) too big)\n      break\n    end\n    # ...\n    # compute ρ = (f(x) - f(c))/ΔT\n    # update x and σ\n    # ... \n    if # ρ ≥ η₁\n      compute_g!() # compute g(c) and  ωg(c)\n      # check overflow\n      # ...\n      # update values for next iteration\n      # ...\n      selectPic!()\n    end\n    # ...\n    # check for termination\n    # ...\n  end # while main loop\n  return status()\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The callback functions, and what they are supposed to do, are ","category":"page"},{"location":"","page":"Home","title":"Home","text":"compute_f_at_x!(): Selects evaluation precision and computes objective function at the current point x_k such that possible conditions on error bounds are ensured. In the main loop, although the objective at x_k has already been computed at a previous iteration (f(c_j) = f(x_k) with j the last successful iteration) it might be necessary to recompute it to achieve smaller evaluation error (omega_f(x_k)) if necessary. This function is also called for initialization (before the main loop), where typically no bound on evaluation error is required.\ncompute_f_at_c!(): Selects evaluation precision and computes objective function at the candidate c_k such that possible conditions on error bounds are ensured.\ncompute_g!(): Selects evaluation precision and computes gradient at the candidate c_k. It is possible here to include condiditions on gradient bound error omega_g(x_k). In the default implementation, multiple sources of rounding error are taken into account in the mu indicator that requires to compute the step, candidate and model decrease (see Rounding Errors Handling). That is why in the default implementation no conditions on gradient error are required in this callback function, but are implemented in recompute_g!().\nrecompute_g!(): Selects evaluation precision and recomputes gradient at the current point x_k. This callback enables to recompute the gradient if necessary with a better precision if needed. In the default implementation, this callback implement the condition on the mu indicator (see Rouding Error Handling) and implements strategies to achieve sufficiently low value of mu to ensure convergence.\nselectPic!(): Select FP format for c_k+1, enables to lower the FP format used for evaluation at the next iteration. See Candidate Precision Selection section for details.","category":"page"},{"location":"#**Callback-Functions:-Templates**","page":"Home","title":"Callback Functions: Templates","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"prec_fail::Bool = compute_f_at_c!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T) where {H, L, E, T <: Tuple}\nprec_fail::Bool = compute_f_at_x_default!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T) where {H, L, E, T <: Tuple}\nprec_fail::Bool = compute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T, g::T) where {H, L, E, T <: Tuple}\nprec_fail::Bool, recompute_g::Bool = recompute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T, g::T, s::T) where {H, L, E, T <: Tuple}","category":"page"},{"location":"","page":"Home","title":"Home","text":"Arguments:","category":"page"},{"location":"","page":"Home","title":"Home","text":"m:FPMPNLPModel{H}: multi-precision model, needed to perform objective/gradient evaluation\nst::MPR2State{H}: States or intermediate variables of MPR2 algorithm (rho, mu, ...), see MPR2State documentation\nπ::MPR2Precisions: Current precision indices, see MPR2Precisions documentation\np::MPR2Params{H, L}: algorithm parameters, see MPR2Params documentation\ne::E: user defined structure to store additional information if needed\nx::T: current point\ns::T: current step\nc::T: current candidate\ng::T: current gradient","category":"page"},{"location":"","page":"Home","title":"Home","text":"Outputs","category":"page"},{"location":"","page":"Home","title":"Home","text":"prec_fail::Bool: true if the evaluation failed, typical reason is that not enough evaluation could be reached. If prec_fail == true, stops the algorithm and set st.status = :exception.\nrecompute_g::Bool: true if recompute_g!() has recomputed the gradient. In this case, the set, candidate and model decrease are recomputed (see Minimal Implementation Description)","category":"page"},{"location":"#**Callback-Functions:-Expected-Behaviors**","page":"Home","title":"Callback Functions: Expected Behaviors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modified Variables: The callback functions are expected to update all the variables that they modify. These variables are typically MPR2 states in st::MPR2State{H} structure, the current precisions in π::MPR2Precisions structure, extra values in user defined e::E structure. The callback functions compute_g!() and recompute_g!() are also expected to modify the gradient g::T.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Variable that should not be modified: The callback functions should not modify x, s, and c.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Below is a table that recaps what variables each callback can/should update. |Callback| Modified Variables| | ––– | ––––––––- | |compute_f_at_x!()| st.status, st.f, st.ωf, π.πf |compute_f_at_c!()| st.status, st.f⁺, st.ωf⁺, π.πf⁺ |compute_g!()| st.status, g, st.ωg, π.πg |recompute_g()| st.status, g, st.ωg, π.πg, st.ΔT, π.ΔT, st.x_norm, π.πnx, st.s_norm, π.πns, st.ϕ, st.ϕhat, π.πc, st.μ","category":"page"},{"location":"#**Multi-Precision-Evaluation-and-Vectors-Containers**","page":"Home","title":"Multi-Precision Evaluation and Vectors Containers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MPR2 performs objective and gradient evaluations and error bounds estimation with different FP format. These evaluations are performed with FPMPNLPModels.objerrmp() and FPMPNLPModels.graderrmp!() functions (see FPMPNLPModels documentation and FPMPNLPModel section). These functions expect as input a Vector{S} where to evaluate the objective/gradient where S is the FP format that matches FPMPNLPModel.FPList[id] the FP format of index id.  That is, it is not possible to perform an evaluation in an FP format different than the FP format of the input vector. This further means that, when MPR2 is running, it is necessary to change the FP format of x_k, s_k, c_k, and g_k.   In practice, x_k, s_k, c_k, g_k are implemented as tuple of vectors of the FP formats used to perform evaluations (i.e. FPMPNLPModel.FPList).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Example 1: Simple Callback Here is a simple callback function for computing the objective function that does not take evaluation error into account.","category":"page"},{"location":"","page":"Home","title":"Home","text":"function my_compute_f_at_c!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T) where {H, L, E, T <: Tuple}\n  st.f⁺ = objmp(m, c[π.πc]) # FP format m.FPList[π.πc] will be used for obj evaluation.\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The implementation of MPR2 automatically updates the containers for x, s, c, and g during execution, so that the user does not have to deal with that part. The function MultiPrecisionR2.umpt!() (see documentation) update the containers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Warning: MultiPrecisionR2.umpt!(x::Tuple, y::Vector{S}) updates only the vectors of x of FP format with precision greater or equal to the FP format of y. umpt! is implemented this to avoid rounding error due to casting into a lower precision format and overflow. This is closely related to the concept of \"forbidden evaluation\" detailed in the next section. If the precision π.πx = 2, it means that the x[i]s vectors are up-to-date for i>=2. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Example: Container Update","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MultiPrecisionR2\nFP = [Float16,Float32,Float64]\nxini = ones(5)\nx = Tuple(fp.(xini) for fp in FP)\nxnew = Float32.(zeros(5))\numpt!(x,xnew)\nx # only x[2] and x[3] are updated","category":"page"},{"location":"","page":"Home","title":"Home","text":"Example: Lower Precision Casting Error","category":"page"},{"location":"","page":"Home","title":"Home","text":"x64 = [70000.0,1.000000001,0.000000001,-230000.0] # Vector{Float64}\nx16 = Float16.(x64) # definitely not x64","category":"page"},{"location":"#**Forbidden-Evaluations**","page":"Home","title":"Forbidden Evaluations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A \"forbidden evaluation\" consists in evaluating the objective or the gradient with a FP format lower than the FP format of the point where it is evaluated. For example, if x is a Vector{Float64}, evaluating the objective in Float16 will implicitely cast x into a Vector{Float16} and then evaluate the objective with this Float16 vector. The problem is that due to rounding error, the casted vector is different than the initial Float64 x. The objective is therefore not evaluated at x but at a different point. This causes numerical instability in MPR2 and must be avoided.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Example: Forbidden Evaluation","category":"page"},{"location":"","page":"Home","title":"Home","text":"f(x) = 1/(x-10000) # objective\nx64 = 10001.0\nf(x64) # returns 1 as expected\nx16 = Float16(x64) # rounding error occurs: x16 != x64\nf(x16) # definitely not the expected value for f(x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"When implementing the callback functions, the user should make sure that the evaluation precision selected for objective or gradient evaluation (π.πf, π.πg) is always greater or equal than the precision of the point where the evaluation is performed (π.πx or π.πc).","category":"page"},{"location":"#**Step-and-Candidate-Computation-Precision**","page":"Home","title":"Step and Candidate Computation Precision","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MPR2 implementation checks for possible under/overflow when computing the step s and the candidate c and increase FP format precision if necessary to avoid that. MPR2 implementation also updates π.πs and π.πc if necessary. These careful step and candidate computations are implemented in the MultiPrecisionR2.ComputeStep!() and MultiPrecisionR2.computeCandidate!() (see documentation). It is important to note that, even if the gradient g has been computed with π.πg precision, the precision π.πs and π.πc can be greater than π.πg because overflow has occurred. Has a consequence, the user should not take for granted than π.πc == π.πg when selecting FP format for objective/gradient evaluation at c but must rely on the candidate precision π.πc.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Example: Step Overflow","category":"page"},{"location":"","page":"Home","title":"Home","text":"g(x) = 2*x # gradient\nx16 = Float16(1000)\nsigma = Float16(1/2^10) # regularization parameter\ng16 = g(x16)\ns = g16/Float16(sigma) # overflow\ns = Float32(g16)/Float32(sigma) # no overflow, s is a Float32, this is what computeStep!() does","category":"page"},{"location":"#**Candidate-Precision-Selection**","page":"Home","title":"Candidate Precision Selection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In light of the \"forbidden evaluation\" concept (see section Forbidden Evaluations), if no particular care is given, the objective/gradient evaluation precisions can only increase from one iteration to another. To illustrate that consider that ","category":"page"},{"location":"","page":"Home","title":"Home","text":"At iteration k: x_k is Float32, meaning that hatg(x_k) is Float32 (or higher precision FP format), but let's say Float32 here. By extension, s_k = -g_ksigma_k , and c_k = x_k+s_k are both Float32. It means that f(x_k) and f(x_k+s_k) are computed with Float32 or higher because of forbidden evaluations.\nAt iteration k+1:\nIf the iteration is successful: then x_k+1 = c_k is Float32, meaning again that hatg(x_k+1) is Float32 or higher precision format, and s_k+1 and c_k+1 are also Float32 or higher precision format. It further means that f(x_k+1) and f(c_k+1) can only be computed with Float32 or higher percision format.\nIf the iteration is unsuccessful: then x_k+1 = x_k is Float32 and we have the same than if the iteration is successful.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To overcome this issue, and enable to decrease the FP formats used for objective/gradient evaluation, the user has the freedom to chose at iteration k the FP format of c_k+1. Indeed, there is no restriction on how the candidate is computed. Considering the above example, at iteration k+1 c_k+1 is Float32 but we can cast it (with rounding error) into a Float16, without breaking the convergence. Casting c_k+1 allows to compute f(c_k+1) with Float16, and if the iteration is successful, x_k+2 = c_k+1 is also a Float16, meaning that the gradient and objective can be computed with Float16 at x_k+2.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The callback function selectPif!(), called at the end of the main loop (see section Minimal Implementation Description) update π.πc so that MPnlp.FPList[π.πc] will be the FP format of c at the next iteration. The function ComputeCandidate!(), at the begining of the main loop, handles the casting of the candidate into MPnlp.FPList[π.πc].","category":"page"},{"location":"","page":"Home","title":"Home","text":"The expected template for selectPif!() callback function is function selectPic!(π::MPR2Precisions). Only π.πc is expected to be modify.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Callback Modified Variables\nselectPic!() π.πc","category":"page"},{"location":"#What-MultiPrecisionR2.solve!()-Handles","page":"Home","title":"What MultiPrecisionR2.solve!() Handles","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Runs the main loop until a stopping condition is reached (max iteration or nabla f(x) leq epsilon).\nUses the default callback functions for whichever has not been provided by the user.\nDeals with error due to norm computation to ensure nabla f(x) leq epsilon.\nDeals with high precision format computation to simulate \"exactly computed\" values (see sections High Precision Format and High Precision Format: MPR2 Solver).\nUpdate the containers x, s, c and g (see Multi-Precision Evaluation and Vector Containers)\nMake sure no under/overflow occurs when computing s and c, update the precision π.πs and π.πc if necessary (see Step and Candidate Computation Precision)","category":"page"},{"location":"#What-MultiPrecisionR2.solve!()-Does-not-Handle","page":"Home","title":"What MultiPrecisionR2.solve!() Does not Handle","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"solve!() does not check that objective/gradient evaluation are preformed with a suitable FP format in the callback functions(see sections Multi-Precision Evaluation and Vector Containers and Forbidden Evaluation).\nsolve!() does not ensure convergence if the user uses its own callback functions.\nsolve!()does not update objective/gradient values and precision outside the callback functions. See Callback Functions: Expected Behavior for proper callbacks implementation.\nsolve!() does not handle overflow that might occur when evaluation the objective/gradient in the callback functions. It is up to the user to make sure no overflowed value is returned by the callbacks. See Implementation Examples for dealing with overflow properly.\nsolve!() does not throw error/warning if evaluations in callback have failed. It is up to the user to handle that.","category":"page"},{"location":"#Callback-Functions-Cheat-Sheet","page":"Home","title":"Callback Functions Cheat Sheet","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Callback Description Outputs Expected Modified Variables\ncompute_f_at_x!() Select obj. FP format, compute f(x_k) and ωf(x_k) precfail::Bool : true if \\omegaf(x_k)$ is too big, stops main loop st.status, st.f, st.ωf, π.πf\ncompute_f_at_c!() Select obj. FP format and compute f(c_k) and ωf(c_k) precfail::Bool: true if \\omegaf(c_k)$ is too big, stops main loop st.status, st.f⁺, st.ωf⁺, π.πf⁺\ncompute_g!() Select grad FP format and compute g(c_k) and ωg(c_k) precfail::Bool: true if \\omegag(c_k)$ is too big, stops main loop st.status, g, st.ωg, π.πg\nrecompute_g() Select grad FP format and recompute g(x_k) and ωg(x_k) precfail::Bool: true if \\omegag(ck)$ is too big, stops main loop, grecompute::Bool: true if hatg(x_k) was recomputed st.status, g, st.ωg, π.πg, st.ΔT, π.ΔT, st.x_norm, π.πnx, st.s_norm, π.πns, st.ϕ, st.ϕhat, π.πc, st.μ\nselectPic!()  void π.πc","category":"page"},{"location":"#Implementation-Examples","page":"Home","title":"Implementation Examples","text":"","category":"section"},{"location":"#Example-1:-Precision-Selection-Strategy-Based-on-Step-Size-(Error-Free)","page":"Home","title":"Example 1: Precision Selection Strategy Based on Step Size (Error Free)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This example implements a precision selection strategy for the objective and gradient based on the norm of the step size, which does not take into account evaluation errors. The strategy is to choose the FP format for evaluation such that the norm of the step is greater than the square root of the unit roundoff.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The callback functions must handles precision selection for evaluations and optionally error/warning messages if evaluation fails (typically overflow or lack of precision)","category":"page"},{"location":"","page":"Home","title":"Home","text":"using LinearAlgebra\n\nfunction my_compute_f_at_c!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T) where {H, L, E, T <: Tuple}\n  πmax = length(m.FPList) # get maximal allowed precision\n  eval_prec = findfirst(u -> sqrt(u) < st.s_norm, m.UList) # select precision according to the criterion\n  if eval_prec === nothing # not enough precsion\n    @warn \" not enough precision for objective evaluation at c: ||s|| = $(st.s_norm) < sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))\"\n    st.status = :exception\n    return true\n  end\n  π.πf⁺ = max(eval_prec,π.πc) # evaluation precision should be greater or equal to the FP format of the candidate (see forbidden evaluation)\n  st.f⁺ = obj(m,c[π.πf⁺]) # eval objective only. solve!() made sure c[π.πf⁺] is up-to-date (see containers section)\n  while st.f⁺ == Inf # check for overflow\n    π.πf⁺ += 1\n    if π.πf⁺ > πmax\n      @warn \" not enough precision for objective evaluation at c: overflow\"\n      st.status = :exception\n      return true # objective overflow with highest precision FP format: this is a fail\n    end\n    st.f⁺ = obj(m,c[π.πf⁺])\n  end\n  return false\nend\n  \nfunction my_compute_f_at_x!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T) where {H, L, E, T <: Tuple}\n  πmax = length(m.FPList) # get maximal allowed precision\n  if st.iter == 0 # initial evaluation, step = 0, choose max precision\n    π.πf = πmax\n  else # evaluation in main loop\n    eval_prec = findfirst(u -> sqrt(u) < st.s_norm, m.UList) # select precision according to the criterion\n    if eval_prec === nothing # not enough precsion\n      @warn \" not enough precision for objective evaluation at x: ||s|| = $(st.s_norm) < sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))\"\n      st.status = :exception\n      return true\n    end\n    π.πf = max(eval_prec,π.πx) # evaluation precision should be greater or equal to the FP format of the current solution (see forbidden evaluation)\n  end\n  st.f = obj(m,x[π.πf]) # eval objective only. solve!() made sure x[π.πf] is up-to-date (see containers section)\n  while st.f == Inf # check for overflow\n    π.πf += 1\n    if π.πf > πmax\n      @warn \" not enough precision for objective evaluation at x: overflow\"\n      st.status = :exception\n      return true # objective overflow with highest precision FP format: this is a fail\n    end\n    st.f = obj(m,x[π.πf])\n  end\n  return false\nend\n\nfunction my_compute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T, g::T) where {H, L, E, T <: Tuple}\n  πmax = length(m.FPList) # get maximal allowed precision\n  if st.iter == 0 # initial evaluation, step = 0, choose max precision\n    π.πg = πmax\n  else # evaluation in main loop\n    eval_prec = findfirst(u -> sqrt(u) < st.s_norm, m.UList) # select precision according to the criterion\n    if eval_prec === nothing # not enough precsion\n      @warn \" not enough precision for gradient evaluation at c: ||s|| = $(st.s_norm) < sqrt(u($(m.FPList[end]))) = $(sqrt(m.UList[end]))\"\n      st.status = :exception\n      return true\n    end\n    π.πg = max(eval_prec,π.πg) # evaluation precision should be greater or equal to the FP format of the candidate (see forbidden evaluation)\n  end\n  grad!(m,c[π.πg],g[π.πg]) # eval gradient only. solve!() made sure x[π.πg] is up-to-date (see containers section)\n  while findfirst(elem->elem == Inf,g[π.πg]) !== nothing # check for overflow, gradient vector version\n    π.πg += 1\n    if π.πg > πmax\n      @warn \" not enough precision for gradient evaluation at c: overflow\"\n      st.status = :exception\n      return true # objective overflow with highest precision FP format: this is a fail\n    end\n    grad!(m,c[π.πg])\n  end\n  return false\nend\n\nfunction my_recompute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T, g::T, s::T) where {H, L, E, T <: Tuple}\n  # simply update norm of the step, since recompute_g!() is called at the begining of the main loop after step computation\n  πmax = length(m.FPList) # get maximum precision index\n  π.πns = π.πs # select precision for step norm computation\n  s_norm = norm(s[π.πs])\n  while s_norm == Inf || s_norm ==0.0 # handle possible over/underflow\n    π.πns = π.πns+1 # increase precision to avoid over/underflow\n    if π.πns > πmax\n      st.status = :exception\n      return true, false # overflow occurs with max precion: cannot compute s_norm with provided FP formats. Returns fail.\n    end\n    s_norm = norm(s[π.πns]) # compute norm with higher precision step, solve!() made sure s[π.πns] is up-to-date\n  end\n  st.s_norm = s_norm\n  return false, false\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let's try this implementation on a simple quadratic objective.","category":"page"},{"location":"","page":"Home","title":"Home","text":"FP = [Float16, Float32] # selected FP formats,\nf(x) = x[1]^2 + x[2]^2 # objective function\nomega = [0.0,0.0]\nx = Float32.(1.5*ones(2)) # initial point\nMPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omega, ωgRelErr = omega); # indicates the use of relative error only to avoid interval evaluation, relative errors will not be computed with above callbacks\nstat = MPR2(MPmodel;\ncompute_f_at_x! = my_compute_f_at_x!,\ncompute_f_at_c! = my_compute_f_at_c!,\ncompute_g! = my_compute_g!,\nrecompute_g! = my_recompute_g!);\nstat  # first-order stationary point has been found","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let's now try our implementation on the Rosenbrock function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"FP = [Float16, Float32] # selected FP formats,\nf(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nomega = [0.0,0.0]\nx = Float32.(1.5*ones(2)) # initial point\nMPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omega, ωgRelErr = omega); # indicates the use of relative error only to avoid interval evaluation, relative errors will not be computed with above callbacks\nstat = MPR2(MPmodel;\ncompute_f_at_x! = my_compute_f_at_x!,\ncompute_f_at_c! = my_compute_f_at_c!,\ncompute_g! = my_compute_g!,\nrecompute_g! = my_recompute_g!); # throw lack of precision warning","category":"page"},{"location":"","page":"Home","title":"Home","text":"The strategy implemented for precision selection does not allow to find a first-order critical point for the Rosenbrock function: the step becomes too small before MPR2 converges. Although this implementation is fast since it does not bother with evaluation errors, it is not very satisfactory since Example 1 in section Lack of Precision shows that the default implementation is able to converge to a first-order critical point. This highlights that it is important to understand how rounding errors occur and affect the convergence of the algorithm (see section MPR2 Algorithm: General Description) and that \"naive\" strategies like the one implemented above might not be satisfactory.","category":"page"},{"location":"#Example-2:-Switching-to-Gradient-Descent-When-Lacking-Objective-Precision","page":"Home","title":"Example 2: Switching to Gradient Descent When Lacking Objective Precision","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"It might happen that solve!() stops early because the objective evaluation lacks precision. Consider for example that we use consider relative evaluation error for the objective. If MPR2 converges to a point where the objective is big, the error can be big too, and if the gradient is small the convergence condition omega f(x_k) leq eta_0 Delta T_k = hatg(x_k)^2sigma_k is likely to fail. In that case, the user might want to continue running the algorithm without caring about the objective, that is, as a simple gradient descent. solve!() implementation allows enough flexibility to do so. In the implementation below, the user defined structure e is used to indicate what \"mode\" the algorithm is running: default mode or gradient descent. The callbacks compute_f_at_x! sets st.f = Inf and compute_f_at_c! sets st.f⁺ = 0 if gradient descent mode is used. This ensures that rho_k = Inf geq eta_1 and the step is accepted in gradient descent mode. In the implementation below, compute_f_at_x! and compute_f_at_c! selects the precision such that omega f(x_k) leq eta_0 Delta T_k in default mode. We implement compute_g! to set σ so that ComputeStep!() will use the learning rate 1/σ. We use the default recompute_g callback.","category":"page"},{"location":"","page":"Home","title":"Home","text":"mutable struct my_struct\n  gdmode::Bool\n  learning_rate\nend\n\nfunction my_compute_f_at_c!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T) where {H, L, E, T <: Tuple}\n  if !e.gdmode\n    ωfBound = p.η₀*st.ΔT\n    π.πf⁺ = π.πx # basic precision selection strategy\n    st.f⁺, st.ωf⁺, π.πf⁺ = objReachPrec(m, c, ωfBound, π = π.πf⁺)\n    if st.f⁺ == Inf # stop algo if objective overflow\n      @warn \"Objective evaluation overflow at x\"\n      st.status = :exception\n      return true\n    end\n    if st.ωf⁺ > ωfBound # evaluation error too big\n      @warn \"Objective evaluation error at x too big to ensure convergence: switching to gradient descent\"\n      e.gdmode = true\n      st.f⁺ = 0\n      return false\n    end\n  else # gradient descent mode\n    st.f⁺ = 0\n  end\n  return false\nend\n\nfunction my_compute_f_at_x!(m::FPMPNLPModel{H}, st::MPR2State{H}, π::MPR2Precisions, p::MPR2Params{H, L}, e::E, x::T) where {H, L, E, T <: Tuple}\n  πmax = length(m.EpsList)\n  if st.iter == 0 # initial evaluation before main loop\n    st.f, st.ωf, π.πf = objReachPrec(m, x, m.OFList[end], π = π.πf)\n  else # evaluation in the main loop\n    if !e.gdmode\n      ωfBound = p.η₀*st.ΔT\n      if st.ωf > ωfBound # need to reevaluate the objective at x\n        if π.πf == πmax # already at highest precision \n          @warn \"Objective evaluation error at x too big to ensure convergence: switching to gradient descent\"\n          e.gdmode = true\n          st.f = Inf\n          return false\n        end\n        π.πf += 1 # increase evaluation precision of f at x\n        st.f, st.ωf, π.πf = objReachPrec(m, x, ωfBound, π = π.πf)\n        if st.ωf > ωfBound # error evaluation too big with max precison\n          @warn \"Objective evaluation error at x too big to ensure convergence: switching to gradient descent\"\n          e.gdmode = true\n          st.f = Inf\n          return false\n        end\n      end\n    else # gradient descent mode\n      st.f = Inf\n    end\n  end\n  return false\nend\n\nfunction my_compute_g!(m::FPMPNLPModel{H}, st::MPR2State{H},  π::MPR2Precisions, p::MPR2Params{H, L}, e::E, c::T, g::T) where {H, L, E, T <: Tuple}\n  π.πg = π.πc # default strategy, could be a callback\n  st.ωg, π.πg = gradReachPrec!(m, c, g, m.OFList[end], π = π.πg)\n  if e.gdmode\n    st.σ = 1/e.learning_rate\n  end\n  return false\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let us first run MPR2() with the default implementation and relative evaluation error.","category":"page"},{"location":"","page":"Home","title":"Home","text":"FP = [Float16, Float32] # selected FP formats,\n#f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nf(x) = x[1]^2 + x[2]^2 +0.5\nomegaf = Float64.([0.01,0.005])\nomegag = Float64.([0.05,0.01])\nx = Float32.(1.5*ones(2)) # initial point\nMPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omegaf, ωgRelErr = omegag);\nstat = MPR2(MPmodel,verbose=1); # stops at iteration 3, throw lack of precision warning","category":"page"},{"location":"","page":"Home","title":"Home","text":"We run MPR2() with the callback functions defined above and the default callbacks for compute_g!() and recompute_g!(). We use relative objective and gradient error.","category":"page"},{"location":"","page":"Home","title":"Home","text":"FP = [Float16, Float32] # selected FP formats,\n#f(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2 # Rosenbrock function\nf(x) = x[1]^2 + x[2]^2 +0.5\nomegaf = Float64.([0.01,0.005])\nomegag = Float64.([0.05,0.01])\nx = ones(Float32,2) # initial point\nMPmodel = FPMPNLPModel(f,x,FP, ωfRelErr = omegaf, ωgRelErr = omegag);\ne = my_struct(false,1e-2)\nstat = MPR2(MPmodel;\ne = e,\ncompute_f_at_x! = my_compute_f_at_x!,\ncompute_f_at_c! = my_compute_f_at_c!,\ncompute_g! = my_compute_g!); # switch to gradient descent at iteration 3, converges to first order critical point","category":"page"}]
}
